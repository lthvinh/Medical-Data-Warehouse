[2025-05-01T07:06:07.645+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-01T07:06:07.674+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: medical_dag_v12.raw_enriched_group.creating_enriched_plan_transitions manual__2025-05-01T07:05:50.707029+00:00 [queued]>
[2025-05-01T07:06:07.690+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: medical_dag_v12.raw_enriched_group.creating_enriched_plan_transitions manual__2025-05-01T07:05:50.707029+00:00 [queued]>
[2025-05-01T07:06:07.691+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-05-01T07:06:07.717+0000] {taskinstance.py:2890} INFO - Executing <Task(SparkSubmitOperator): raw_enriched_group.creating_enriched_plan_transitions> on 2025-05-01 07:05:50.707029+00:00
[2025-05-01T07:06:07.727+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'medical_dag_v12', 'raw_enriched_group.creating_enriched_plan_transitions', 'manual__2025-05-01T07:05:50.707029+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/raw_enriched_dag.py', '--cfg-path', '/tmp/tmp5ydqkxu6']
[2025-05-01T07:06:07.730+0000] {standard_task_runner.py:105} INFO - Job 85: Subtask raw_enriched_group.creating_enriched_plan_transitions
[2025-05-01T07:06:07.734+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=4834) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-05-01T07:06:07.735+0000] {standard_task_runner.py:72} INFO - Started process 4835 to run task
[2025-05-01T07:06:07.795+0000] {task_command.py:467} INFO - Running <TaskInstance: medical_dag_v12.raw_enriched_group.creating_enriched_plan_transitions manual__2025-05-01T07:05:50.707029+00:00 [running]> on host airflow-scheduler
[2025-05-01T07:06:07.910+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='medical_dag_v12' AIRFLOW_CTX_TASK_ID='raw_enriched_group.creating_enriched_plan_transitions' AIRFLOW_CTX_EXECUTION_DATE='2025-05-01T07:05:50.707029+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-01T07:05:50.707029+00:00'
[2025-05-01T07:06:07.911+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-05-01T07:06:07.948+0000] {base.py:84} INFO - Retrieving connection 'spark_conn'
[2025-05-01T07:06:07.950+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local --packages io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4 --name arrow-spark --deploy-mode client /opt/etl/raw_enriched/creating_enriched_plan_transitions.py
[2025-05-01T07:06:09.916+0000] {spark_submit.py:649} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-01T07:06:10.025+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-01T07:06:10.027+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-01T07:06:10.032+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12 added as a dependency
[2025-05-01T07:06:10.033+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-05-01T07:06:10.034+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-6adf3e50-e140-477d-ba05-43d14eeffeac;1.0
[2025-05-01T07:06:10.035+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-01T07:06:10.199+0000] {spark_submit.py:649} INFO - found io.delta#delta-spark_2.12;3.3.0 in central
[2025-05-01T07:06:10.234+0000] {spark_submit.py:649} INFO - found io.delta#delta-storage;3.3.0 in central
[2025-05-01T07:06:10.263+0000] {spark_submit.py:649} INFO - found org.antlr#antlr4-runtime;4.9.3 in central
[2025-05-01T07:06:10.348+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2025-05-01T07:06:10.380+0000] {spark_submit.py:649} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2025-05-01T07:06:10.414+0000] {spark_submit.py:649} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-05-01T07:06:10.449+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve 395ms :: artifacts dl 19ms
[2025-05-01T07:06:10.450+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-01T07:06:10.451+0000] {spark_submit.py:649} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2025-05-01T07:06:10.452+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12;3.3.0 from central in [default]
[2025-05-01T07:06:10.453+0000] {spark_submit.py:649} INFO - io.delta#delta-storage;3.3.0 from central in [default]
[2025-05-01T07:06:10.454+0000] {spark_submit.py:649} INFO - org.antlr#antlr4-runtime;4.9.3 from central in [default]
[2025-05-01T07:06:10.455+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2025-05-01T07:06:10.456+0000] {spark_submit.py:649} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-05-01T07:06:10.457+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:10.457+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-01T07:06:10.458+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-01T07:06:10.459+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:10.460+0000] {spark_submit.py:649} INFO - |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
[2025-05-01T07:06:10.460+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:10.461+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-6adf3e50-e140-477d-ba05-43d14eeffeac
[2025-05-01T07:06:10.462+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-01T07:06:10.473+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 6 already retrieved (0kB/12ms)
[2025-05-01T07:06:10.862+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-01T07:06:17.333+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkContext: Running Spark version 3.5.3
[2025-05-01T07:06:17.333+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-01T07:06:17.334+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkContext: Java version 17.0.14
[2025-05-01T07:06:17.362+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceUtils: ==============================================================
[2025-05-01T07:06:17.363+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-01T07:06:17.363+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceUtils: ==============================================================
[2025-05-01T07:06:17.364+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkContext: Submitted application: MyETLPipeline
[2025-05-01T07:06:17.396+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-01T07:06:17.406+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceProfile: Limiting resource is cpu
[2025-05-01T07:06:17.407+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-01T07:06:17.489+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SecurityManager: Changing view acls to: ***
[2025-05-01T07:06:17.490+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SecurityManager: Changing modify acls to: ***
[2025-05-01T07:06:17.491+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SecurityManager: Changing view acls groups to:
[2025-05-01T07:06:17.491+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SecurityManager: Changing modify acls groups to:
[2025-05-01T07:06:17.492+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-05-01T07:06:17.837+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO Utils: Successfully started service 'sparkDriver' on port 46425.
[2025-05-01T07:06:17.886+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkEnv: Registering MapOutputTracker
[2025-05-01T07:06:17.931+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-01T07:06:17.958+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-01T07:06:17.959+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-01T07:06:17.964+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-01T07:06:17.998+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1aed7600-d512-40ae-b350-ae572a653b34
[2025-05-01T07:06:18.021+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-01T07:06:18.044+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-01T07:06:18.249+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-01T07:06:18.357+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-01T07:06:18.423+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at spark://***-scheduler:46425/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:18.425+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://***-scheduler:46425/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746083177325
[2025-05-01T07:06:18.426+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at spark://***-scheduler:46425/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:18.426+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://***-scheduler:46425/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746083177325
[2025-05-01T07:06:18.427+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://***-scheduler:46425/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746083177325
[2025-05-01T07:06:18.428+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://***-scheduler:46425/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746083177325
[2025-05-01T07:06:18.429+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:18.431+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-01T07:06:18.485+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746083177325
[2025-05-01T07:06:18.486+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-01T07:06:18.499+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:18.500+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-storage-3.3.0.jar
[2025-05-01T07:06:18.507+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746083177325
[2025-05-01T07:06:18.508+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Copying /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-01T07:06:18.521+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746083177325
[2025-05-01T07:06:18.522+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:18 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-01T07:06:20.385+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746083177325
[2025-05-01T07:06:20.386+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-01T07:06:20.510+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Starting executor ID driver on host ***-scheduler
[2025-05-01T07:06:20.512+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-01T07:06:20.513+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Java version 17.0.14
[2025-05-01T07:06:20.522+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-01T07:06:20.523+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2b96c559 for default.
[2025-05-01T07:06:20.540+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:20.572+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-storage-3.3.0.jar
[2025-05-01T07:06:20.578+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746083177325
[2025-05-01T07:06:20.590+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-01T07:06:20.599+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:20.638+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-01T07:06:20.644+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746083177325
[2025-05-01T07:06:20.647+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-01T07:06:20.654+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746083177325
[2025-05-01T07:06:20.655+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Utils: /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-01T07:06:20.661+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:20 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746083177325
[2025-05-01T07:06:21.562+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-01T07:06:21.572+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Fetching spark://***-scheduler:46425/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:21.646+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO TransportClientFactory: Successfully created connection to ***-scheduler/172.18.0.4:46425 after 48 ms (0 ms spent in bootstraps)
[2025-05-01T07:06:21.656+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: Fetching spark://***-scheduler:46425/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp6564186464583870799.tmp
[2025-05-01T07:06:21.775+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp6564186464583870799.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-01T07:06:21.784+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-spark_2.12-3.3.0.jar to class loader default
[2025-05-01T07:06:21.785+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Fetching spark://***-scheduler:46425/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746083177325
[2025-05-01T07:06:21.786+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: Fetching spark://***-scheduler:46425/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp4232442171233976880.tmp
[2025-05-01T07:06:21.789+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp4232442171233976880.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-storage-3.3.0.jar
[2025-05-01T07:06:21.796+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/io.delta_delta-storage-3.3.0.jar to class loader default
[2025-05-01T07:06:21.797+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Fetching spark://***-scheduler:46425/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746083177325
[2025-05-01T07:06:21.798+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: Fetching spark://***-scheduler:46425/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp13123539372906845832.tmp
[2025-05-01T07:06:21.801+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp13123539372906845832.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-01T07:06:21.808+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.antlr_antlr4-runtime-4.9.3.jar to class loader default
[2025-05-01T07:06:21.809+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Executor: Fetching spark://***-scheduler:46425/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746083177325
[2025-05-01T07:06:21.810+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:21 INFO Utils: Fetching spark://***-scheduler:46425/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp18287673367016206947.tmp
[2025-05-01T07:06:24.148+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp18287673367016206947.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-01T07:06:24.234+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to class loader default
[2025-05-01T07:06:24.236+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Executor: Fetching spark://***-scheduler:46425/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746083177325
[2025-05-01T07:06:24.237+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: Fetching spark://***-scheduler:46425/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp17356805424623864450.tmp
[2025-05-01T07:06:24.249+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp17356805424623864450.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-01T07:06:24.258+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to class loader default
[2025-05-01T07:06:24.259+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Executor: Fetching spark://***-scheduler:46425/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746083177325
[2025-05-01T07:06:24.261+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: Fetching spark://***-scheduler:46425/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp10212557456738593312.tmp
[2025-05-01T07:06:24.271+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/fetchFileTemp10212557456738593312.tmp has been previously copied to /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-01T07:06:24.280+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Executor: Adding file:/tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/userFiles-33d07207-e228-4f58-8dde-d7807420a428/org.apache.hadoop_hadoop-aws-3.3.4.jar to class loader default
[2025-05-01T07:06:24.306+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35203.
[2025-05-01T07:06:24.307+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO NettyBlockTransferService: Server created on ***-scheduler:35203
[2025-05-01T07:06:24.311+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-01T07:06:24.337+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ***-scheduler, 35203, None)
[2025-05-01T07:06:24.349+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO BlockManagerMasterEndpoint: Registering block manager ***-scheduler:35203 with 434.4 MiB RAM, BlockManagerId(driver, ***-scheduler, 35203, None)
[2025-05-01T07:06:24.357+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ***-scheduler, 35203, None)
[2025-05-01T07:06:24.361+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ***-scheduler, 35203, None)
[2025-05-01T07:06:25.442+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-01T07:06:25.446+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:25 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-05-01T07:06:27.316+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:27 INFO HiveConf: Found configuration file null
[2025-05-01T07:06:27.333+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:27 INFO HiveUtils: Initializing HiveMetastoreConnection version 3.1.3 using maven.
[2025-05-01T07:06:27.354+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-01T07:06:27.357+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-01T07:06:27.382+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-01T07:06:27.383+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-01T07:06:27.384+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api added as a dependency
[2025-05-01T07:06:27.385+0000] {spark_submit.py:649} INFO - org.apache.derby#derby added as a dependency
[2025-05-01T07:06:27.385+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore added as a dependency
[2025-05-01T07:06:27.386+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec added as a dependency
[2025-05-01T07:06:27.387+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common added as a dependency
[2025-05-01T07:06:27.388+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde added as a dependency
[2025-05-01T07:06:27.389+0000] {spark_submit.py:649} INFO - com.google.guava#guava added as a dependency
[2025-05-01T07:06:27.390+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api added as a dependency
[2025-05-01T07:06:27.391+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime added as a dependency
[2025-05-01T07:06:27.392+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-9f3919fd-6777-43a8-8503-26bf14daed24;1.0
[2025-05-01T07:06:27.393+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-01T07:06:27.426+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-api;2.10.0 in central
[2025-05-01T07:06:27.447+0000] {spark_submit.py:649} INFO - found org.apache.derby#derby;10.14.1.0 in central
[2025-05-01T07:06:27.486+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-metastore;3.1.3 in central
[2025-05-01T07:06:27.544+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-serde;3.1.3 in central
[2025-05-01T07:06:27.606+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-common;3.1.3 in central
[2025-05-01T07:06:27.691+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-classification;3.1.3 in central
[2025-05-01T07:06:27.733+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.10 in central
[2025-05-01T07:06:27.769+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-upgrade-acid;3.1.3 in central
[2025-05-01T07:06:27.818+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-shims;3.1.3 in central
[2025-05-01T07:06:23.136+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-common;3.1.3 in central
[2025-05-01T07:06:23.182+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 in central
[2025-05-01T07:06:23.229+0000] {spark_submit.py:649} INFO - found com.google.guava#guava;19.0 in central
[2025-05-01T07:06:23.263+0000] {spark_submit.py:649} INFO - found commons-lang#commons-lang;2.6 in central
[2025-05-01T07:06:31.422+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libthrift;0.9.3 in central
[2025-05-01T07:06:31.469+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpclient;4.5.13 in central
[2025-05-01T07:06:31.517+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpcore;4.4.13 in central
[2025-05-01T07:06:31.560+0000] {spark_submit.py:649} INFO - found commons-logging#commons-logging;1.2 in central
[2025-05-01T07:06:31.603+0000] {spark_submit.py:649} INFO - found commons-codec#commons-codec;1.15 in central
[2025-05-01T07:06:31.644+0000] {spark_submit.py:649} INFO - found org.apache.zookeeper#zookeeper;3.4.6 in central
[2025-05-01T07:06:31.700+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-log4j12;1.6.1 in central
[2025-05-01T07:06:31.741+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.16 in central
[2025-05-01T07:06:31.788+0000] {spark_submit.py:649} INFO - found jline#jline;2.12 in central
[2025-05-01T07:06:31.832+0000] {spark_submit.py:649} INFO - found io.netty#netty;3.7.0.Final in central
[2025-05-01T07:06:31.892+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-core;2.17.1 in central
[2025-05-01T07:06:31.947+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-0.23;3.1.3 in central
[2025-05-01T07:06:32.021+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 in central
[2025-05-01T07:06:32.083+0000] {spark_submit.py:649} INFO - found javax.servlet#javax.servlet-api;3.1.0 in central
[2025-05-01T07:06:32.162+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-annotations;3.1.0 in central
[2025-05-01T07:06:32.210+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-servlet;4.0 in central
[2025-05-01T07:06:32.257+0000] {spark_submit.py:649} INFO - found com.google.inject#guice;4.0 in central
[2025-05-01T07:06:32.329+0000] {spark_submit.py:649} INFO - found javax.inject#javax.inject;1 in central
[2025-05-01T07:06:32.367+0000] {spark_submit.py:649} INFO - found aopalliance#aopalliance;1.0 in central
[2025-05-01T07:06:32.421+0000] {spark_submit.py:649} INFO - found com.google.protobuf#protobuf-java;2.5.0 in central
[2025-05-01T07:06:32.463+0000] {spark_submit.py:649} INFO - found commons-io#commons-io;2.6 in central
[2025-05-01T07:06:32.515+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-json;1.19 in central
[2025-05-01T07:06:32.569+0000] {spark_submit.py:649} INFO - found org.codehaus.jettison#jettison;1.1 in central
[2025-05-01T07:06:32.611+0000] {spark_submit.py:649} INFO - found com.sun.xml.bind#jaxb-impl;2.2.3-1 in central
[2025-05-01T07:06:32.658+0000] {spark_submit.py:649} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2025-05-01T07:06:32.700+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2025-05-01T07:06:32.748+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2025-05-01T07:06:32.803+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-jaxrs;1.9.13 in central
[2025-05-01T07:06:32.872+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-xc;1.9.13 in central
[2025-05-01T07:06:32.955+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-core;1.19 in central
[2025-05-01T07:06:33.010+0000] {spark_submit.py:649} INFO - found javax.ws.rs#jsr311-api;1.1.1 in central
[2025-05-01T07:06:33.053+0000] {spark_submit.py:649} INFO - found com.sun.jersey.contribs#jersey-guice;1.19 in central
[2025-05-01T07:06:33.107+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-servlet;1.19 in central
[2025-05-01T07:06:33.153+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-server;1.19 in central
[2025-05-01T07:06:33.212+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-common;3.1.0 in central
[2025-05-01T07:06:33.279+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-api;3.1.0 in central
[2025-05-01T07:06:33.400+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.12.0 in central
[2025-05-01T07:06:33.456+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-auth;3.1.0 in central
[2025-05-01T07:06:33.582+0000] {spark_submit.py:649} INFO - found com.nimbusds#nimbus-jose-jwt;4.41.1 in central
[2025-05-01T07:06:33.634+0000] {spark_submit.py:649} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-01T07:06:33.676+0000] {spark_submit.py:649} INFO - found net.minidev#json-smart;2.3 in central
[2025-05-01T07:06:33.717+0000] {spark_submit.py:649} INFO - found net.minidev#accessors-smart;1.2 in central
[2025-05-01T07:06:33.759+0000] {spark_submit.py:649} INFO - found org.ow2.asm#asm;5.0.4 in central
[2025-05-01T07:06:33.810+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-simplekdc;1.0.1 in central
[2025-05-01T07:06:33.853+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-client;1.0.1 in central
[2025-05-01T07:06:33.901+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-config;1.0.1 in central
[2025-05-01T07:06:33.936+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-core;1.0.1 in central
[2025-05-01T07:06:33.970+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-pkix;1.0.1 in central
[2025-05-01T07:06:34.011+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-asn1;1.0.1 in central
[2025-05-01T07:06:34.060+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-util;1.0.1 in central
[2025-05-01T07:06:34.127+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-common;1.0.1 in central
[2025-05-01T07:06:34.175+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-crypto;1.0.1 in central
[2025-05-01T07:06:34.252+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-util;1.0.1 in central
[2025-05-01T07:06:34.295+0000] {spark_submit.py:649} INFO - found org.apache.kerby#token-provider;1.0.1 in central
[2025-05-01T07:06:34.344+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-admin;1.0.1 in central
[2025-05-01T07:06:34.386+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-server;1.0.1 in central
[2025-05-01T07:06:34.422+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-identity;1.0.1 in central
[2025-05-01T07:06:34.458+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-xdr;1.0.1 in central
[2025-05-01T07:06:34.489+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-compress;1.19 in central
[2025-05-01T07:06:34.520+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util;9.3.19.v20170502 in central
[2025-05-01T07:06:34.552+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-client;1.19 in central
[2025-05-01T07:06:34.595+0000] {spark_submit.py:649} INFO - found commons-cli#commons-cli;1.2 in central
[2025-05-01T07:06:34.641+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.17 in central
[2025-05-01T07:06:34.674+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-core;2.12.0 in central
[2025-05-01T07:06:34.702+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-databind;2.12.0 in central
[2025-05-01T07:06:34.734+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 in central
[2025-05-01T07:06:34.770+0000] {spark_submit.py:649} INFO - found jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central
[2025-05-01T07:06:34.821+0000] {spark_submit.py:649} INFO - found jakarta.activation#jakarta.activation-api;1.2.1 in central
[2025-05-01T07:06:34.863+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 in central
[2025-05-01T07:06:34.903+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 in central
[2025-05-01T07:06:34.937+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 in central
[2025-05-01T07:06:34.977+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-common;3.1.0 in central
[2025-05-01T07:06:35.020+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-registry;3.1.0 in central
[2025-05-01T07:06:35.110+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-common;3.1.0 in central
[2025-05-01T07:06:35.192+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-math3;3.1.1 in central
[2025-05-01T07:06:35.316+0000] {spark_submit.py:649} INFO - found commons-net#commons-net;3.6 in central
[2025-05-01T07:06:35.355+0000] {spark_submit.py:649} INFO - found commons-collections#commons-collections;3.2.2 in central
[2025-05-01T07:06:35.400+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-server;9.3.20.v20170531 in central
[2025-05-01T07:06:35.435+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-http;9.3.20.v20170531 in central
[2025-05-01T07:06:35.487+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-io;9.3.20.v20170531 in central
[2025-05-01T07:06:35.537+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 in central
[2025-05-01T07:06:35.580+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-security;9.3.20.v20170531 in central
[2025-05-01T07:06:35.623+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 in central
[2025-05-01T07:06:35.668+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-xml;9.3.20.v20170531 in central
[2025-05-01T07:06:35.836+0000] {spark_submit.py:649} INFO - found commons-beanutils#commons-beanutils;1.9.3 in central
[2025-05-01T07:06:35.875+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-configuration2;2.1.1 in central
[2025-05-01T07:06:35.943+0000] {spark_submit.py:649} INFO - found org.apache.avro#avro;1.8.2 in central
[2025-05-01T07:06:36.009+0000] {spark_submit.py:649} INFO - found com.thoughtworks.paranamer#paranamer;2.7 in central
[2025-05-01T07:06:36.042+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.4 in central
[2025-05-01T07:06:36.085+0000] {spark_submit.py:649} INFO - found org.tukaani#xz;1.5 in central
[2025-05-01T07:06:36.116+0000] {spark_submit.py:649} INFO - found com.google.re2j#re2j;1.1 in central
[2025-05-01T07:06:36.159+0000] {spark_submit.py:649} INFO - found com.google.code.gson#gson;2.2.4 in central
[2025-05-01T07:06:36.209+0000] {spark_submit.py:649} INFO - found com.jcraft#jsch;0.1.54 in central
[2025-05-01T07:06:36.249+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-01T07:06:36.286+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core4;4.1.0-incubating in central
[2025-05-01T07:06:36.373+0000] {spark_submit.py:649} INFO - found org.codehaus.woodstox#stax2-api;3.1.4 in central
[2025-05-01T07:06:36.414+0000] {spark_submit.py:649} INFO - found com.fasterxml.woodstox#woodstox-core;5.0.3 in central
[2025-05-01T07:06:36.464+0000] {spark_submit.py:649} INFO - found commons-daemon#commons-daemon;1.0.13 in central
[2025-05-01T07:06:36.512+0000] {spark_submit.py:649} INFO - found dnsjava#dnsjava;2.1.7 in central
[2025-05-01T07:06:36.542+0000] {spark_submit.py:649} INFO - found org.fusesource.leveldbjni#leveldbjni-all;1.8 in central
[2025-05-01T07:06:36.569+0000] {spark_submit.py:649} INFO - found org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 in central
[2025-05-01T07:06:36.595+0000] {spark_submit.py:649} INFO - found org.ehcache#ehcache;3.3.1 in central
[2025-05-01T07:06:36.619+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP-java7;2.4.12 in central
[2025-05-01T07:06:36.653+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 in central
[2025-05-01T07:06:36.728+0000] {spark_submit.py:649} INFO - found de.ruedigermoeller#fst;2.50 in central
[2025-05-01T07:06:36.755+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#java-util;1.9.0 in central
[2025-05-01T07:06:36.793+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#json-io;2.5.1 in central
[2025-05-01T07:06:36.833+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 in central
[2025-05-01T07:06:37.011+0000] {spark_submit.py:649} INFO - found javax.servlet.jsp#jsp-api;2.1 in central
[2025-05-01T07:06:37.058+0000] {spark_submit.py:649} INFO - found com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 in central
[2025-05-01T07:06:37.097+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-scheduler;3.1.3 in central
[2025-05-01T07:06:37.129+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-storage-api;2.7.0 in central
[2025-05-01T07:06:37.143+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-lang3;3.9 in central
[2025-05-01T07:06:37.175+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-core;1.5.8 in central
[2025-05-01T07:06:37.210+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-shims;1.5.8 in central
[2025-05-01T07:06:37.262+0000] {spark_submit.py:649} INFO - found io.airlift#aircompressor;0.10 in central
[2025-05-01T07:06:37.298+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 in central
[2025-05-01T07:06:37.334+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-client;9.3.20.v20170531 in central
[2025-05-01T07:06:37.368+0000] {spark_submit.py:649} INFO - found joda-time#joda-time;2.9.9 in central
[2025-05-01T07:06:37.412+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-1.2-api;2.17.1 in central
[2025-05-01T07:06:37.446+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-web;2.17.1 in central
[2025-05-01T07:06:37.488+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant;1.9.1 in central
[2025-05-01T07:06:37.516+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant-launcher;1.9.1 in central
[2025-05-01T07:06:37.546+0000] {spark_submit.py:649} INFO - found net.sf.jpam#jpam;1.1 in central
[2025-05-01T07:06:37.582+0000] {spark_submit.py:649} INFO - found com.tdunning#json;1.8 in central
[2025-05-01T07:06:37.619+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.1.0 in central
[2025-05-01T07:06:37.656+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-jvm;3.1.0 in central
[2025-05-01T07:06:37.694+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-json;3.1.0 in central
[2025-05-01T07:06:37.728+0000] {spark_submit.py:649} INFO - found com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in central
[2025-05-01T07:06:37.764+0000] {spark_submit.py:649} INFO - found javolution#javolution;5.5.1 in central
[2025-05-01T07:06:37.823+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-service-rpc;3.1.3 in central
[2025-05-01T07:06:37.846+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libfb303;0.9.3 in central
[2025-05-01T07:06:37.873+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-vector;0.8.0 in central
[2025-05-01T07:06:37.897+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-format;0.8.0 in central
[2025-05-01T07:06:37.919+0000] {spark_submit.py:649} INFO - found com.vlkan#flatbuffers;1.2.0-3f79e055 in central
[2025-05-01T07:06:37.944+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-memory;0.8.0 in central
[2025-05-01T07:06:37.978+0000] {spark_submit.py:649} INFO - found io.netty#netty-buffer;4.1.17.Final in central
[2025-05-01T07:06:38.004+0000] {spark_submit.py:649} INFO - found io.netty#netty-common;4.1.17.Final in central
[2025-05-01T07:06:38.050+0000] {spark_submit.py:649} INFO - found com.carrotsearch#hppc;0.7.2 in central
[2025-05-01T07:06:38.075+0000] {spark_submit.py:649} INFO - found net.sf.opencsv#opencsv;2.3 in central
[2025-05-01T07:06:38.096+0000] {spark_submit.py:649} INFO - found org.apache.parquet#parquet-hadoop-bundle;1.10.0 in central
[2025-05-01T07:06:38.125+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-standalone-metastore;3.1.3 in central
[2025-05-01T07:06:38.165+0000] {spark_submit.py:649} INFO - found com.jolbox#bonecp;0.8.0.RELEASE in central
[2025-05-01T07:06:38.193+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP;2.6.1 in central
[2025-05-01T07:06:38.215+0000] {spark_submit.py:649} INFO - found commons-dbcp#commons-dbcp;1.4 in central
[2025-05-01T07:06:38.235+0000] {spark_submit.py:649} INFO - found commons-pool#commons-pool;1.5.4 in central
[2025-05-01T07:06:38.267+0000] {spark_submit.py:649} INFO - found org.antlr#antlr-runtime;3.5.2 in central
[2025-05-01T07:06:38.320+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-api-jdo;4.2.4 in central
[2025-05-01T07:06:38.340+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-core;4.1.17 in central
[2025-05-01T07:06:38.361+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-rdbms;4.1.19 in central
[2025-05-01T07:06:38.380+0000] {spark_submit.py:649} INFO - found org.datanucleus#javax.jdo;3.2.0-m3 in central
[2025-05-01T07:06:38.414+0000] {spark_submit.py:649} INFO - found javax.transaction#transaction-api;1.1 in central
[2025-05-01T07:06:38.436+0000] {spark_submit.py:649} INFO - found sqlline#sqlline;1.3.0 in central
[2025-05-01T07:06:38.468+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-client;2.0.0-alpha4 in central
[2025-05-01T07:06:38.487+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 in central
[2025-05-01T07:06:38.510+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 in central
[2025-05-01T07:06:38.528+0000] {spark_submit.py:649} INFO - found org.apache.yetus#audience-annotations;0.5.0 in central
[2025-05-01T07:06:38.546+0000] {spark_submit.py:649} INFO - found junit#junit;4.11 in central
[2025-05-01T07:06:38.561+0000] {spark_submit.py:649} INFO - found org.hamcrest#hamcrest-core;1.3 in central
[2025-05-01T07:06:38.583+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol;2.0.0-alpha4 in central
[2025-05-01T07:06:38.616+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 in central
[2025-05-01T07:06:38.633+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 in central
[2025-05-01T07:06:38.652+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core;3.2.0-incubating in central
[2025-05-01T07:06:38.670+0000] {spark_submit.py:649} INFO - found org.jruby.jcodings#jcodings;1.0.18 in central
[2025-05-01T07:06:38.687+0000] {spark_submit.py:649} INFO - found org.jruby.joni#joni;2.1.11 in central
[2025-05-01T07:06:38.700+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.2.1 in central
[2025-05-01T07:06:38.720+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-crypto;1.0.0 in central
[2025-05-01T07:06:38.744+0000] {spark_submit.py:649} INFO - found javax.jdo#jdo-api;3.0.1 in central
[2025-05-01T07:06:38.776+0000] {spark_submit.py:649} INFO - found javax.transaction#jta;1.1 in central
[2025-05-01T07:06:38.797+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-api;0.6.0 in central
[2025-05-01T07:06:38.818+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-core;0.6.0 in central
[2025-05-01T07:06:38.857+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-assistedinject;3.0 in central
[2025-05-01T07:06:38.881+0000] {spark_submit.py:649} INFO - found it.unimi.dsi#fastutil;6.5.6 in central
[2025-05-01T07:06:38.900+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-common;0.6.0-incubating in central
[2025-05-01T07:06:38.932+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-core;0.6.0-incubating in central
[2025-05-01T07:06:38.964+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-api;0.6.0-incubating in central
[2025-05-01T07:06:38.996+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-api;0.6.0-incubating in central
[2025-05-01T07:06:39.020+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-zookeeper;0.6.0-incubating in central
[2025-05-01T07:06:39.064+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-core;0.6.0-incubating in central
[2025-05-01T07:06:39.096+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 in central
[2025-05-01T07:06:39.126+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-exec;3.1.3 in central
[2025-05-01T07:06:39.160+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-tez;3.1.3 in central
[2025-05-01T07:06:39.191+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-client;3.1.3 in central
[2025-05-01T07:06:39.217+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-common;3.1.3 in central
[2025-05-01T07:06:39.310+0000] {spark_submit.py:649} INFO - found org.antlr#ST4;4.0.4 in central
[2025-05-01T07:06:39.343+0000] {spark_submit.py:649} INFO - found org.apache.ivy#ivy;2.4.0 in central
[2025-05-01T07:06:39.359+0000] {spark_submit.py:649} INFO - found org.codehaus.groovy#groovy-all;2.4.11 in central
[2025-05-01T07:06:39.393+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-core;1.16.0 in central
[2025-05-01T07:06:39.412+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-linq4j;1.16.0 in central
[2025-05-01T07:06:39.446+0000] {spark_submit.py:649} INFO - found com.esri.geometry#esri-geometry-api;2.0.0 in central
[2025-05-01T07:06:39.462+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.1 in central
[2025-05-01T07:06:39.479+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#sketches-core;0.9.0 in central
[2025-05-01T07:06:39.498+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#memory;0.9.0 in central
[2025-05-01T07:06:39.519+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#janino;2.7.6 in central
[2025-05-01T07:06:39.542+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#commons-compiler;2.7.6 in central
[2025-05-01T07:06:39.571+0000] {spark_submit.py:649} INFO - found org.apache.calcite.avatica#avatica;1.11.0 in central
[2025-05-01T07:06:39.600+0000] {spark_submit.py:649} INFO - found stax#stax-api;1.0.1 in central
[2025-05-01T07:06:39.649+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-01T07:06:39.666+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.8.2 in central
[2025-05-01T07:06:39.696+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-01T07:06:39.715+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.36 in central
[2025-05-01T07:06:40.049+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve 12416ms :: artifacts dl 250ms
[2025-05-01T07:06:40.050+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-01T07:06:40.051+0000] {spark_submit.py:649} INFO - aopalliance#aopalliance;1.0 from central in [default]
[2025-05-01T07:06:40.052+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-api;0.6.0 from central in [default]
[2025-05-01T07:06:40.054+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-core;0.6.0 from central in [default]
[2025-05-01T07:06:40.054+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 from central in [default]
[2025-05-01T07:06:40.056+0000] {spark_submit.py:649} INFO - com.carrotsearch#hppc;0.7.2 from central in [default]
[2025-05-01T07:06:40.057+0000] {spark_submit.py:649} INFO - com.cedarsoftware#java-util;1.9.0 from central in [default]
[2025-05-01T07:06:40.058+0000] {spark_submit.py:649} INFO - com.cedarsoftware#json-io;2.5.1 from central in [default]
[2025-05-01T07:06:40.059+0000] {spark_submit.py:649} INFO - com.esri.geometry#esri-geometry-api;2.0.0 from central in [default]
[2025-05-01T07:06:40.060+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-annotations;2.12.0 from central in [default]
[2025-05-01T07:06:40.061+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-core;2.12.0 from central in [default]
[2025-05-01T07:06:40.062+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-databind;2.12.0 from central in [default]
[2025-05-01T07:06:40.063+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 from central in [default]
[2025-05-01T07:06:40.064+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 from central in [default]
[2025-05-01T07:06:40.065+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 from central in [default]
[2025-05-01T07:06:40.066+0000] {spark_submit.py:649} INFO - com.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]
[2025-05-01T07:06:40.067+0000] {spark_submit.py:649} INFO - com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from central in [default]
[2025-05-01T07:06:40.068+0000] {spark_submit.py:649} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-01T07:06:40.069+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.1 from central in [default]
[2025-05-01T07:06:40.070+0000] {spark_submit.py:649} INFO - com.google.code.gson#gson;2.2.4 from central in [default]
[2025-05-01T07:06:40.071+0000] {spark_submit.py:649} INFO - com.google.guava#guava;19.0 from central in [default]
[2025-05-01T07:06:40.073+0000] {spark_submit.py:649} INFO - com.google.inject#guice;4.0 from central in [default]
[2025-05-01T07:06:40.074+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-assistedinject;3.0 from central in [default]
[2025-05-01T07:06:40.075+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-servlet;4.0 from central in [default]
[2025-05-01T07:06:40.076+0000] {spark_submit.py:649} INFO - com.google.protobuf#protobuf-java;2.5.0 from central in [default]
[2025-05-01T07:06:40.078+0000] {spark_submit.py:649} INFO - com.google.re2j#re2j;1.1 from central in [default]
[2025-05-01T07:06:40.079+0000] {spark_submit.py:649} INFO - com.jcraft#jsch;0.1.54 from central in [default]
[2025-05-01T07:06:40.080+0000] {spark_submit.py:649} INFO - com.jolbox#bonecp;0.8.0.RELEASE from central in [default]
[2025-05-01T07:06:40.081+0000] {spark_submit.py:649} INFO - com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 from central in [default]
[2025-05-01T07:06:40.082+0000] {spark_submit.py:649} INFO - com.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]
[2025-05-01T07:06:40.083+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-client;1.19 from central in [default]
[2025-05-01T07:06:40.084+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-core;1.19 from central in [default]
[2025-05-01T07:06:40.085+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-json;1.19 from central in [default]
[2025-05-01T07:06:40.086+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-server;1.19 from central in [default]
[2025-05-01T07:06:40.087+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-servlet;1.19 from central in [default]
[2025-05-01T07:06:40.088+0000] {spark_submit.py:649} INFO - com.sun.jersey.contribs#jersey-guice;1.19 from central in [default]
[2025-05-01T07:06:40.089+0000] {spark_submit.py:649} INFO - com.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]
[2025-05-01T07:06:40.090+0000] {spark_submit.py:649} INFO - com.tdunning#json;1.8 from central in [default]
[2025-05-01T07:06:40.091+0000] {spark_submit.py:649} INFO - com.thoughtworks.paranamer#paranamer;2.7 from central in [default]
[2025-05-01T07:06:40.093+0000] {spark_submit.py:649} INFO - com.vlkan#flatbuffers;1.2.0-3f79e055 from central in [default]
[2025-05-01T07:06:40.094+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#memory;0.9.0 from central in [default]
[2025-05-01T07:06:40.095+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#sketches-core;0.9.0 from central in [default]
[2025-05-01T07:06:40.096+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP;2.6.1 from central in [default]
[2025-05-01T07:06:40.096+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP-java7;2.4.12 from central in [default]
[2025-05-01T07:06:40.098+0000] {spark_submit.py:649} INFO - commons-beanutils#commons-beanutils;1.9.3 from central in [default]
[2025-05-01T07:06:40.099+0000] {spark_submit.py:649} INFO - commons-cli#commons-cli;1.2 from central in [default]
[2025-05-01T07:06:40.100+0000] {spark_submit.py:649} INFO - commons-codec#commons-codec;1.15 from central in [default]
[2025-05-01T07:06:40.101+0000] {spark_submit.py:649} INFO - commons-collections#commons-collections;3.2.2 from central in [default]
[2025-05-01T07:06:40.102+0000] {spark_submit.py:649} INFO - commons-daemon#commons-daemon;1.0.13 from central in [default]
[2025-05-01T07:06:40.104+0000] {spark_submit.py:649} INFO - commons-dbcp#commons-dbcp;1.4 from central in [default]
[2025-05-01T07:06:40.105+0000] {spark_submit.py:649} INFO - commons-io#commons-io;2.6 from central in [default]
[2025-05-01T07:06:40.106+0000] {spark_submit.py:649} INFO - commons-lang#commons-lang;2.6 from central in [default]
[2025-05-01T07:06:40.107+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.2 from central in [default]
[2025-05-01T07:06:40.108+0000] {spark_submit.py:649} INFO - commons-net#commons-net;3.6 from central in [default]
[2025-05-01T07:06:40.109+0000] {spark_submit.py:649} INFO - commons-pool#commons-pool;1.5.4 from central in [default]
[2025-05-01T07:06:40.110+0000] {spark_submit.py:649} INFO - de.ruedigermoeller#fst;2.50 from central in [default]
[2025-05-01T07:06:40.111+0000] {spark_submit.py:649} INFO - dnsjava#dnsjava;2.1.7 from central in [default]
[2025-05-01T07:06:40.112+0000] {spark_submit.py:649} INFO - io.airlift#aircompressor;0.10 from central in [default]
[2025-05-01T07:06:40.113+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.2.1 from central in [default]
[2025-05-01T07:06:40.114+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-json;3.1.0 from central in [default]
[2025-05-01T07:06:40.115+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-jvm;3.1.0 from central in [default]
[2025-05-01T07:06:40.116+0000] {spark_submit.py:649} INFO - io.netty#netty;3.7.0.Final from central in [default]
[2025-05-01T07:06:40.117+0000] {spark_submit.py:649} INFO - io.netty#netty-buffer;4.1.17.Final from central in [default]
[2025-05-01T07:06:40.118+0000] {spark_submit.py:649} INFO - io.netty#netty-common;4.1.17.Final from central in [default]
[2025-05-01T07:06:40.119+0000] {spark_submit.py:649} INFO - it.unimi.dsi#fastutil;6.5.6 from central in [default]
[2025-05-01T07:06:40.120+0000] {spark_submit.py:649} INFO - jakarta.activation#jakarta.activation-api;1.2.1 from central in [default]
[2025-05-01T07:06:40.121+0000] {spark_submit.py:649} INFO - jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]
[2025-05-01T07:06:40.122+0000] {spark_submit.py:649} INFO - javax.inject#javax.inject;1 from central in [default]
[2025-05-01T07:06:40.122+0000] {spark_submit.py:649} INFO - javax.jdo#jdo-api;3.0.1 from central in [default]
[2025-05-01T07:06:40.123+0000] {spark_submit.py:649} INFO - javax.servlet#javax.servlet-api;3.1.0 from central in [default]
[2025-05-01T07:06:40.124+0000] {spark_submit.py:649} INFO - javax.servlet.jsp#jsp-api;2.1 from central in [default]
[2025-05-01T07:06:40.124+0000] {spark_submit.py:649} INFO - javax.transaction#jta;1.1 from central in [default]
[2025-05-01T07:06:40.125+0000] {spark_submit.py:649} INFO - javax.transaction#transaction-api;1.1 from central in [default]
[2025-05-01T07:06:40.126+0000] {spark_submit.py:649} INFO - javax.ws.rs#jsr311-api;1.1.1 from central in [default]
[2025-05-01T07:06:40.127+0000] {spark_submit.py:649} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2025-05-01T07:06:40.128+0000] {spark_submit.py:649} INFO - javolution#javolution;5.5.1 from central in [default]
[2025-05-01T07:06:40.129+0000] {spark_submit.py:649} INFO - jline#jline;2.12 from central in [default]
[2025-05-01T07:06:40.130+0000] {spark_submit.py:649} INFO - joda-time#joda-time;2.9.9 from central in [default]
[2025-05-01T07:06:40.131+0000] {spark_submit.py:649} INFO - junit#junit;4.11 from central in [default]
[2025-05-01T07:06:40.132+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.17 from central in [default]
[2025-05-01T07:06:40.133+0000] {spark_submit.py:649} INFO - net.minidev#accessors-smart;1.2 from central in [default]
[2025-05-01T07:06:40.134+0000] {spark_submit.py:649} INFO - net.minidev#json-smart;2.3 from central in [default]
[2025-05-01T07:06:40.135+0000] {spark_submit.py:649} INFO - net.sf.jpam#jpam;1.1 from central in [default]
[2025-05-01T07:06:40.135+0000] {spark_submit.py:649} INFO - net.sf.opencsv#opencsv;2.3 from central in [default]
[2025-05-01T07:06:40.136+0000] {spark_submit.py:649} INFO - org.antlr#ST4;4.0.4 from central in [default]
[2025-05-01T07:06:40.137+0000] {spark_submit.py:649} INFO - org.antlr#antlr-runtime;3.5.2 from central in [default]
[2025-05-01T07:06:40.138+0000] {spark_submit.py:649} INFO - org.apache.ant#ant;1.9.1 from central in [default]
[2025-05-01T07:06:40.139+0000] {spark_submit.py:649} INFO - org.apache.ant#ant-launcher;1.9.1 from central in [default]
[2025-05-01T07:06:40.140+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-format;0.8.0 from central in [default]
[2025-05-01T07:06:40.141+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-memory;0.8.0 from central in [default]
[2025-05-01T07:06:40.142+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-vector;0.8.0 from central in [default]
[2025-05-01T07:06:40.143+0000] {spark_submit.py:649} INFO - org.apache.avro#avro;1.8.2 from central in [default]
[2025-05-01T07:06:40.144+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-core;1.16.0 from central in [default]
[2025-05-01T07:06:40.145+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-linq4j;1.16.0 from central in [default]
[2025-05-01T07:06:40.146+0000] {spark_submit.py:649} INFO - org.apache.calcite.avatica#avatica;1.11.0 from central in [default]
[2025-05-01T07:06:40.147+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-compress;1.19 from central in [default]
[2025-05-01T07:06:40.149+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-configuration2;2.1.1 from central in [default]
[2025-05-01T07:06:40.150+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-crypto;1.0.0 from central in [default]
[2025-05-01T07:06:40.151+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.9 from central in [default]
[2025-05-01T07:06:40.152+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-math3;3.1.1 from central in [default]
[2025-05-01T07:06:40.153+0000] {spark_submit.py:649} INFO - org.apache.derby#derby;10.14.1.0 from central in [default]
[2025-05-01T07:06:40.154+0000] {spark_submit.py:649} INFO - org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 from central in [default]
[2025-05-01T07:06:40.155+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-annotations;3.1.0 from central in [default]
[2025-05-01T07:06:40.156+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-auth;3.1.0 from central in [default]
[2025-05-01T07:06:40.156+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-01T07:06:40.157+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-01T07:06:40.158+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-common;3.1.0 from central in [default]
[2025-05-01T07:06:40.159+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-api;3.1.0 from central in [default]
[2025-05-01T07:06:40.161+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-common;3.1.0 from central in [default]
[2025-05-01T07:06:40.162+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-registry;3.1.0 from central in [default]
[2025-05-01T07:06:40.163+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 from central in [default]
[2025-05-01T07:06:40.164+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-common;3.1.0 from central in [default]
[2025-05-01T07:06:40.165+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 from central in [default]
[2025-05-01T07:06:40.166+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 from central in [default]
[2025-05-01T07:06:40.166+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-client;2.0.0-alpha4 from central in [default]
[2025-05-01T07:06:40.167+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol;2.0.0-alpha4 from central in [default]
[2025-05-01T07:06:40.168+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 from central in [default]
[2025-05-01T07:06:40.169+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 from central in [default]
[2025-05-01T07:06:40.170+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 from central in [default]
[2025-05-01T07:06:40.171+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 from central in [default]
[2025-05-01T07:06:40.172+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-classification;3.1.3 from central in [default]
[2025-05-01T07:06:40.173+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common;3.1.3 from central in [default]
[2025-05-01T07:06:40.175+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec;3.1.3 from central in [default]
[2025-05-01T07:06:40.176+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-client;3.1.3 from central in [default]
[2025-05-01T07:06:40.177+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-common;3.1.3 from central in [default]
[2025-05-01T07:06:40.178+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-tez;3.1.3 from central in [default]
[2025-05-01T07:06:40.179+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore;3.1.3 from central in [default]
[2025-05-01T07:06:40.180+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde;3.1.3 from central in [default]
[2025-05-01T07:06:40.181+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-service-rpc;3.1.3 from central in [default]
[2025-05-01T07:06:40.182+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-shims;3.1.3 from central in [default]
[2025-05-01T07:06:40.182+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-standalone-metastore;3.1.3 from central in [default]
[2025-05-01T07:06:40.183+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-storage-api;2.7.0 from central in [default]
[2025-05-01T07:06:40.184+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-upgrade-acid;3.1.3 from central in [default]
[2025-05-01T07:06:40.185+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-0.23;3.1.3 from central in [default]
[2025-05-01T07:06:40.185+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-common;3.1.3 from central in [default]
[2025-05-01T07:06:40.186+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-scheduler;3.1.3 from central in [default]
[2025-05-01T07:06:40.187+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core;3.2.0-incubating from central in [default]
[2025-05-01T07:06:40.187+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
[2025-05-01T07:06:40.188+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpclient;4.5.13 from central in [default]
[2025-05-01T07:06:40.189+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpcore;4.4.13 from central in [default]
[2025-05-01T07:06:40.189+0000] {spark_submit.py:649} INFO - org.apache.ivy#ivy;2.4.0 from central in [default]
[2025-05-01T07:06:40.190+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-admin;1.0.1 from central in [default]
[2025-05-01T07:06:40.191+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-client;1.0.1 from central in [default]
[2025-05-01T07:06:40.192+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-common;1.0.1 from central in [default]
[2025-05-01T07:06:40.194+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-core;1.0.1 from central in [default]
[2025-05-01T07:06:40.194+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-crypto;1.0.1 from central in [default]
[2025-05-01T07:06:40.195+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-identity;1.0.1 from central in [default]
[2025-05-01T07:06:40.196+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-server;1.0.1 from central in [default]
[2025-05-01T07:06:40.197+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]
[2025-05-01T07:06:40.198+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-util;1.0.1 from central in [default]
[2025-05-01T07:06:40.199+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-asn1;1.0.1 from central in [default]
[2025-05-01T07:06:40.199+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-config;1.0.1 from central in [default]
[2025-05-01T07:06:40.200+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-pkix;1.0.1 from central in [default]
[2025-05-01T07:06:40.201+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-util;1.0.1 from central in [default]
[2025-05-01T07:06:40.202+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-xdr;1.0.1 from central in [default]
[2025-05-01T07:06:40.203+0000] {spark_submit.py:649} INFO - org.apache.kerby#token-provider;1.0.1 from central in [default]
[2025-05-01T07:06:40.204+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-1.2-api;2.17.1 from central in [default]
[2025-05-01T07:06:40.205+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api;2.10.0 from central in [default]
[2025-05-01T07:06:40.206+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-core;2.17.1 from central in [default]
[2025-05-01T07:06:40.207+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 from central in [default]
[2025-05-01T07:06:40.208+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-web;2.17.1 from central in [default]
[2025-05-01T07:06:40.209+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-core;1.5.8 from central in [default]
[2025-05-01T07:06:40.210+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-shims;1.5.8 from central in [default]
[2025-05-01T07:06:40.211+0000] {spark_submit.py:649} INFO - org.apache.parquet#parquet-hadoop-bundle;1.10.0 from central in [default]
[2025-05-01T07:06:40.212+0000] {spark_submit.py:649} INFO - org.apache.thrift#libfb303;0.9.3 from central in [default]
[2025-05-01T07:06:40.212+0000] {spark_submit.py:649} INFO - org.apache.thrift#libthrift;0.9.3 from central in [default]
[2025-05-01T07:06:40.213+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-api;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.214+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-common;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.215+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-core;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.216+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-api;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.217+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-core;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.218+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-zookeeper;0.6.0-incubating from central in [default]
[2025-05-01T07:06:40.219+0000] {spark_submit.py:649} INFO - org.apache.yetus#audience-annotations;0.5.0 from central in [default]
[2025-05-01T07:06:40.220+0000] {spark_submit.py:649} INFO - org.apache.zookeeper#zookeeper;3.4.6 from central in [default]
[2025-05-01T07:06:40.221+0000] {spark_submit.py:649} INFO - org.codehaus.groovy#groovy-all;2.4.11 from central in [default]
[2025-05-01T07:06:40.222+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2025-05-01T07:06:40.223+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]
[2025-05-01T07:06:40.224+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2025-05-01T07:06:40.225+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-xc;1.9.13 from central in [default]
[2025-05-01T07:06:40.226+0000] {spark_submit.py:649} INFO - org.codehaus.janino#commons-compiler;2.7.6 from central in [default]
[2025-05-01T07:06:40.226+0000] {spark_submit.py:649} INFO - org.codehaus.janino#janino;2.7.6 from central in [default]
[2025-05-01T07:06:40.227+0000] {spark_submit.py:649} INFO - org.codehaus.jettison#jettison;1.1 from central in [default]
[2025-05-01T07:06:40.228+0000] {spark_submit.py:649} INFO - org.codehaus.woodstox#stax2-api;3.1.4 from central in [default]
[2025-05-01T07:06:40.229+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]
[2025-05-01T07:06:40.230+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-core;4.1.17 from central in [default]
[2025-05-01T07:06:40.231+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]
[2025-05-01T07:06:40.231+0000] {spark_submit.py:649} INFO - org.datanucleus#javax.jdo;3.2.0-m3 from central in [default]
[2025-05-01T07:06:40.232+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-client;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.233+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-http;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.234+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-io;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.236+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.237+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-security;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.238+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-server;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.239+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.240+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util;9.3.19.v20170502 from central in [default]
[2025-05-01T07:06:40.241+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 from central in [default]
[2025-05-01T07:06:40.242+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.243+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-xml;9.3.20.v20170531 from central in [default]
[2025-05-01T07:06:40.244+0000] {spark_submit.py:649} INFO - org.ehcache#ehcache;3.3.1 from central in [default]
[2025-05-01T07:06:40.245+0000] {spark_submit.py:649} INFO - org.fusesource.leveldbjni#leveldbjni-all;1.8 from central in [default]
[2025-05-01T07:06:40.246+0000] {spark_submit.py:649} INFO - org.hamcrest#hamcrest-core;1.3 from central in [default]
[2025-05-01T07:06:40.246+0000] {spark_submit.py:649} INFO - org.jruby.jcodings#jcodings;1.0.18 from central in [default]
[2025-05-01T07:06:40.247+0000] {spark_submit.py:649} INFO - org.jruby.joni#joni;2.1.11 from central in [default]
[2025-05-01T07:06:40.248+0000] {spark_submit.py:649} INFO - org.ow2.asm#asm;5.0.4 from central in [default]
[2025-05-01T07:06:40.249+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.36 from central in [default]
[2025-05-01T07:06:40.250+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-log4j12;1.6.1 from central in [default]
[2025-05-01T07:06:40.251+0000] {spark_submit.py:649} INFO - org.tukaani#xz;1.5 from central in [default]
[2025-05-01T07:06:40.253+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
[2025-05-01T07:06:40.254+0000] {spark_submit.py:649} INFO - sqlline#sqlline;1.3.0 from central in [default]
[2025-05-01T07:06:40.256+0000] {spark_submit.py:649} INFO - stax#stax-api;1.0.1 from central in [default]
[2025-05-01T07:06:40.256+0000] {spark_submit.py:649} INFO - :: evicted modules:
[2025-05-01T07:06:40.257+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.10 by [org.slf4j#slf4j-api;1.7.36] in [default]
[2025-05-01T07:06:40.258+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.16 by [log4j#log4j;1.2.17] in [default]
[2025-05-01T07:06:40.259+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-01T07:06:40.259+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.4 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-01T07:06:40.260+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.4 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]
[2025-05-01T07:06:40.261+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-01T07:06:40.261+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.0.4 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-01T07:06:40.262+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.0 by [io.dropwizard.metrics#metrics-core;3.2.1] in [default]
[2025-05-01T07:06:40.263+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.2 by [io.dropwizard.metrics#metrics-core;3.1.0] in [default]
[2025-05-01T07:06:40.263+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.2 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-01T07:06:40.264+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.2 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-01T07:06:40.265+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.6 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-01T07:06:40.266+0000] {spark_submit.py:649} INFO - com.google.inject#guice;3.0 by [com.google.inject#guice;4.0] in [default]
[2025-05-01T07:06:40.267+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;2.0.1 by [com.google.code.findbugs#jsr305;3.0.0] in [default]
[2025-05-01T07:06:40.268+0000] {spark_submit.py:649} INFO - com.google.guava#guava;14.0.1 by [com.google.guava#guava;19.0] in [default]
[2025-05-01T07:06:40.269+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:40.269+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-01T07:06:40.270+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-01T07:06:40.271+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:40.272+0000] {spark_submit.py:649} INFO - |      default     |  224  |   0   |   0   |   15  ||  209  |   0   |
[2025-05-01T07:06:40.272+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-01T07:06:40.273+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-9f3919fd-6777-43a8-8503-26bf14daed24
[2025-05-01T07:06:40.274+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-01T07:06:40.274+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 209 already retrieved (0kB/88ms)
[2025-05-01T07:06:41.443+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:41 INFO IsolatedClientLoader: Downloaded metastore jars to /tmp/hive-v3_1-33cdd18c-d2ba-448c-bab0-77006b0f5449
[2025-05-01T07:06:42.297+0000] {spark_submit.py:649} INFO - Hive Session ID = c0a428b8-b1cc-4910-ae38-bca2d4b93a6f
[2025-05-01T07:06:42.408+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:42 INFO HiveClientImpl: Warehouse location for Hive client (version 3.1.3) is file:/opt/***/spark-warehouse
[2025-05-01T07:06:43.697+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-05-01T07:06:43.728+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:43 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-05-01T07:06:43.729+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:43 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-05-01T07:06:45.845+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:45 INFO InMemoryFileIndex: It took 161 ms to list leaf files for 1 paths.
[2025-05-01T07:06:47.946+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:47 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
[2025-05-01T07:06:48.236+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:48 INFO DeltaLog: Loading version 0.
[2025-05-01T07:06:49.525+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:49 INFO Snapshot: [tableId=7a946dca-04b7-44b1-9f44-7b35b6f37aeb] Created snapshot Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log, version=0, metadata=Metadata(11c90e62-9f54-4b6f-a8f8-fd951bbfaf27,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746082677158)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json; isDirectory=false; length=6036; replication=1; blocksize=33554432; modification_time=1746082705648; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=333941ed0bf72a54fa20b8d8b3accc96 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@787d311d,1746082705648), checksumOpt=Some(VersionChecksum(Some(c0c22e8e-0189-4eed-aa06-603ed4ac338f),31301816,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(11c90e62-9f54-4b6f-a8f8-fd951bbfaf27,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746082677158)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-eb1c0524-495e-442e-a7d0-555f4587a31d-c000.snappy.parquet,Map(),31301816,1746082702000,false,{"numRecords":111602,"minValues":{"ID":"00011a5a-4cca-c00f-1fe2-a33ff195","CLAIMID":"0001da0a-1087-7b47-dc4e-c62c3df4","CHARGEID":0,"PATIENTID":"00732e11-5e4d-37b7-01f8-929a2553","TYPE":"CHARGE","AMOUNT":0.0,"METHOD":"CASH","FROMDATE":"1931-04-21T11:59:06.000Z","TODATE":"1931-04-21T12:19:24.000Z","PLACEOFSERVICE":"00ffb204-07b4-3c72-a678-fa252e6c","PROCEDURECODE":3,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":1,"NOTES":"0.25 ML Leuprolide Acetate 30 MG","UNITAMOUNT":0.0,"TRANSFEROUTID":1,"TRANSFERTYPE":"1","PAYMENTS":0.0,"ADJUSTMENTS":0.0,"TRANSFERS":0.01,"OUTSTANDING":0.0,"APPOINTMENTID":"0004591c-7253-3a60-3435-8bd8dd25","PATIENTINSURANCEID":"00732e11-5e4d-37b7-01f8-929a2553","FEESCHEDULEID":1,"PROVIDERID":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"ffffad8f-7a4a-8675-09fa-0ada11bd","CLAIMID":"ffff94bb-e888-c037-6e14-76215741","CHARGEID":143657,"PATIENTID":"fb164202-4e38-04a0-470a-b7229db1","TYPE":"TRANSFEROUT","AMOUNT":103958.4,"METHOD":"ECHECK","FROMDATE":"2024-11-04T08:21:17.000Z","TODATE":"2024-11-05T00:34:10.000Z","PLACEOFSERVICE":"fe30f2b4-9bc8-346e-afba-4455d176","PROCEDURECODE":456191000124101,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":20,"NOTES":"zoster vaccine  live","UNITAMOUNT":103958.4,"TRANSFEROUTID":143655,"TRANSFERTYPE":"p","PAYMENTS":91678.0,"ADJUSTMENTS":0.0,"TRANSFERS":31187.52,"OUTSTANDING":91678.0,"APPOINTMENTID":"fffa37f5-08f3-5c8a-09e0-a379d113","PATIENTINSURANCEID":"fb164202-4e38-04a0-470a-b7229db1","FEESCHEDULEID":1,"PROVIDERID":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"CLAIMID":0,"CHARGEID":0,"PATIENTID":0,"TYPE":0,"AMOUNT":60590,"METHOD":69967,"FROMDATE":0,"TODATE":0,"PLACEOFSERVICE":0,"PROCEDURECODE":0,"MODIFIER1":111602,"MODIFIER2":111602,"DIAGNOSISREF1":0,"DIAGNOSISREF2":74224,"DIAGNOSISREF3":101620,"DIAGNOSISREF4":108971,"UNITS":0,"DEPARTMENTID":0,"NOTES":0,"UNITAMOUNT":0,"TRANSFEROUTID":92647,"TRANSFERTYPE":60590,"PAYMENTS":0,"ADJUSTMENTS":0,"TRANSFERS":73692,"OUTSTANDING":0,"APPOINTMENTID":0,"LINENOTE":111602,"PATIENTINSURANCEID":7254,"FEESCHEDULEID":0,"PROVIDERID":0}},null,null,None,None,None))))))
[2025-05-01T07:06:49.553+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:49 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log, version=0, metadata=Metadata(11c90e62-9f54-4b6f-a8f8-fd951bbfaf27,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746082677158)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json; isDirectory=false; length=6036; replication=1; blocksize=33554432; modification_time=1746082705648; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=333941ed0bf72a54fa20b8d8b3accc96 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@787d311d,1746082705648), checksumOpt=Some(VersionChecksum(Some(c0c22e8e-0189-4eed-aa06-603ed4ac338f),31301816,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(11c90e62-9f54-4b6f-a8f8-fd951bbfaf27,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746082677158)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-eb1c0524-495e-442e-a7d0-555f4587a31d-c000.snappy.parquet,Map(),31301816,1746082702000,false,{"numRecords":111602,"minValues":{"ID":"00011a5a-4cca-c00f-1fe2-a33ff195","CLAIMID":"0001da0a-1087-7b47-dc4e-c62c3df4","CHARGEID":0,"PATIENTID":"00732e11-5e4d-37b7-01f8-929a2553","TYPE":"CHARGE","AMOUNT":0.0,"METHOD":"CASH","FROMDATE":"1931-04-21T11:59:06.000Z","TODATE":"1931-04-21T12:19:24.000Z","PLACEOFSERVICE":"00ffb204-07b4-3c72-a678-fa252e6c","PROCEDURECODE":3,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":1,"NOTES":"0.25 ML Leuprolide Acetate 30 MG","UNITAMOUNT":0.0,"TRANSFEROUTID":1,"TRANSFERTYPE":"1","PAYMENTS":0.0,"ADJUSTMENTS":0.0,"TRANSFERS":0.01,"OUTSTANDING":0.0,"APPOINTMENTID":"0004591c-7253-3a60-3435-8bd8dd25","PATIENTINSURANCEID":"00732e11-5e4d-37b7-01f8-929a2553","FEESCHEDULEID":1,"PROVIDERID":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"ffffad8f-7a4a-8675-09fa-0ada11bd","CLAIMID":"ffff94bb-e888-c037-6e14-76215741","CHARGEID":143657,"PATIENTID":"fb164202-4e38-04a0-470a-b7229db1","TYPE":"TRANSFEROUT","AMOUNT":103958.4,"METHOD":"ECHECK","FROMDATE":"2024-11-04T08:21:17.000Z","TODATE":"2024-11-05T00:34:10.000Z","PLACEOFSERVICE":"fe30f2b4-9bc8-346e-afba-4455d176","PROCEDURECODE":456191000124101,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":20,"NOTES":"zoster vaccine  live","UNITAMOUNT":103958.4,"TRANSFEROUTID":143655,"TRANSFERTYPE":"p","PAYMENTS":91678.0,"ADJUSTMENTS":0.0,"TRANSFERS":31187.52,"OUTSTANDING":91678.0,"APPOINTMENTID":"fffa37f5-08f3-5c8a-09e0-a379d113","PATIENTINSURANCEID":"fb164202-4e38-04a0-470a-b7229db1","FEESCHEDULEID":1,"PROVIDERID":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"CLAIMID":0,"CHARGEID":0,"PATIENTID":0,"TYPE":0,"AMOUNT":60590,"METHOD":69967,"FROMDATE":0,"TODATE":0,"PLACEOFSERVICE":0,"PROCEDURECODE":0,"MODIFIER1":111602,"MODIFIER2":111602,"DIAGNOSISREF1":0,"DIAGNOSISREF2":74224,"DIAGNOSISREF3":101620,"DIAGNOSISREF4":108971,"UNITS":0,"DEPARTMENTID":0,"NOTES":0,"UNITAMOUNT":0,"TRANSFEROUTID":92647,"TRANSFERTYPE":60590,"PAYMENTS":0,"ADJUSTMENTS":0,"TRANSFERS":73692,"OUTSTANDING":0,"APPOINTMENTID":0,"LINENOTE":111602,"PATIENTINSURANCEID":7254,"FEESCHEDULEID":0,"PROVIDERID":0}},null,null,None,None,None))))))
[2025-05-01T07:06:52.722+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:52 INFO MergeIntoCommand: DELTA: MERGE operation - materialize source
[2025-05-01T07:06:52.743+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:52 INFO MergeIntoCommand: DELTA: Done
[2025-05-01T07:06:52.747+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:52 INFO MergeIntoCommand: DELTA: MERGE operation - writing new files for only inserts
[2025-05-01T07:06:52.767+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:52 INFO Snapshot: DELTA: Compute snapshot for version: 0
[2025-05-01T07:06:52.918+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 204.7 KiB, free 434.2 MiB)
[2025-05-01T07:06:53.039+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 434.2 MiB)
[2025-05-01T07:06:53.045+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ***-scheduler:35203 (size: 35.8 KiB, free: 434.4 MiB)
[2025-05-01T07:06:53.054+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:53 INFO SparkContext: Created broadcast 0 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:06:54.704+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:54 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 6036)
[2025-05-01T07:06:56.649+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:56 INFO DataSourceStrategy: Pruning directories with:
[2025-05-01T07:06:56.654+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:56 INFO FileSourceStrategy: Pushed Filters:
[2025-05-01T07:06:56.657+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:56 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-01T07:06:56.805+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-05-01T07:06:57.893+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO CodeGenerator: Code generated in 724.713429 ms
[2025-05-01T07:06:57.900+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 205.0 KiB, free 434.0 MiB)
[2025-05-01T07:06:57.917+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.9 MiB)
[2025-05-01T07:06:57.919+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ***-scheduler:35203 (size: 35.9 KiB, free: 434.3 MiB)
[2025-05-01T07:06:57.922+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO SparkContext: Created broadcast 1 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:06:57.960+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4200340 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-01T07:06:58.401+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Registering RDD 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 0
[2025-05-01T07:06:58.412+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Got map stage job 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-01T07:06:58.413+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Final stage: ShuffleMapStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:06:58.415+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Parents of final stage: List()
[2025-05-01T07:06:58.416+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:06:58.422+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:06:58.572+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 105.9 KiB, free 433.8 MiB)
[2025-05-01T07:06:58.582+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.8 MiB)
[2025-05-01T07:06:58.584+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ***-scheduler:35203 (size: 32.6 KiB, free: 434.3 MiB)
[2025-05-01T07:06:58.585+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:06:58.623+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-01T07:06:58.626+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-01T07:06:58.741+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10901 bytes)
[2025-05-01T07:06:58.768+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-01T07:06:59.474+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:59 INFO CodeGenerator: Code generated in 377.564841 ms
[2025-05-01T07:06:59.577+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:59 INFO CodeGenerator: Code generated in 46.147289 ms
[2025-05-01T07:06:59.622+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:59 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json, range: 0-6036, partition values: [0]
[2025-05-01T07:06:59.873+0000] {spark_submit.py:649} INFO - 25/05/01 07:06:59 INFO CodeGenerator: Code generated in 197.803657 ms
[2025-05-01T07:07:00.226+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO CodeGenerator: Code generated in 18.352687 ms
[2025-05-01T07:07:00.267+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO CodeGenerator: Code generated in 22.797881 ms
[2025-05-01T07:07:00.373+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1884 bytes result sent to driver
[2025-05-01T07:07:00.401+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1690 ms on ***-scheduler (executor driver) (1/1)
[2025-05-01T07:07:00.406+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-01T07:07:00.423+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO DAGScheduler: ShuffleMapStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.969 s
[2025-05-01T07:07:00.426+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO DAGScheduler: looking for newly runnable stages
[2025-05-01T07:07:00.427+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO DAGScheduler: running: Set()
[2025-05-01T07:07:00.429+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO DAGScheduler: waiting: Set()
[2025-05-01T07:07:00.430+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO DAGScheduler: failed: Set()
[2025-05-01T07:07:00.707+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ***-scheduler:35203 in memory (size: 32.6 KiB, free: 434.3 MiB)
[2025-05-01T07:07:05.134+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:05 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-01T07:07:05.136+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:05 INFO CodeGenerator: Code generated in 912.754726 ms
[2025-05-01T07:07:05.360+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:05 INFO CodeGenerator: Code generated in 167.994939 ms
[2025-05-01T07:07:06.323+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO CodeGenerator: Code generated in 139.684054 ms
[2025-05-01T07:07:06.340+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Registering RDD 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2025-05-01T07:07:06.342+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Got map stage job 1 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-01T07:07:06.343+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Final stage: ShuffleMapStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:07:06.344+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-05-01T07:07:06.346+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:06.347+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:07:06.466+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 605.6 KiB, free 433.3 MiB)
[2025-05-01T07:07:06.472+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 138.8 KiB, free 433.2 MiB)
[2025-05-01T07:07:06.474+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ***-scheduler:35203 (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-01T07:07:06.475+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:06.476+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-01T07:07:06.476+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 50 tasks resource profile 0
[2025-05-01T07:07:06.485+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 1) (***-scheduler, executor driver, partition 34, NODE_LOCAL, 10195 bytes)
[2025-05-01T07:07:06.486+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO Executor: Running task 34.0 in stage 2.0 (TID 1)
[2025-05-01T07:07:06.661+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:06.666+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
[2025-05-01T07:07:06.788+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO CodeGenerator: Code generated in 114.164844 ms
[2025-05-01T07:07:06.816+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO CodeGenerator: Code generated in 16.327844 ms
[2025-05-01T07:07:06.845+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:06 INFO CodeGenerator: Code generated in 12.953179 ms
[2025-05-01T07:07:07.199+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO CodeGenerator: Code generated in 179.281239 ms
[2025-05-01T07:07:07.748+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-01T07:07:07.750+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO CodeGenerator: Code generated in 509.92867 ms
[2025-05-01T07:07:07.772+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO MemoryStore: Block rdd_10_34 stored as values in memory (estimated size 1616.0 B, free 433.2 MiB)
[2025-05-01T07:07:07.773+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO BlockManagerInfo: Added rdd_10_34 in memory on ***-scheduler:35203 (size: 1616.0 B, free: 434.2 MiB)
[2025-05-01T07:07:07.909+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO CodeGenerator: Code generated in 128.141494 ms
[2025-05-01T07:07:07.955+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:07 INFO CodeGenerator: Code generated in 8.643004 ms
[2025-05-01T07:07:08.032+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 26.323539 ms
[2025-05-01T07:07:08.047+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 10.07166 ms
[2025-05-01T07:07:08.088+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 29.56798 ms
[2025-05-01T07:07:08.119+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 11.206509 ms
[2025-05-01T07:07:08.140+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 8.952983 ms
[2025-05-01T07:07:08.154+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO CodeGenerator: Code generated in 9.931497 ms
[2025-05-01T07:07:08.162+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Finished task 34.0 in stage 2.0 (TID 1). 5308 bytes result sent to driver
[2025-05-01T07:07:08.164+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 2) (***-scheduler, executor driver, partition 42, NODE_LOCAL, 10195 bytes)
[2025-05-01T07:07:08.165+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Running task 42.0 in stage 2.0 (TID 2)
[2025-05-01T07:07:08.166+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 1) in 1684 ms on ***-scheduler (executor driver) (1/50)
[2025-05-01T07:07:08.217+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:08.218+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:08.385+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO MemoryStore: Block rdd_10_42 stored as values in memory (estimated size 872.0 B, free 433.2 MiB)
[2025-05-01T07:07:08.387+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO BlockManagerInfo: Added rdd_10_42 in memory on ***-scheduler:35203 (size: 872.0 B, free: 434.2 MiB)
[2025-05-01T07:07:08.456+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Finished task 42.0 in stage 2.0 (TID 2). 5308 bytes result sent to driver
[2025-05-01T07:07:08.460+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:08.461+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-05-01T07:07:08.462+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 2) in 297 ms on ***-scheduler (executor driver) (2/50)
[2025-05-01T07:07:08.520+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:08.522+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:08.626+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO MemoryStore: Block rdd_10_0 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:08.627+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO BlockManagerInfo: Added rdd_10_0 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:08.702+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 5308 bytes result sent to driver
[2025-05-01T07:07:08.713+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:08.717+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-05-01T07:07:08.718+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 248 ms on ***-scheduler (executor driver) (3/50)
[2025-05-01T07:07:08.761+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:08.763+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:08.852+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO MemoryStore: Block rdd_10_1 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:08.854+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO BlockManagerInfo: Added rdd_10_1 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:08.912+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 5308 bytes result sent to driver
[2025-05-01T07:07:08.916+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:08.919+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-05-01T07:07:08.920+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 212 ms on ***-scheduler (executor driver) (4/50)
[2025-05-01T07:07:08.962+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:08.963+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:09.025+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_2 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.026+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_2 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.068+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 5308 bytes result sent to driver
[2025-05-01T07:07:09.070+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.071+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-05-01T07:07:09.072+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 158 ms on ***-scheduler (executor driver) (5/50)
[2025-05-01T07:07:09.095+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.096+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:09.169+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_3 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.171+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_3 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.253+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 5394 bytes result sent to driver
[2025-05-01T07:07:09.256+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.257+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-05-01T07:07:09.260+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 188 ms on ***-scheduler (executor driver) (6/50)
[2025-05-01T07:07:09.298+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.300+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:09.399+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_4 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.400+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_4 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.461+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 5308 bytes result sent to driver
[2025-05-01T07:07:09.463+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.464+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 210 ms on ***-scheduler (executor driver) (7/50)
[2025-05-01T07:07:09.466+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-05-01T07:07:09.498+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.499+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:09.561+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_5 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.562+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_5 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.612+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 5308 bytes result sent to driver
[2025-05-01T07:07:09.614+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.615+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-05-01T07:07:09.616+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 153 ms on ***-scheduler (executor driver) (8/50)
[2025-05-01T07:07:09.643+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.644+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:09.699+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_6 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.701+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_6 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.752+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 5308 bytes result sent to driver
[2025-05-01T07:07:09.754+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.756+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-05-01T07:07:09.757+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 142 ms on ***-scheduler (executor driver) (9/50)
[2025-05-01T07:07:09.781+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.782+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:09.859+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO MemoryStore: Block rdd_10_7 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:09.860+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO BlockManagerInfo: Added rdd_10_7 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:09.918+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 5308 bytes result sent to driver
[2025-05-01T07:07:09.920+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:09.922+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 169 ms on ***-scheduler (executor driver) (10/50)
[2025-05-01T07:07:09.924+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-05-01T07:07:09.972+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:09.973+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2025-05-01T07:07:10.050+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_8 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.052+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_8 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.107+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 5308 bytes result sent to driver
[2025-05-01T07:07:10.109+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.111+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-05-01T07:07:10.112+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 191 ms on ***-scheduler (executor driver) (11/50)
[2025-05-01T07:07:10.141+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:10.143+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:10.222+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_9 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.223+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_9 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.287+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 5308 bytes result sent to driver
[2025-05-01T07:07:10.289+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.290+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 181 ms on ***-scheduler (executor driver) (12/50)
[2025-05-01T07:07:10.292+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-05-01T07:07:10.330+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:10.331+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:10.395+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_10 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.396+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_10 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.444+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 5308 bytes result sent to driver
[2025-05-01T07:07:10.446+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.447+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-05-01T07:07:10.449+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 159 ms on ***-scheduler (executor driver) (13/50)
[2025-05-01T07:07:10.483+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:10.485+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:10.554+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_11 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.555+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_11 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.613+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 5308 bytes result sent to driver
[2025-05-01T07:07:10.616+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 15) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.617+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 12.0 in stage 2.0 (TID 15)
[2025-05-01T07:07:10.619+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 171 ms on ***-scheduler (executor driver) (14/50)
[2025-05-01T07:07:10.659+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:10.660+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:10.744+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_12 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.746+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_12 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.807+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 12.0 in stage 2.0 (TID 15). 5308 bytes result sent to driver
[2025-05-01T07:07:10.809+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 16) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.810+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 13.0 in stage 2.0 (TID 16)
[2025-05-01T07:07:10.812+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 15) in 196 ms on ***-scheduler (executor driver) (15/50)
[2025-05-01T07:07:10.847+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:10.848+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:10.920+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO MemoryStore: Block rdd_10_13 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:10.921+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO BlockManagerInfo: Added rdd_10_13 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:10.970+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Finished task 13.0 in stage 2.0 (TID 16). 5308 bytes result sent to driver
[2025-05-01T07:07:10.972+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 17) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:10.973+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 16) in 165 ms on ***-scheduler (executor driver) (16/50)
[2025-05-01T07:07:10.975+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:10 INFO Executor: Running task 14.0 in stage 2.0 (TID 17)
[2025-05-01T07:07:11.003+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.004+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:11.061+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_14 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.062+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_14 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.099+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 14.0 in stage 2.0 (TID 17). 5308 bytes result sent to driver
[2025-05-01T07:07:11.100+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 18) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.102+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 17) in 130 ms on ***-scheduler (executor driver) (17/50)
[2025-05-01T07:07:11.102+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 15.0 in stage 2.0 (TID 18)
[2025-05-01T07:07:11.126+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.128+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:11.189+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_15 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.191+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_15 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.230+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 15.0 in stage 2.0 (TID 18). 5351 bytes result sent to driver
[2025-05-01T07:07:11.232+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 19) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.233+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 18) in 133 ms on ***-scheduler (executor driver) (18/50)
[2025-05-01T07:07:11.234+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 16.0 in stage 2.0 (TID 19)
[2025-05-01T07:07:11.263+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.264+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:11.339+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_16 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.341+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_16 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.382+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 16.0 in stage 2.0 (TID 19). 5351 bytes result sent to driver
[2025-05-01T07:07:11.383+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 20) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.384+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 19) in 153 ms on ***-scheduler (executor driver) (19/50)
[2025-05-01T07:07:11.385+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 17.0 in stage 2.0 (TID 20)
[2025-05-01T07:07:11.409+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.410+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:11.491+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_17 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.492+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_17 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.541+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 17.0 in stage 2.0 (TID 20). 5308 bytes result sent to driver
[2025-05-01T07:07:11.543+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 21) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.544+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 18.0 in stage 2.0 (TID 21)
[2025-05-01T07:07:11.545+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 20) in 160 ms on ***-scheduler (executor driver) (20/50)
[2025-05-01T07:07:11.573+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.575+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:11.638+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_18 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.639+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_18 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.690+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 18.0 in stage 2.0 (TID 21). 5394 bytes result sent to driver
[2025-05-01T07:07:11.692+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 22) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.694+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 21) in 151 ms on ***-scheduler (executor driver) (21/50)
[2025-05-01T07:07:11.695+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 19.0 in stage 2.0 (TID 22)
[2025-05-01T07:07:11.732+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.733+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:11.801+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_19 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.802+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_19 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.853+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 19.0 in stage 2.0 (TID 22). 5351 bytes result sent to driver
[2025-05-01T07:07:11.855+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 23) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.856+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 20.0 in stage 2.0 (TID 23)
[2025-05-01T07:07:11.858+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 22) in 165 ms on ***-scheduler (executor driver) (22/50)
[2025-05-01T07:07:11.880+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.881+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:11.930+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO MemoryStore: Block rdd_10_20 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:11.931+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO BlockManagerInfo: Added rdd_10_20 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:11.963+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Finished task 20.0 in stage 2.0 (TID 23). 5308 bytes result sent to driver
[2025-05-01T07:07:11.965+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 24) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:11.967+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 23) in 112 ms on ***-scheduler (executor driver) (23/50)
[2025-05-01T07:07:11.968+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO Executor: Running task 21.0 in stage 2.0 (TID 24)
[2025-05-01T07:07:11.986+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:11.987+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.051+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_21 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.052+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_21 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.090+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 21.0 in stage 2.0 (TID 24). 5308 bytes result sent to driver
[2025-05-01T07:07:12.092+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 25) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.093+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 22.0 in stage 2.0 (TID 25)
[2025-05-01T07:07:12.094+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 24) in 128 ms on ***-scheduler (executor driver) (24/50)
[2025-05-01T07:07:12.112+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.113+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.165+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_22 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.166+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_22 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.199+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 22.0 in stage 2.0 (TID 25). 5308 bytes result sent to driver
[2025-05-01T07:07:12.201+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 26) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.202+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 23.0 in stage 2.0 (TID 26)
[2025-05-01T07:07:12.203+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 25) in 111 ms on ***-scheduler (executor driver) (25/50)
[2025-05-01T07:07:12.221+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.222+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.266+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_23 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.267+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_23 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.297+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 23.0 in stage 2.0 (TID 26). 5308 bytes result sent to driver
[2025-05-01T07:07:12.299+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 27) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.300+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 26) in 99 ms on ***-scheduler (executor driver) (26/50)
[2025-05-01T07:07:12.301+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 24.0 in stage 2.0 (TID 27)
[2025-05-01T07:07:12.318+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.319+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.362+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_24 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.363+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_24 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.401+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 24.0 in stage 2.0 (TID 27). 5308 bytes result sent to driver
[2025-05-01T07:07:12.403+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 28) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.404+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 27) in 105 ms on ***-scheduler (executor driver) (27/50)
[2025-05-01T07:07:12.405+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 25.0 in stage 2.0 (TID 28)
[2025-05-01T07:07:12.422+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.423+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.470+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_25 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.472+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_25 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.502+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 25.0 in stage 2.0 (TID 28). 5308 bytes result sent to driver
[2025-05-01T07:07:12.504+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 29) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.505+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 26.0 in stage 2.0 (TID 29)
[2025-05-01T07:07:12.506+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 28) in 102 ms on ***-scheduler (executor driver) (28/50)
[2025-05-01T07:07:12.530+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.531+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.573+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_26 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.575+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_26 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.602+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 26.0 in stage 2.0 (TID 29). 5308 bytes result sent to driver
[2025-05-01T07:07:12.604+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 30) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.606+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 27.0 in stage 2.0 (TID 30)
[2025-05-01T07:07:12.607+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 29) in 102 ms on ***-scheduler (executor driver) (29/50)
[2025-05-01T07:07:12.626+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.627+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.689+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_27 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.690+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_27 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.730+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 27.0 in stage 2.0 (TID 30). 5308 bytes result sent to driver
[2025-05-01T07:07:12.733+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 31) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.734+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 28.0 in stage 2.0 (TID 31)
[2025-05-01T07:07:12.735+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 30) in 130 ms on ***-scheduler (executor driver) (30/50)
[2025-05-01T07:07:12.758+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.760+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.816+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_28 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.818+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_28 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.857+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 28.0 in stage 2.0 (TID 31). 5308 bytes result sent to driver
[2025-05-01T07:07:12.860+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 32) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.861+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 31) in 128 ms on ***-scheduler (executor driver) (31/50)
[2025-05-01T07:07:12.862+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 29.0 in stage 2.0 (TID 32)
[2025-05-01T07:07:12.887+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:12.889+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:12.939+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO MemoryStore: Block rdd_10_29 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:12.940+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO BlockManagerInfo: Added rdd_10_29 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:12.983+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Finished task 29.0 in stage 2.0 (TID 32). 5351 bytes result sent to driver
[2025-05-01T07:07:12.985+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 33) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:12.986+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO Executor: Running task 30.0 in stage 2.0 (TID 33)
[2025-05-01T07:07:12.987+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:12 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 32) in 127 ms on ***-scheduler (executor driver) (32/50)
[2025-05-01T07:07:13.010+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.012+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:13.072+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_30 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.073+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_30 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.118+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 30.0 in stage 2.0 (TID 33). 5351 bytes result sent to driver
[2025-05-01T07:07:13.119+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 34) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.121+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 31.0 in stage 2.0 (TID 34)
[2025-05-01T07:07:13.122+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 33) in 136 ms on ***-scheduler (executor driver) (33/50)
[2025-05-01T07:07:13.144+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.145+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.192+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_31 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.193+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_31 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.223+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 31.0 in stage 2.0 (TID 34). 5308 bytes result sent to driver
[2025-05-01T07:07:13.225+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 35) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.226+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 34) in 106 ms on ***-scheduler (executor driver) (34/50)
[2025-05-01T07:07:13.227+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 32.0 in stage 2.0 (TID 35)
[2025-05-01T07:07:13.245+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.246+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.292+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_32 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.295+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_32 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.325+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 32.0 in stage 2.0 (TID 35). 5308 bytes result sent to driver
[2025-05-01T07:07:13.327+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 36) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.328+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 33.0 in stage 2.0 (TID 36)
[2025-05-01T07:07:13.329+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 35) in 103 ms on ***-scheduler (executor driver) (35/50)
[2025-05-01T07:07:13.349+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.350+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.394+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_33 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.395+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_33 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.432+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 33.0 in stage 2.0 (TID 36). 5308 bytes result sent to driver
[2025-05-01T07:07:13.433+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 37) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.434+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 36) in 108 ms on ***-scheduler (executor driver) (36/50)
[2025-05-01T07:07:13.435+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 35.0 in stage 2.0 (TID 37)
[2025-05-01T07:07:13.454+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.455+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.508+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_35 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.510+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_35 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.546+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 35.0 in stage 2.0 (TID 37). 5308 bytes result sent to driver
[2025-05-01T07:07:13.548+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 38) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.549+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 36.0 in stage 2.0 (TID 38)
[2025-05-01T07:07:13.550+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 37) in 116 ms on ***-scheduler (executor driver) (37/50)
[2025-05-01T07:07:13.576+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.577+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.631+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_36 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.632+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_36 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.674+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 36.0 in stage 2.0 (TID 38). 5308 bytes result sent to driver
[2025-05-01T07:07:13.675+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 39) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.676+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 37.0 in stage 2.0 (TID 39)
[2025-05-01T07:07:13.678+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 38) in 129 ms on ***-scheduler (executor driver) (38/50)
[2025-05-01T07:07:13.699+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.700+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.753+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_37 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.755+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_37 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.785+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 37.0 in stage 2.0 (TID 39). 5308 bytes result sent to driver
[2025-05-01T07:07:13.786+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 40) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.787+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 38.0 in stage 2.0 (TID 40)
[2025-05-01T07:07:13.788+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 39) in 112 ms on ***-scheduler (executor driver) (39/50)
[2025-05-01T07:07:13.808+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.809+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.850+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_38 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.851+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_38 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.881+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 38.0 in stage 2.0 (TID 40). 5308 bytes result sent to driver
[2025-05-01T07:07:13.882+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 41) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.883+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 40) in 97 ms on ***-scheduler (executor driver) (40/50)
[2025-05-01T07:07:13.884+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 39.0 in stage 2.0 (TID 41)
[2025-05-01T07:07:13.901+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.902+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:13.941+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO MemoryStore: Block rdd_10_39 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:13.942+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO BlockManagerInfo: Added rdd_10_39 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:13.978+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Finished task 39.0 in stage 2.0 (TID 41). 5308 bytes result sent to driver
[2025-05-01T07:07:13.980+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 42) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:13.981+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO Executor: Running task 40.0 in stage 2.0 (TID 42)
[2025-05-01T07:07:13.981+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 41) in 98 ms on ***-scheduler (executor driver) (41/50)
[2025-05-01T07:07:13.996+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:13.997+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.056+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_40 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.057+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_40 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.100+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 40.0 in stage 2.0 (TID 42). 5308 bytes result sent to driver
[2025-05-01T07:07:14.102+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 43) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.103+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 42) in 124 ms on ***-scheduler (executor driver) (42/50)
[2025-05-01T07:07:14.105+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 41.0 in stage 2.0 (TID 43)
[2025-05-01T07:07:14.130+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.132+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.192+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_41 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.193+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_41 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.251+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 41.0 in stage 2.0 (TID 43). 5351 bytes result sent to driver
[2025-05-01T07:07:14.254+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 44) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.256+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 43.0 in stage 2.0 (TID 44)
[2025-05-01T07:07:14.258+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 43) in 154 ms on ***-scheduler (executor driver) (43/50)
[2025-05-01T07:07:14.286+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.288+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:14.387+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_43 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.389+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_43 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.430+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 43.0 in stage 2.0 (TID 44). 5308 bytes result sent to driver
[2025-05-01T07:07:14.432+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 45) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.433+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 44.0 in stage 2.0 (TID 45)
[2025-05-01T07:07:14.435+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 44) in 179 ms on ***-scheduler (executor driver) (44/50)
[2025-05-01T07:07:14.455+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.456+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.513+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_44 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.514+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_44 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.549+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 44.0 in stage 2.0 (TID 45). 5308 bytes result sent to driver
[2025-05-01T07:07:14.551+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 46) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.552+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 45.0 in stage 2.0 (TID 46)
[2025-05-01T07:07:14.553+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 45) in 120 ms on ***-scheduler (executor driver) (45/50)
[2025-05-01T07:07:14.570+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.571+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.618+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_45 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.619+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_45 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.650+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 45.0 in stage 2.0 (TID 46). 5308 bytes result sent to driver
[2025-05-01T07:07:14.651+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 47) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.652+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 46.0 in stage 2.0 (TID 47)
[2025-05-01T07:07:14.653+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 46) in 102 ms on ***-scheduler (executor driver) (46/50)
[2025-05-01T07:07:14.670+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.671+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.727+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_46 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.728+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_46 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.757+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 46.0 in stage 2.0 (TID 47). 5308 bytes result sent to driver
[2025-05-01T07:07:14.759+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 48) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.760+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 47) in 108 ms on ***-scheduler (executor driver) (47/50)
[2025-05-01T07:07:14.761+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 47.0 in stage 2.0 (TID 48)
[2025-05-01T07:07:14.776+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.778+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.819+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_47 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.820+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_47 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.848+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 47.0 in stage 2.0 (TID 48). 5308 bytes result sent to driver
[2025-05-01T07:07:14.849+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 49) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.850+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 48.0 in stage 2.0 (TID 49)
[2025-05-01T07:07:14.851+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 48) in 92 ms on ***-scheduler (executor driver) (48/50)
[2025-05-01T07:07:14.866+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.867+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:14.909+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO MemoryStore: Block rdd_10_48 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:14.910+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO BlockManagerInfo: Added rdd_10_48 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:14.940+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Finished task 48.0 in stage 2.0 (TID 49). 5308 bytes result sent to driver
[2025-05-01T07:07:14.942+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 50) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10195 bytes)
[2025-05-01T07:07:14.943+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO Executor: Running task 49.0 in stage 2.0 (TID 50)
[2025-05-01T07:07:14.944+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 49) in 94 ms on ***-scheduler (executor driver) (49/50)
[2025-05-01T07:07:14.959+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:14.960+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-01T07:07:15.007+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO MemoryStore: Block rdd_10_49 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-01T07:07:15.008+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO BlockManagerInfo: Added rdd_10_49 in memory on ***-scheduler:35203 (size: 46.0 B, free: 434.2 MiB)
[2025-05-01T07:07:15.049+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO Executor: Finished task 49.0 in stage 2.0 (TID 50). 5308 bytes result sent to driver
[2025-05-01T07:07:15.051+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 50) in 110 ms on ***-scheduler (executor driver) (50/50)
[2025-05-01T07:07:15.052+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-01T07:07:15.054+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: ShuffleMapStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 8.678 s
[2025-05-01T07:07:15.055+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: looking for newly runnable stages
[2025-05-01T07:07:15.056+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: running: Set()
[2025-05-01T07:07:15.057+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: waiting: Set()
[2025-05-01T07:07:15.058+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: failed: Set()
[2025-05-01T07:07:15.140+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:07:15.147+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Got job 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-01T07:07:15.149+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:07:15.150+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2025-05-01T07:07:15.151+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:15.152+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:07:15.170+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 537.2 KiB, free 432.7 MiB)
[2025-05-01T07:07:15.175+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 125.4 KiB, free 432.6 MiB)
[2025-05-01T07:07:15.176+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ***-scheduler:35203 (size: 125.4 KiB, free: 434.1 MiB)
[2025-05-01T07:07:15.177+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:15.179+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-01T07:07:15.180+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-05-01T07:07:15.183+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 51) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-01T07:07:15.185+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 51)
[2025-05-01T07:07:15.239+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO ShuffleBlockFetcherIterator: Getting 50 (5.0 KiB) non-empty blocks including 50 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:15.240+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-01T07:07:15.287+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO CodeGenerator: Code generated in 8.667364 ms
[2025-05-01T07:07:15.324+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO CodeGenerator: Code generated in 26.897249 ms
[2025-05-01T07:07:15.370+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 51). 7311 bytes result sent to driver
[2025-05-01T07:07:15.373+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 51) in 191 ms on ***-scheduler (executor driver) (1/1)
[2025-05-01T07:07:15.374+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-05-01T07:07:15.375+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: ResultStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.218 s
[2025-05-01T07:07:15.376+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-01T07:07:15.377+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-05-01T07:07:15.379+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO DAGScheduler: Job 2 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.238919 s
[2025-05-01T07:07:15.451+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO CodeGenerator: Code generated in 47.280528 ms
[2025-05-01T07:07:15.454+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO Snapshot: DELTA: Done
[2025-05-01T07:07:15.492+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ***-scheduler:35203 in memory (size: 125.4 KiB, free: 434.2 MiB)
[2025-05-01T07:07:16.135+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO CodeGenerator: Code generated in 219.575045 ms
[2025-05-01T07:07:16.191+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:07:16.195+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Got job 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-01T07:07:16.196+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:07:16.197+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-05-01T07:07:16.198+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:16.199+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[18] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:07:16.214+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 687.2 KiB, free 432.5 MiB)
[2025-05-01T07:07:16.219+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 155.3 KiB, free 432.4 MiB)
[2025-05-01T07:07:16.220+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ***-scheduler:35203 (size: 155.3 KiB, free: 434.0 MiB)
[2025-05-01T07:07:16.221+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:16.223+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-01T07:07:16.224+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 50 tasks resource profile 0
[2025-05-01T07:07:16.228+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 52) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.230+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 52)
[2025-05-01T07:07:16.257+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_0 locally
[2025-05-01T07:07:16.441+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO CodeGenerator: Code generated in 182.174021 ms
[2025-05-01T07:07:16.450+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 0.0 in stage 7.0 (TID 52). 4181 bytes result sent to driver
[2025-05-01T07:07:16.451+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 53) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.452+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 52) in 224 ms on ***-scheduler (executor driver) (1/50)
[2025-05-01T07:07:16.453+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 1.0 in stage 7.0 (TID 53)
[2025-05-01T07:07:16.469+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_1 locally
[2025-05-01T07:07:16.472+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 1.0 in stage 7.0 (TID 53). 4181 bytes result sent to driver
[2025-05-01T07:07:16.474+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 54) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.475+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 2.0 in stage 7.0 (TID 54)
[2025-05-01T07:07:16.476+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 53) in 24 ms on ***-scheduler (executor driver) (2/50)
[2025-05-01T07:07:16.490+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_2 locally
[2025-05-01T07:07:16.493+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 2.0 in stage 7.0 (TID 54). 4181 bytes result sent to driver
[2025-05-01T07:07:16.495+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 55) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.496+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 54) in 23 ms on ***-scheduler (executor driver) (3/50)
[2025-05-01T07:07:16.497+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 3.0 in stage 7.0 (TID 55)
[2025-05-01T07:07:16.512+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_3 locally
[2025-05-01T07:07:16.515+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 3.0 in stage 7.0 (TID 55). 4181 bytes result sent to driver
[2025-05-01T07:07:16.516+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 56) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.517+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 4.0 in stage 7.0 (TID 56)
[2025-05-01T07:07:16.518+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 55) in 23 ms on ***-scheduler (executor driver) (4/50)
[2025-05-01T07:07:16.534+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_4 locally
[2025-05-01T07:07:16.538+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 4.0 in stage 7.0 (TID 56). 4181 bytes result sent to driver
[2025-05-01T07:07:16.539+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 57) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.540+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 5.0 in stage 7.0 (TID 57)
[2025-05-01T07:07:16.541+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 56) in 24 ms on ***-scheduler (executor driver) (5/50)
[2025-05-01T07:07:16.554+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_5 locally
[2025-05-01T07:07:16.557+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 5.0 in stage 7.0 (TID 57). 4181 bytes result sent to driver
[2025-05-01T07:07:16.558+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 58) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.559+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 6.0 in stage 7.0 (TID 58)
[2025-05-01T07:07:16.560+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 57) in 21 ms on ***-scheduler (executor driver) (6/50)
[2025-05-01T07:07:16.574+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_6 locally
[2025-05-01T07:07:16.577+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 6.0 in stage 7.0 (TID 58). 4181 bytes result sent to driver
[2025-05-01T07:07:16.578+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 59) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.580+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 58) in 21 ms on ***-scheduler (executor driver) (7/50)
[2025-05-01T07:07:16.581+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 7.0 in stage 7.0 (TID 59)
[2025-05-01T07:07:16.601+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_7 locally
[2025-05-01T07:07:16.606+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 7.0 in stage 7.0 (TID 59). 4181 bytes result sent to driver
[2025-05-01T07:07:16.608+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 60) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.610+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 59) in 31 ms on ***-scheduler (executor driver) (8/50)
[2025-05-01T07:07:16.612+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 8.0 in stage 7.0 (TID 60)
[2025-05-01T07:07:16.631+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_8 locally
[2025-05-01T07:07:16.634+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 8.0 in stage 7.0 (TID 60). 4181 bytes result sent to driver
[2025-05-01T07:07:16.636+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 61) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.638+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 9.0 in stage 7.0 (TID 61)
[2025-05-01T07:07:16.639+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 60) in 30 ms on ***-scheduler (executor driver) (9/50)
[2025-05-01T07:07:16.658+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_9 locally
[2025-05-01T07:07:16.662+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 9.0 in stage 7.0 (TID 61). 4181 bytes result sent to driver
[2025-05-01T07:07:16.664+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 62) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.666+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 61) in 30 ms on ***-scheduler (executor driver) (10/50)
[2025-05-01T07:07:16.667+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 10.0 in stage 7.0 (TID 62)
[2025-05-01T07:07:16.688+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_10 locally
[2025-05-01T07:07:16.693+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 10.0 in stage 7.0 (TID 62). 4181 bytes result sent to driver
[2025-05-01T07:07:16.695+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 63) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.696+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 11.0 in stage 7.0 (TID 63)
[2025-05-01T07:07:16.698+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 62) in 33 ms on ***-scheduler (executor driver) (11/50)
[2025-05-01T07:07:16.718+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_11 locally
[2025-05-01T07:07:16.723+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 11.0 in stage 7.0 (TID 63). 4181 bytes result sent to driver
[2025-05-01T07:07:16.725+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 64) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.727+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 12.0 in stage 7.0 (TID 64)
[2025-05-01T07:07:16.728+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 63) in 32 ms on ***-scheduler (executor driver) (12/50)
[2025-05-01T07:07:16.749+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_12 locally
[2025-05-01T07:07:16.752+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 12.0 in stage 7.0 (TID 64). 4181 bytes result sent to driver
[2025-05-01T07:07:16.754+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 65) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.755+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 13.0 in stage 7.0 (TID 65)
[2025-05-01T07:07:16.756+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 64) in 31 ms on ***-scheduler (executor driver) (13/50)
[2025-05-01T07:07:16.792+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_13 locally
[2025-05-01T07:07:16.810+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 13.0 in stage 7.0 (TID 65). 4224 bytes result sent to driver
[2025-05-01T07:07:16.812+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 66) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.814+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 14.0 in stage 7.0 (TID 66)
[2025-05-01T07:07:16.815+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 65) in 61 ms on ***-scheduler (executor driver) (14/50)
[2025-05-01T07:07:16.841+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_14 locally
[2025-05-01T07:07:16.846+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 14.0 in stage 7.0 (TID 66). 4181 bytes result sent to driver
[2025-05-01T07:07:16.847+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 67) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.849+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 66) in 38 ms on ***-scheduler (executor driver) (15/50)
[2025-05-01T07:07:16.852+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 15.0 in stage 7.0 (TID 67)
[2025-05-01T07:07:16.875+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_15 locally
[2025-05-01T07:07:16.880+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 15.0 in stage 7.0 (TID 67). 4181 bytes result sent to driver
[2025-05-01T07:07:16.882+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 68) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.884+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 16.0 in stage 7.0 (TID 68)
[2025-05-01T07:07:16.885+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 67) in 36 ms on ***-scheduler (executor driver) (16/50)
[2025-05-01T07:07:16.912+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_16 locally
[2025-05-01T07:07:16.916+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 16.0 in stage 7.0 (TID 68). 4181 bytes result sent to driver
[2025-05-01T07:07:16.918+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 69) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.920+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 17.0 in stage 7.0 (TID 69)
[2025-05-01T07:07:16.921+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 68) in 38 ms on ***-scheduler (executor driver) (17/50)
[2025-05-01T07:07:16.940+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_17 locally
[2025-05-01T07:07:16.958+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 17.0 in stage 7.0 (TID 69). 4224 bytes result sent to driver
[2025-05-01T07:07:16.960+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 70) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.962+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ***-scheduler:35203 in memory (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-01T07:07:16.963+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 69) in 44 ms on ***-scheduler (executor driver) (18/50)
[2025-05-01T07:07:16.964+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 18.0 in stage 7.0 (TID 70)
[2025-05-01T07:07:16.984+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO BlockManager: Found block rdd_10_18 locally
[2025-05-01T07:07:16.988+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Finished task 18.0 in stage 7.0 (TID 70). 4181 bytes result sent to driver
[2025-05-01T07:07:16.990+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 71) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:16.991+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 70) in 32 ms on ***-scheduler (executor driver) (19/50)
[2025-05-01T07:07:16.992+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:16 INFO Executor: Running task 19.0 in stage 7.0 (TID 71)
[2025-05-01T07:07:17.010+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_19 locally
[2025-05-01T07:07:17.014+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 19.0 in stage 7.0 (TID 71). 4181 bytes result sent to driver
[2025-05-01T07:07:17.016+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 72) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.017+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 71) in 28 ms on ***-scheduler (executor driver) (20/50)
[2025-05-01T07:07:17.018+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 20.0 in stage 7.0 (TID 72)
[2025-05-01T07:07:17.034+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_20 locally
[2025-05-01T07:07:17.038+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 20.0 in stage 7.0 (TID 72). 4181 bytes result sent to driver
[2025-05-01T07:07:17.039+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 73) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.040+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 21.0 in stage 7.0 (TID 73)
[2025-05-01T07:07:17.041+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 72) in 25 ms on ***-scheduler (executor driver) (21/50)
[2025-05-01T07:07:17.058+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_21 locally
[2025-05-01T07:07:17.062+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 21.0 in stage 7.0 (TID 73). 4181 bytes result sent to driver
[2025-05-01T07:07:17.063+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 74) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.065+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 22.0 in stage 7.0 (TID 74)
[2025-05-01T07:07:17.066+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 73) in 26 ms on ***-scheduler (executor driver) (22/50)
[2025-05-01T07:07:17.080+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_22 locally
[2025-05-01T07:07:17.083+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 22.0 in stage 7.0 (TID 74). 4181 bytes result sent to driver
[2025-05-01T07:07:17.085+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 75) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.086+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 23.0 in stage 7.0 (TID 75)
[2025-05-01T07:07:17.087+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 74) in 22 ms on ***-scheduler (executor driver) (23/50)
[2025-05-01T07:07:17.101+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_23 locally
[2025-05-01T07:07:17.105+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 23.0 in stage 7.0 (TID 75). 4181 bytes result sent to driver
[2025-05-01T07:07:17.106+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 76) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.108+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 24.0 in stage 7.0 (TID 76)
[2025-05-01T07:07:17.109+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 75) in 23 ms on ***-scheduler (executor driver) (24/50)
[2025-05-01T07:07:17.123+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_24 locally
[2025-05-01T07:07:17.126+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 24.0 in stage 7.0 (TID 76). 4181 bytes result sent to driver
[2025-05-01T07:07:17.127+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 77) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.128+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 25.0 in stage 7.0 (TID 77)
[2025-05-01T07:07:17.129+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 76) in 23 ms on ***-scheduler (executor driver) (25/50)
[2025-05-01T07:07:17.144+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_25 locally
[2025-05-01T07:07:17.147+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 25.0 in stage 7.0 (TID 77). 4181 bytes result sent to driver
[2025-05-01T07:07:17.148+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 78) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.150+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 26.0 in stage 7.0 (TID 78)
[2025-05-01T07:07:17.151+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 77) in 23 ms on ***-scheduler (executor driver) (26/50)
[2025-05-01T07:07:17.165+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_26 locally
[2025-05-01T07:07:17.170+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 26.0 in stage 7.0 (TID 78). 4181 bytes result sent to driver
[2025-05-01T07:07:17.171+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 79) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.172+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 27.0 in stage 7.0 (TID 79)
[2025-05-01T07:07:17.173+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 78) in 24 ms on ***-scheduler (executor driver) (27/50)
[2025-05-01T07:07:17.189+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_27 locally
[2025-05-01T07:07:17.192+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 27.0 in stage 7.0 (TID 79). 4181 bytes result sent to driver
[2025-05-01T07:07:17.193+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 80) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.194+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 28.0 in stage 7.0 (TID 80)
[2025-05-01T07:07:17.196+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 79) in 24 ms on ***-scheduler (executor driver) (28/50)
[2025-05-01T07:07:17.210+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_28 locally
[2025-05-01T07:07:17.214+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 28.0 in stage 7.0 (TID 80). 4181 bytes result sent to driver
[2025-05-01T07:07:17.215+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 81) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.216+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 29.0 in stage 7.0 (TID 81)
[2025-05-01T07:07:17.217+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 80) in 23 ms on ***-scheduler (executor driver) (29/50)
[2025-05-01T07:07:17.232+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_29 locally
[2025-05-01T07:07:17.236+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 29.0 in stage 7.0 (TID 81). 4181 bytes result sent to driver
[2025-05-01T07:07:17.239+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 82) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.241+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 30.0 in stage 7.0 (TID 82)
[2025-05-01T07:07:17.242+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 81) in 26 ms on ***-scheduler (executor driver) (30/50)
[2025-05-01T07:07:17.260+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_30 locally
[2025-05-01T07:07:17.264+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 30.0 in stage 7.0 (TID 82). 4181 bytes result sent to driver
[2025-05-01T07:07:17.266+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 83) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.267+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 31.0 in stage 7.0 (TID 83)
[2025-05-01T07:07:17.268+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 82) in 30 ms on ***-scheduler (executor driver) (31/50)
[2025-05-01T07:07:17.286+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_31 locally
[2025-05-01T07:07:17.290+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 31.0 in stage 7.0 (TID 83). 4181 bytes result sent to driver
[2025-05-01T07:07:17.292+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 84) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.293+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 32.0 in stage 7.0 (TID 84)
[2025-05-01T07:07:17.294+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 83) in 28 ms on ***-scheduler (executor driver) (32/50)
[2025-05-01T07:07:17.311+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_32 locally
[2025-05-01T07:07:17.315+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 32.0 in stage 7.0 (TID 84). 4181 bytes result sent to driver
[2025-05-01T07:07:17.318+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 85) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.319+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 33.0 in stage 7.0 (TID 85)
[2025-05-01T07:07:17.320+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 84) in 27 ms on ***-scheduler (executor driver) (33/50)
[2025-05-01T07:07:17.341+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_33 locally
[2025-05-01T07:07:17.344+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 33.0 in stage 7.0 (TID 85). 4181 bytes result sent to driver
[2025-05-01T07:07:17.346+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 86) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.348+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 34.0 in stage 7.0 (TID 86)
[2025-05-01T07:07:17.349+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 85) in 32 ms on ***-scheduler (executor driver) (34/50)
[2025-05-01T07:07:17.371+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_34 locally
[2025-05-01T07:07:17.378+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 34.0 in stage 7.0 (TID 86). 4349 bytes result sent to driver
[2025-05-01T07:07:17.380+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 87) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.381+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 35.0 in stage 7.0 (TID 87)
[2025-05-01T07:07:17.382+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 86) in 35 ms on ***-scheduler (executor driver) (35/50)
[2025-05-01T07:07:17.400+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_35 locally
[2025-05-01T07:07:17.405+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 35.0 in stage 7.0 (TID 87). 4181 bytes result sent to driver
[2025-05-01T07:07:17.407+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 88) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.409+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 87) in 29 ms on ***-scheduler (executor driver) (36/50)
[2025-05-01T07:07:17.410+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 36.0 in stage 7.0 (TID 88)
[2025-05-01T07:07:17.427+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_36 locally
[2025-05-01T07:07:17.431+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 36.0 in stage 7.0 (TID 88). 4181 bytes result sent to driver
[2025-05-01T07:07:17.432+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 89) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.434+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 37.0 in stage 7.0 (TID 89)
[2025-05-01T07:07:17.435+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 88) in 27 ms on ***-scheduler (executor driver) (37/50)
[2025-05-01T07:07:17.453+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_37 locally
[2025-05-01T07:07:17.457+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 37.0 in stage 7.0 (TID 89). 4181 bytes result sent to driver
[2025-05-01T07:07:17.458+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 90) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.459+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 38.0 in stage 7.0 (TID 90)
[2025-05-01T07:07:17.460+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 89) in 28 ms on ***-scheduler (executor driver) (38/50)
[2025-05-01T07:07:17.476+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_38 locally
[2025-05-01T07:07:17.480+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 38.0 in stage 7.0 (TID 90). 4181 bytes result sent to driver
[2025-05-01T07:07:17.481+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 91) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.484+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 39.0 in stage 7.0 (TID 91)
[2025-05-01T07:07:17.485+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 90) in 25 ms on ***-scheduler (executor driver) (39/50)
[2025-05-01T07:07:17.500+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_39 locally
[2025-05-01T07:07:17.503+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 39.0 in stage 7.0 (TID 91). 4181 bytes result sent to driver
[2025-05-01T07:07:17.504+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 92) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.506+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 40.0 in stage 7.0 (TID 92)
[2025-05-01T07:07:17.507+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 91) in 25 ms on ***-scheduler (executor driver) (40/50)
[2025-05-01T07:07:17.523+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_40 locally
[2025-05-01T07:07:17.527+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 40.0 in stage 7.0 (TID 92). 4181 bytes result sent to driver
[2025-05-01T07:07:17.528+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 93) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.530+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 41.0 in stage 7.0 (TID 93)
[2025-05-01T07:07:17.531+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 92) in 25 ms on ***-scheduler (executor driver) (41/50)
[2025-05-01T07:07:17.550+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_41 locally
[2025-05-01T07:07:17.553+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 41.0 in stage 7.0 (TID 93). 4181 bytes result sent to driver
[2025-05-01T07:07:17.555+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 94) (***-scheduler, executor driver, partition 42, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.556+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 42.0 in stage 7.0 (TID 94)
[2025-05-01T07:07:17.557+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 93) in 28 ms on ***-scheduler (executor driver) (42/50)
[2025-05-01T07:07:17.571+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_42 locally
[2025-05-01T07:07:17.574+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 42.0 in stage 7.0 (TID 94). 4224 bytes result sent to driver
[2025-05-01T07:07:17.575+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 95) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.577+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 43.0 in stage 7.0 (TID 95)
[2025-05-01T07:07:17.578+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 94) in 22 ms on ***-scheduler (executor driver) (43/50)
[2025-05-01T07:07:17.593+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_43 locally
[2025-05-01T07:07:17.595+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 43.0 in stage 7.0 (TID 95). 4181 bytes result sent to driver
[2025-05-01T07:07:17.596+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 96) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.598+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 44.0 in stage 7.0 (TID 96)
[2025-05-01T07:07:17.599+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 95) in 22 ms on ***-scheduler (executor driver) (44/50)
[2025-05-01T07:07:17.615+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_44 locally
[2025-05-01T07:07:17.618+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 44.0 in stage 7.0 (TID 96). 4181 bytes result sent to driver
[2025-05-01T07:07:17.620+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 97) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.621+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 45.0 in stage 7.0 (TID 97)
[2025-05-01T07:07:17.622+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 96) in 24 ms on ***-scheduler (executor driver) (45/50)
[2025-05-01T07:07:17.636+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_45 locally
[2025-05-01T07:07:17.639+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 45.0 in stage 7.0 (TID 97). 4181 bytes result sent to driver
[2025-05-01T07:07:17.641+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 98) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.642+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 97) in 23 ms on ***-scheduler (executor driver) (46/50)
[2025-05-01T07:07:17.643+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 46.0 in stage 7.0 (TID 98)
[2025-05-01T07:07:17.659+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_46 locally
[2025-05-01T07:07:17.661+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 46.0 in stage 7.0 (TID 98). 4181 bytes result sent to driver
[2025-05-01T07:07:17.663+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 99) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.664+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 47.0 in stage 7.0 (TID 99)
[2025-05-01T07:07:17.665+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 98) in 23 ms on ***-scheduler (executor driver) (47/50)
[2025-05-01T07:07:17.682+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_47 locally
[2025-05-01T07:07:17.685+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 47.0 in stage 7.0 (TID 99). 4181 bytes result sent to driver
[2025-05-01T07:07:17.687+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 100) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.688+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 48.0 in stage 7.0 (TID 100)
[2025-05-01T07:07:17.689+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 99) in 26 ms on ***-scheduler (executor driver) (48/50)
[2025-05-01T07:07:17.702+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_48 locally
[2025-05-01T07:07:17.706+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 48.0 in stage 7.0 (TID 100). 4181 bytes result sent to driver
[2025-05-01T07:07:17.708+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 101) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10206 bytes)
[2025-05-01T07:07:17.709+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Running task 49.0 in stage 7.0 (TID 101)
[2025-05-01T07:07:17.710+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 100) in 22 ms on ***-scheduler (executor driver) (49/50)
[2025-05-01T07:07:17.723+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManager: Found block rdd_10_49 locally
[2025-05-01T07:07:17.725+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO Executor: Finished task 49.0 in stage 7.0 (TID 101). 4181 bytes result sent to driver
[2025-05-01T07:07:17.727+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 101) in 20 ms on ***-scheduler (executor driver) (50/50)
[2025-05-01T07:07:17.728+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-05-01T07:07:17.729+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO DAGScheduler: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.526 s
[2025-05-01T07:07:17.730+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-01T07:07:17.731+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-05-01T07:07:17.732+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO DAGScheduler: Job 3 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.536893 s
[2025-05-01T07:07:17.758+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO CodeGenerator: Code generated in 19.619246 ms
[2025-05-01T07:07:17.835+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ***-scheduler:35203 in memory (size: 155.3 KiB, free: 434.3 MiB)
[2025-05-01T07:07:18.228+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO FileSourceStrategy: Pushed Filters:
[2025-05-01T07:07:18.230+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-01T07:07:18.247+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO FileSourceStrategy: Pushed Filters: IsNotNull(HASH)
[2025-05-01T07:07:18.249+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(HASH#135)
[2025-05-01T07:07:18.436+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-05-01T07:07:18.601+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO CodeGenerator: Code generated in 28.144998 ms
[2025-05-01T07:07:18.617+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 206.5 KiB, free 433.7 MiB)
[2025-05-01T07:07:18.639+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 433.7 MiB)
[2025-05-01T07:07:18.641+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ***-scheduler:35203 (size: 36.3 KiB, free: 434.3 MiB)
[2025-05-01T07:07:18.642+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-01T07:07:18.817+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO CodeGenerator: Code generated in 112.184253 ms
[2025-05-01T07:07:18.845+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO CodeGenerator: Code generated in 221.645118 ms
[2025-05-01T07:07:18.858+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 205.0 KiB, free 433.5 MiB)
[2025-05-01T07:07:18.910+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.5 MiB)
[2025-05-01T07:07:18.912+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ***-scheduler:35203 (size: 36.0 KiB, free: 434.3 MiB)
[2025-05-01T07:07:18.914+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO SparkContext: Created broadcast 7 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:07:18.950+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO CodeGenerator: Code generated in 16.300588 ms
[2025-05-01T07:07:18.961+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 54665438 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-01T07:07:19.028+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO CodeGenerator: Code generated in 49.158567 ms
[2025-05-01T07:07:19.052+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 35496120 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-01T07:07:19.054+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Registering RDD 22 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2025-05-01T07:07:19.057+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Got map stage job 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-01T07:07:19.058+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:07:19.061+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Parents of final stage: List()
[2025-05-01T07:07:19.062+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:19.063+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[22] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:07:19.067+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 91.0 KiB, free 433.4 MiB)
[2025-05-01T07:07:19.070+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.7 KiB, free 433.3 MiB)
[2025-05-01T07:07:19.072+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ***-scheduler:35203 (size: 30.7 KiB, free: 434.2 MiB)
[2025-05-01T07:07:19.073+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:19.075+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[22] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-01T07:07:19.078+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-05-01T07:07:19.079+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 102) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10840 bytes)
[2025-05-01T07:07:19.081+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO Executor: Running task 0.0 in stage 8.0 (TID 102)
[2025-05-01T07:07:19.144+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-01T07:07:19.146+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-05-01T07:07:19.148+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-05-01T07:07:19.150+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Parents of final stage: List()
[2025-05-01T07:07:19.151+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:19.153+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-05-01T07:07:19.154+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.5 KiB, free 433.3 MiB)
[2025-05-01T07:07:19.156+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 433.3 MiB)
[2025-05-01T07:07:19.157+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ***-scheduler:35203 (size: 8.4 KiB, free: 434.2 MiB)
[2025-05-01T07:07:19.159+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:19.162+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-05-01T07:07:19.164+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-05-01T07:07:19.268+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO CodeGenerator: Code generated in 158.79368 ms
[2025-05-01T07:07:19.350+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO CodeGenerator: Code generated in 64.786686 ms
[2025-05-01T07:07:19.481+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO CodeGenerator: Code generated in 73.605924 ms
[2025-05-01T07:07:19.507+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO FileScanRDD: Reading File path: s3a://medical-bucket/raw/transactional/medical-data-sample/claims_transactions.csv, range: 0-50471134, partition values: [empty row]
[2025-05-01T07:07:19.588+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:19 INFO CodeGenerator: Code generated in 58.924718 ms
[2025-05-01T07:07:29.365+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 102). 2813 bytes result sent to driver
[2025-05-01T07:07:29.371+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 103) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10929 bytes)
[2025-05-01T07:07:29.373+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 102) in 10292 ms on ***-scheduler (executor driver) (1/1)
[2025-05-01T07:07:29.374+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-05-01T07:07:29.375+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 103)
[2025-05-01T07:07:29.375+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 10.314 s
[2025-05-01T07:07:29.376+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO DAGScheduler: looking for newly runnable stages
[2025-05-01T07:07:29.377+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO DAGScheduler: running: Set(ResultStage 9)
[2025-05-01T07:07:29.378+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO DAGScheduler: waiting: Set()
[2025-05-01T07:07:29.379+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO DAGScheduler: failed: Set()
[2025-05-01T07:07:29.414+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO CodeGenerator: Code generated in 13.159022 ms
[2025-05-01T07:07:29.418+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/part-00000-eb1c0524-495e-442e-a7d0-555f4587a31d-c000.snappy.parquet, range: 0-31301816, partition values: [empty row]
[2025-05-01T07:07:29.651+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:29 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-01T07:07:30.343+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:30 INFO FilterCompat: Filtering using predicate: noteq(HASH, null)
[2025-05-01T07:07:30.518+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:30 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-01T07:07:31.207+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:31 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-05-01T07:07:32.338+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO MemoryStore: Block taskresult_103 stored as bytes in memory (estimated size 23.6 MiB, free 409.7 MiB)
[2025-05-01T07:07:32.339+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO BlockManagerInfo: Added taskresult_103 in memory on ***-scheduler:35203 (size: 23.6 MiB, free: 410.7 MiB)
[2025-05-01T07:07:32.340+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO Executor: Finished task 0.0 in stage 9.0 (TID 103). 24703665 bytes result sent via BlockManager)
[2025-05-01T07:07:32.390+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO TransportClientFactory: Successfully created connection to ***-scheduler/172.18.0.4:35203 after 4 ms (0 ms spent in bootstraps)
[2025-05-01T07:07:32.648+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 103) in 3282 ms on ***-scheduler (executor driver) (1/1)
[2025-05-01T07:07:32.650+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-05-01T07:07:32.651+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 13.500 s
[2025-05-01T07:07:32.652+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-01T07:07:32.653+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-05-01T07:07:32.654+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 13.506327 s
[2025-05-01T07:07:32.655+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO BlockManagerInfo: Removed taskresult_103 on ***-scheduler:35203 in memory (size: 23.6 MiB, free: 434.2 MiB)
[2025-05-01T07:07:32.677+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:32 INFO CodeGenerator: Code generated in 8.430849 ms
[2025-05-01T07:07:33.129+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 116.0 MiB, free 317.3 MiB)
[2025-05-01T07:07:33.406+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ***-scheduler:35203 in memory (size: 30.7 KiB, free: 434.2 MiB)
[2025-05-01T07:07:33.412+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ***-scheduler:35203 in memory (size: 8.4 KiB, free: 434.3 MiB)
[2025-05-01T07:07:33.640+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 313.5 MiB)
[2025-05-01T07:07:33.641+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 430.3 MiB)
[2025-05-01T07:07:33.647+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece1 stored as bytes in memory (estimated size 4.0 MiB, free 309.5 MiB)
[2025-05-01T07:07:33.648+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece1 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 426.3 MiB)
[2025-05-01T07:07:33.652+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece2 stored as bytes in memory (estimated size 4.0 MiB, free 305.5 MiB)
[2025-05-01T07:07:33.654+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece2 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 422.3 MiB)
[2025-05-01T07:07:33.659+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece3 stored as bytes in memory (estimated size 4.0 MiB, free 301.5 MiB)
[2025-05-01T07:07:33.660+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece3 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 418.3 MiB)
[2025-05-01T07:07:33.665+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece4 stored as bytes in memory (estimated size 4.0 MiB, free 297.5 MiB)
[2025-05-01T07:07:33.667+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece4 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 414.3 MiB)
[2025-05-01T07:07:33.670+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece5 stored as bytes in memory (estimated size 4.0 MiB, free 293.5 MiB)
[2025-05-01T07:07:33.671+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece5 in memory on ***-scheduler:35203 (size: 4.0 MiB, free: 410.3 MiB)
[2025-05-01T07:07:33.674+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO MemoryStore: Block broadcast_10_piece6 stored as bytes in memory (estimated size 2.5 MiB, free 291.0 MiB)
[2025-05-01T07:07:33.675+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO BlockManagerInfo: Added broadcast_10_piece6 in memory on ***-scheduler:35203 (size: 2.5 MiB, free: 407.8 MiB)
[2025-05-01T07:07:33.677+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-01T07:07:33.708+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 30690953, minimum partition size: 1048576
[2025-05-01T07:07:33.891+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:33 INFO CodeGenerator: Code generated in 83.728189 ms
[2025-05-01T07:07:34.036+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-01T07:07:34.039+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Got job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-01T07:07:34.040+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-01T07:07:34.041+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-05-01T07:07:34.042+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Missing parents: List()
[2025-05-01T07:07:34.043+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-01T07:07:34.092+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 488.0 KiB, free 290.5 MiB)
[2025-05-01T07:07:34.096+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 165.9 KiB, free 290.3 MiB)
[2025-05-01T07:07:34.097+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ***-scheduler:35203 (size: 165.9 KiB, free: 407.6 MiB)
[2025-05-01T07:07:34.098+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-05-01T07:07:34.099+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-01T07:07:34.100+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-05-01T07:07:34.102+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 104) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-01T07:07:34.103+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO Executor: Running task 0.0 in stage 11.0 (TID 104)
[2025-05-01T07:07:34.185+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO ShuffleBlockFetcherIterator: Getting 1 (29.3 MiB) non-empty blocks including 1 (29.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-01T07:07:34.186+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-05-01T07:07:34.261+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodeGenerator: Code generated in 73.758704 ms
[2025-05-01T07:07:34.330+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodeGenerator: Code generated in 31.306849 ms
[2025-05-01T07:07:34.631+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodeGenerator: Code generated in 146.58793 ms
[2025-05-01T07:07:34.662+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodeGenerator: Code generated in 11.988392 ms
[2025-05-01T07:07:34.670+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodecConfig: Compression: SNAPPY
[2025-05-01T07:07:34.679+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO CodecConfig: Compression: SNAPPY
[2025-05-01T07:07:34.734+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-05-01T07:07:34.761+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-05-01T07:07:34.763+0000] {spark_submit.py:649} INFO - {
[2025-05-01T07:07:34.764+0000] {spark_submit.py:649} INFO - "type" : "struct",
[2025-05-01T07:07:29.996+0000] {spark_submit.py:649} INFO - "fields" : [ {
[2025-05-01T07:07:29.997+0000] {spark_submit.py:649} INFO - "name" : "ID",
[2025-05-01T07:07:29.999+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.000+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.001+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.002+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.003+0000] {spark_submit.py:649} INFO - "name" : "CLAIMID",
[2025-05-01T07:07:30.005+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.006+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.007+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.008+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.010+0000] {spark_submit.py:649} INFO - "name" : "CHARGEID",
[2025-05-01T07:07:30.012+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.014+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.015+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.016+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.017+0000] {spark_submit.py:649} INFO - "name" : "PATIENTID",
[2025-05-01T07:07:30.018+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.022+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.023+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.025+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.026+0000] {spark_submit.py:649} INFO - "name" : "TYPE",
[2025-05-01T07:07:30.027+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.029+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.030+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.031+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.032+0000] {spark_submit.py:649} INFO - "name" : "AMOUNT",
[2025-05-01T07:07:30.033+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.034+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.035+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.037+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.037+0000] {spark_submit.py:649} INFO - "name" : "METHOD",
[2025-05-01T07:07:30.039+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.040+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.041+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.042+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.043+0000] {spark_submit.py:649} INFO - "name" : "FROMDATE",
[2025-05-01T07:07:30.044+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-01T07:07:30.045+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.045+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.046+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.047+0000] {spark_submit.py:649} INFO - "name" : "TODATE",
[2025-05-01T07:07:30.048+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-01T07:07:30.049+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.050+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.051+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.052+0000] {spark_submit.py:649} INFO - "name" : "PLACEOFSERVICE",
[2025-05-01T07:07:30.053+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.054+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.056+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.057+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.057+0000] {spark_submit.py:649} INFO - "name" : "PROCEDURECODE",
[2025-05-01T07:07:30.058+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.059+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.060+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.061+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.062+0000] {spark_submit.py:649} INFO - "name" : "MODIFIER1",
[2025-05-01T07:07:30.063+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.064+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.065+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.067+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.068+0000] {spark_submit.py:649} INFO - "name" : "MODIFIER2",
[2025-05-01T07:07:30.069+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.070+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.070+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.071+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.072+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF1",
[2025-05-01T07:07:30.073+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.074+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.075+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.076+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.077+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF2",
[2025-05-01T07:07:30.078+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.078+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.080+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.081+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.082+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF3",
[2025-05-01T07:07:30.083+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.084+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.085+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.087+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.088+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF4",
[2025-05-01T07:07:30.089+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.090+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.090+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.092+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.093+0000] {spark_submit.py:649} INFO - "name" : "UNITS",
[2025-05-01T07:07:30.094+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.096+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.097+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.100+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.102+0000] {spark_submit.py:649} INFO - "name" : "DEPARTMENTID",
[2025-05-01T07:07:30.104+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.105+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.106+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.107+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.108+0000] {spark_submit.py:649} INFO - "name" : "NOTES",
[2025-05-01T07:07:30.110+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.111+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.112+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.114+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.116+0000] {spark_submit.py:649} INFO - "name" : "UNITAMOUNT",
[2025-05-01T07:07:30.117+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.119+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.120+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.121+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.122+0000] {spark_submit.py:649} INFO - "name" : "TRANSFEROUTID",
[2025-05-01T07:07:30.123+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.125+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.127+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.128+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.130+0000] {spark_submit.py:649} INFO - "name" : "TRANSFERTYPE",
[2025-05-01T07:07:30.131+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.133+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.134+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.135+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.136+0000] {spark_submit.py:649} INFO - "name" : "PAYMENTS",
[2025-05-01T07:07:30.137+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.139+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.140+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.142+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.143+0000] {spark_submit.py:649} INFO - "name" : "ADJUSTMENTS",
[2025-05-01T07:07:30.144+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.146+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.147+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.148+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.149+0000] {spark_submit.py:649} INFO - "name" : "TRANSFERS",
[2025-05-01T07:07:30.150+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.151+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.152+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.153+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.155+0000] {spark_submit.py:649} INFO - "name" : "OUTSTANDING",
[2025-05-01T07:07:30.156+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-01T07:07:30.157+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.159+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.160+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.162+0000] {spark_submit.py:649} INFO - "name" : "APPOINTMENTID",
[2025-05-01T07:07:30.163+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.164+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.165+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.167+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.168+0000] {spark_submit.py:649} INFO - "name" : "LINENOTE",
[2025-05-01T07:07:30.170+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.172+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.175+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.177+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.178+0000] {spark_submit.py:649} INFO - "name" : "PATIENTINSURANCEID",
[2025-05-01T07:07:30.179+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.180+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.182+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.184+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.185+0000] {spark_submit.py:649} INFO - "name" : "FEESCHEDULEID",
[2025-05-01T07:07:30.186+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-01T07:07:30.188+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.189+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.191+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.192+0000] {spark_submit.py:649} INFO - "name" : "PROVIDERID",
[2025-05-01T07:07:30.193+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.195+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.196+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.197+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.198+0000] {spark_submit.py:649} INFO - "name" : "SUPERVISINGPROVIDERID",
[2025-05-01T07:07:30.199+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.202+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.204+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.206+0000] {spark_submit.py:649} INFO - }, {
[2025-05-01T07:07:30.207+0000] {spark_submit.py:649} INFO - "name" : "HASH",
[2025-05-01T07:07:30.208+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-01T07:07:30.209+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-01T07:07:30.210+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-01T07:07:30.211+0000] {spark_submit.py:649} INFO - } ]
[2025-05-01T07:07:30.212+0000] {spark_submit.py:649} INFO - }
[2025-05-01T07:07:30.213+0000] {spark_submit.py:649} INFO - and corresponding Parquet message type:
[2025-05-01T07:07:30.214+0000] {spark_submit.py:649} INFO - message spark_schema {
[2025-05-01T07:07:30.215+0000] {spark_submit.py:649} INFO - optional binary ID (STRING);
[2025-05-01T07:07:30.217+0000] {spark_submit.py:649} INFO - optional binary CLAIMID (STRING);
[2025-05-01T07:07:30.218+0000] {spark_submit.py:649} INFO - optional int64 CHARGEID;
[2025-05-01T07:07:30.220+0000] {spark_submit.py:649} INFO - optional binary PATIENTID (STRING);
[2025-05-01T07:07:30.221+0000] {spark_submit.py:649} INFO - optional binary TYPE (STRING);
[2025-05-01T07:07:30.222+0000] {spark_submit.py:649} INFO - optional double AMOUNT;
[2025-05-01T07:07:30.223+0000] {spark_submit.py:649} INFO - optional binary METHOD (STRING);
[2025-05-01T07:07:30.224+0000] {spark_submit.py:649} INFO - optional int96 FROMDATE;
[2025-05-01T07:07:30.225+0000] {spark_submit.py:649} INFO - optional int96 TODATE;
[2025-05-01T07:07:38.368+0000] {spark_submit.py:649} INFO - optional binary PLACEOFSERVICE (STRING);
[2025-05-01T07:07:38.369+0000] {spark_submit.py:649} INFO - optional int64 PROCEDURECODE;
[2025-05-01T07:07:38.370+0000] {spark_submit.py:649} INFO - optional binary MODIFIER1 (STRING);
[2025-05-01T07:07:38.371+0000] {spark_submit.py:649} INFO - optional binary MODIFIER2 (STRING);
[2025-05-01T07:07:38.373+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF1;
[2025-05-01T07:07:38.374+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF2;
[2025-05-01T07:07:38.375+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF3;
[2025-05-01T07:07:38.376+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF4;
[2025-05-01T07:07:38.378+0000] {spark_submit.py:649} INFO - optional int64 UNITS;
[2025-05-01T07:07:38.379+0000] {spark_submit.py:649} INFO - optional int64 DEPARTMENTID;
[2025-05-01T07:07:38.380+0000] {spark_submit.py:649} INFO - optional binary NOTES (STRING);
[2025-05-01T07:07:38.381+0000] {spark_submit.py:649} INFO - optional double UNITAMOUNT;
[2025-05-01T07:07:38.383+0000] {spark_submit.py:649} INFO - optional int64 TRANSFEROUTID;
[2025-05-01T07:07:38.384+0000] {spark_submit.py:649} INFO - optional binary TRANSFERTYPE (STRING);
[2025-05-01T07:07:38.385+0000] {spark_submit.py:649} INFO - optional double PAYMENTS;
[2025-05-01T07:07:38.386+0000] {spark_submit.py:649} INFO - optional double ADJUSTMENTS;
[2025-05-01T07:07:38.387+0000] {spark_submit.py:649} INFO - optional double TRANSFERS;
[2025-05-01T07:07:38.389+0000] {spark_submit.py:649} INFO - optional double OUTSTANDING;
[2025-05-01T07:07:38.390+0000] {spark_submit.py:649} INFO - optional binary APPOINTMENTID (STRING);
[2025-05-01T07:07:38.391+0000] {spark_submit.py:649} INFO - optional binary LINENOTE (STRING);
[2025-05-01T07:07:38.393+0000] {spark_submit.py:649} INFO - optional binary PATIENTINSURANCEID (STRING);
[2025-05-01T07:07:38.394+0000] {spark_submit.py:649} INFO - optional int64 FEESCHEDULEID;
[2025-05-01T07:07:38.395+0000] {spark_submit.py:649} INFO - optional binary PROVIDERID (STRING);
[2025-05-01T07:07:38.396+0000] {spark_submit.py:649} INFO - optional binary SUPERVISINGPROVIDERID (STRING);
[2025-05-01T07:07:38.397+0000] {spark_submit.py:649} INFO - optional binary HASH (STRING);
[2025-05-01T07:07:38.398+0000] {spark_submit.py:649} INFO - }
[2025-05-01T07:07:38.399+0000] {spark_submit.py:649} INFO - 
[2025-05-01T07:07:38.401+0000] {spark_submit.py:649} INFO - 
[2025-05-01T07:07:38.486+0000] {spark_submit.py:649} INFO - 25/05/01 07:07:38 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-05-01T07:08:04.188+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:04 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray((104,11,0,Vector(AccumulableInfo(489,None,Some(118),None,false,true,None), AccumulableInfo(493,None,Some(209715008),None,false,true,None), AccumulableInfo(495,None,Some(0),None,false,true,None), AccumulableInfo(496,None,Some(1),None,false,true,None), AccumulableInfo(497,None,Some(0),None,false,true,None), AccumulableInfo(498,None,Some(0),None,false,true,None), AccumulableInfo(499,None,Some(29326957),None,false,true,None), AccumulableInfo(500,None,Some(0),None,false,true,None), AccumulableInfo(501,None,Some(111602),None,false,true,None), AccumulableInfo(502,None,Some(0),None,false,true,None), AccumulableInfo(503,None,Some(0),None,false,true,None), AccumulableInfo(504,None,Some(0),None,false,true,None), AccumulableInfo(505,None,Some(0),None,false,true,None), AccumulableInfo(506,None,Some(0),None,false,true,None), AccumulableInfo(507,None,Some(0),None,false,true,None), AccumulableInfo(508,None,Some(0),None,false,true,None), AccumulableInfo(509,None,Some(0),None,false,true,None), AccumulableInfo(510,None,Some(0),None,false,true,None), AccumulableInfo(511,None,Some(0),None,false,true,None), AccumulableInfo(371,Some(records read),Some(111602),None,true,true,Some(sql)), AccumulableInfo(369,Some(local bytes read),Some(29326957),None,true,true,Some(sql)), AccumulableInfo(370,Some(fetch wait time),Some(0),None,true,true,Some(sql)), AccumulableInfo(366,Some(local blocks read),Some(1),None,true,true,Some(sql)), AccumulableInfo(478,Some(time in aggregation build),Some(2248),None,true,true,Some(sql)), AccumulableInfo(476,Some(peak memory),Some(88080304),None,true,true,Some(sql)), AccumulableInfo(475,Some(number of output rows),Some(85525),None,true,true,Some(sql)), AccumulableInfo(479,Some(avg hash probes per key),Some(16),None,true,true,Some(sql)), AccumulableInfo(1,Some(number of source rows),Some(85525),None,true,true,Some(sql)), AccumulableInfo(474,Some(number of output rows),Some(85525),None,true,true,Some(sql))))),Map((11,0) -> org.apache.spark.executor.ExecutorMetrics@7a6853e, (9,0) -> org.apache.spark.executor.ExecutorMetrics@1b794baf)) by listener AppStatusListener took 1.907286822s.
[2025-05-01T07:08:07.979+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO Executor: Finished task 0.0 in stage 11.0 (TID 104). 8129 bytes result sent to driver
[2025-05-01T07:08:07.997+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 104) in 33885 ms on ***-scheduler (executor driver) (1/1)
[2025-05-01T07:08:08.000+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-05-01T07:08:08.002+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO DAGScheduler: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 33.941 s
[2025-05-01T07:08:08.004+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-01T07:08:08.006+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-05-01T07:08:08.007+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:07 INFO DAGScheduler: Job 6 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 30.584176 s
[2025-05-01T07:08:08.009+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:08 INFO DeltaFileFormatWriter: Start to commit write Job 5a56c78c-6e14-432d-807e-7ab222094893.
[2025-05-01T07:08:08.014+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:08 INFO DeltaFileFormatWriter: Write Job 5a56c78c-6e14-432d-807e-7ab222094893 committed. Elapsed time: 3 ms.
[2025-05-01T07:08:08.031+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:08 INFO DeltaFileFormatWriter: Finished processing stats for write job 5a56c78c-6e14-432d-807e-7ab222094893.
[2025-05-01T07:08:08.092+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:08 INFO MergeIntoCommand: DELTA: Done
[2025-05-01T07:08:09.515+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:09 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-01T07:08:09.527+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:09 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-01T07:08:09.565+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:09 INFO SparkUI: Stopped Spark web UI at http://***-scheduler:4040
[2025-05-01T07:08:09.624+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-01T07:08:09.694+0000] {job.py:229} INFO - Heartbeat recovered after 30.30 seconds
[2025-05-01T07:08:05.296+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO MemoryStore: MemoryStore cleared
[2025-05-01T07:08:05.298+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO BlockManager: BlockManager stopped
[2025-05-01T07:08:05.311+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-01T07:08:05.325+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-01T07:08:05.353+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO SparkContext: Successfully stopped SparkContext
[2025-05-01T07:08:05.359+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO ShutdownHookManager: Shutdown hook called
[2025-05-01T07:08:05.361+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126
[2025-05-01T07:08:05.379+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-e397e591-5ad3-46c7-8747-e90b019bb438
[2025-05-01T07:08:05.398+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-525212df-fcea-420f-a8b3-7b97ecc25126/pyspark-6a9e349f-fba5-4006-9c85-2ec0831a8da1
[2025-05-01T07:08:05.414+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:05 INFO ShutdownHookManager: Deleting directory /tmp/hive-v3_1-33cdd18c-d2ba-448c-bab0-77006b0f5449
[2025-05-01T07:08:13.554+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:13 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-05-01T07:08:13.556+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:13 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-05-01T07:08:13.560+0000] {spark_submit.py:649} INFO - 25/05/01 07:08:13 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-05-01T07:08:14.139+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-05-01T07:08:14.142+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=medical_dag_v12, task_id=raw_enriched_group.creating_enriched_plan_transitions, run_id=manual__2025-05-01T07:05:50.707029+00:00, execution_date=20250501T070550, start_date=20250501T070607, end_date=20250501T070814
[2025-05-01T07:08:14.303+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-05-01T07:08:14.358+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-05-01T07:08:14.362+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
