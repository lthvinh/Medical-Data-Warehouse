[2025-05-02T01:26:55.126+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-02T01:26:55.155+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: medical_dag_v16.raw_enriched_group.creating_enriched_claims_transactions manual__2025-05-02T01:20:23.347496+00:00 [queued]>
[2025-05-02T01:26:55.172+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: medical_dag_v16.raw_enriched_group.creating_enriched_claims_transactions manual__2025-05-02T01:20:23.347496+00:00 [queued]>
[2025-05-02T01:26:55.173+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-05-02T01:26:55.202+0000] {taskinstance.py:2890} INFO - Executing <Task(SparkSubmitOperator): raw_enriched_group.creating_enriched_claims_transactions> on 2025-05-02 01:20:23.347496+00:00
[2025-05-02T01:26:55.212+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'medical_dag_v16', 'raw_enriched_group.creating_enriched_claims_transactions', 'manual__2025-05-02T01:20:23.347496+00:00', '--job-id', '134', '--raw', '--subdir', 'DAGS_FOLDER/raw_enriched_dag.py', '--cfg-path', '/tmp/tmpo3kunssc']
[2025-05-02T01:26:55.216+0000] {standard_task_runner.py:105} INFO - Job 134: Subtask raw_enriched_group.creating_enriched_claims_transactions
[2025-05-02T01:26:55.227+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=9664) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-05-02T01:26:55.228+0000] {standard_task_runner.py:72} INFO - Started process 9665 to run task
[2025-05-02T01:26:55.277+0000] {task_command.py:467} INFO - Running <TaskInstance: medical_dag_v16.raw_enriched_group.creating_enriched_claims_transactions manual__2025-05-02T01:20:23.347496+00:00 [running]> on host airflow-scheduler
[2025-05-02T01:26:55.389+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='medical_dag_v16' AIRFLOW_CTX_TASK_ID='raw_enriched_group.creating_enriched_claims_transactions' AIRFLOW_CTX_EXECUTION_DATE='2025-05-02T01:20:23.347496+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-02T01:20:23.347496+00:00'
[2025-05-02T01:26:55.390+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-05-02T01:26:55.432+0000] {base.py:84} INFO - Retrieving connection 'spark_conn'
[2025-05-02T01:26:55.434+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local --packages io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4 --name arrow-spark --deploy-mode client /opt/etl/raw_enriched/creating_enriched_claims_transactions.py
[2025-05-02T01:26:59.095+0000] {spark_submit.py:649} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-02T01:26:59.294+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-02T01:26:59.297+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-02T01:26:59.305+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12 added as a dependency
[2025-05-02T01:26:59.307+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-05-02T01:26:59.308+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-8d556976-d03c-4467-ba2e-58196d1ce4cf;1.0
[2025-05-02T01:26:59.310+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T01:27:10.957+0000] {spark_submit.py:649} INFO - found io.delta#delta-spark_2.12;3.3.0 in central
[2025-05-02T01:27:11.030+0000] {spark_submit.py:649} INFO - found io.delta#delta-storage;3.3.0 in central
[2025-05-02T01:27:11.076+0000] {spark_submit.py:649} INFO - found org.antlr#antlr4-runtime;4.9.3 in central
[2025-05-02T01:27:11.212+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2025-05-02T01:27:11.266+0000] {spark_submit.py:649} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2025-05-02T01:27:11.318+0000] {spark_submit.py:649} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-05-02T01:27:11.380+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve 12040ms :: artifacts dl 32ms
[2025-05-02T01:27:11.382+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-02T01:27:11.384+0000] {spark_submit.py:649} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2025-05-02T01:27:11.385+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12;3.3.0 from central in [default]
[2025-05-02T01:27:11.386+0000] {spark_submit.py:649} INFO - io.delta#delta-storage;3.3.0 from central in [default]
[2025-05-02T01:27:11.387+0000] {spark_submit.py:649} INFO - org.antlr#antlr4-runtime;4.9.3 from central in [default]
[2025-05-02T01:27:11.388+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2025-05-02T01:27:11.389+0000] {spark_submit.py:649} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-05-02T01:27:11.390+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:11.391+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-02T01:27:11.393+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-02T01:27:11.394+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:11.396+0000] {spark_submit.py:649} INFO - |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
[2025-05-02T01:27:11.397+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:11.400+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-8d556976-d03c-4467-ba2e-58196d1ce4cf
[2025-05-02T01:27:11.402+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T01:27:11.418+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 6 already retrieved (0kB/18ms)
[2025-05-02T01:27:11.632+0000] {job.py:229} INFO - Heartbeat recovered after 16.57 seconds
[2025-05-02T01:27:12.226+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-02T01:27:20.180+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SparkContext: Running Spark version 3.5.3
[2025-05-02T01:27:20.181+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-02T01:27:20.182+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SparkContext: Java version 17.0.14
[2025-05-02T01:27:20.220+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceUtils: ==============================================================
[2025-05-02T01:27:20.221+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-02T01:27:20.222+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceUtils: ==============================================================
[2025-05-02T01:27:20.222+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SparkContext: Submitted application: MyETLPipeline
[2025-05-02T01:27:20.276+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-02T01:27:20.298+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceProfile: Limiting resource is cpu
[2025-05-02T01:27:20.300+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-02T01:27:20.429+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SecurityManager: Changing view acls to: ***
[2025-05-02T01:27:20.430+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SecurityManager: Changing modify acls to: ***
[2025-05-02T01:27:20.431+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SecurityManager: Changing view acls groups to:
[2025-05-02T01:27:20.433+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SecurityManager: Changing modify acls groups to:
[2025-05-02T01:27:20.435+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-05-02T01:27:20.859+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO Utils: Successfully started service 'sparkDriver' on port 38781.
[2025-05-02T01:27:20.932+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:20 INFO SparkEnv: Registering MapOutputTracker
[2025-05-02T01:27:21.005+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-02T01:27:21.042+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-02T01:27:21.044+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-02T01:27:21.055+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-02T01:27:21.124+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ecc94f77-fb2e-4311-9861-492057dcfba0
[2025-05-02T01:27:21.155+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-02T01:27:21.187+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-02T01:27:21.529+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-02T01:27:21.700+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-02T01:27:21.799+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at spark://***-scheduler:38781/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:21.800+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://***-scheduler:38781/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746149240168
[2025-05-02T01:27:21.802+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at spark://***-scheduler:38781/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:21.803+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://***-scheduler:38781/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746149240168
[2025-05-02T01:27:21.804+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://***-scheduler:38781/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746149240168
[2025-05-02T01:27:21.805+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://***-scheduler:38781/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746149240168
[2025-05-02T01:27:21.806+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:21.810+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T01:27:21.852+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746149240168
[2025-05-02T01:27:21.853+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T01:27:21.869+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:21.870+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-storage-3.3.0.jar
[2025-05-02T01:27:21.877+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746149240168
[2025-05-02T01:27:21.879+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Copying /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T01:27:21.887+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746149240168
[2025-05-02T01:27:21.888+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:21 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T01:27:22.854+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:22 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746149240168
[2025-05-02T01:27:22.856+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:22 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T01:27:22.989+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:22 INFO Executor: Starting executor ID driver on host ***-scheduler
[2025-05-02T01:27:22.990+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:22 INFO Executor: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-02T01:27:22.991+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:22 INFO Executor: Java version 17.0.14
[2025-05-02T01:27:23.003+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-02T01:27:23.004+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@74beeae1 for default.
[2025-05-02T01:27:23.027+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:23.061+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-storage-3.3.0.jar
[2025-05-02T01:27:23.069+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746149240168
[2025-05-02T01:27:23.085+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T01:27:23.092+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:23.133+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T01:27:23.139+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746149240168
[2025-05-02T01:27:23.142+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T01:27:23.149+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746149240168
[2025-05-02T01:27:23.150+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T01:27:23.157+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746149240168
[2025-05-02T01:27:23.855+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T01:27:23.866+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching spark://***-scheduler:38781/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746149240168
[2025-05-02T01:27:23.944+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO TransportClientFactory: Successfully created connection to ***-scheduler/172.18.0.4:38781 after 54 ms (0 ms spent in bootstraps)
[2025-05-02T01:27:23.954+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: Fetching spark://***-scheduler:38781/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp2935817205942990363.tmp
[2025-05-02T01:27:23.991+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp2935817205942990363.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T01:27:23.999+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to class loader default
[2025-05-02T01:27:24.000+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:23 INFO Executor: Fetching spark://***-scheduler:38781/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746149240168
[2025-05-02T01:27:24.001+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: Fetching spark://***-scheduler:38781/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp2328743337231580980.tmp
[2025-05-02T01:27:24.011+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp2328743337231580980.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T01:27:24.019+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.apache.hadoop_hadoop-aws-3.3.4.jar to class loader default
[2025-05-02T01:27:24.021+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Fetching spark://***-scheduler:38781/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:24.022+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: Fetching spark://***-scheduler:38781/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp14336682474410259649.tmp
[2025-05-02T01:27:24.075+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp14336682474410259649.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T01:27:24.084+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-spark_2.12-3.3.0.jar to class loader default
[2025-05-02T01:27:24.086+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Fetching spark://***-scheduler:38781/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746149240168
[2025-05-02T01:27:24.087+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: Fetching spark://***-scheduler:38781/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp4974585675575916390.tmp
[2025-05-02T01:27:24.088+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp4974585675575916390.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-storage-3.3.0.jar
[2025-05-02T01:27:24.095+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/io.delta_delta-storage-3.3.0.jar to class loader default
[2025-05-02T01:27:24.097+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Fetching spark://***-scheduler:38781/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746149240168
[2025-05-02T01:27:24.098+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: Fetching spark://***-scheduler:38781/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp10414962521658755574.tmp
[2025-05-02T01:27:24.101+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp10414962521658755574.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T01:27:24.108+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/org.antlr_antlr4-runtime-4.9.3.jar to class loader default
[2025-05-02T01:27:24.109+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Executor: Fetching spark://***-scheduler:38781/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746149240168
[2025-05-02T01:27:24.110+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:24 INFO Utils: Fetching spark://***-scheduler:38781/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp10458050719081627660.tmp
[2025-05-02T01:27:26.164+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO Utils: /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/fetchFileTemp10458050719081627660.tmp has been previously copied to /tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T01:27:26.209+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO Executor: Adding file:/tmp/spark-4d48476a-7942-4a78-994d-b1fd1b870b9a/userFiles-a3a5a913-bc8c-4878-9bf8-fbe2d14188c3/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to class loader default
[2025-05-02T01:27:26.233+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40993.
[2025-05-02T01:27:26.234+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO NettyBlockTransferService: Server created on ***-scheduler:40993
[2025-05-02T01:27:26.237+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-02T01:27:26.261+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ***-scheduler, 40993, None)
[2025-05-02T01:27:26.274+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO BlockManagerMasterEndpoint: Registering block manager ***-scheduler:40993 with 434.4 MiB RAM, BlockManagerId(driver, ***-scheduler, 40993, None)
[2025-05-02T01:27:26.284+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ***-scheduler, 40993, None)
[2025-05-02T01:27:26.288+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ***-scheduler, 40993, None)
[2025-05-02T01:27:27.510+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-02T01:27:27.516+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:27 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-05-02T01:27:29.994+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:29 INFO HiveConf: Found configuration file null
[2025-05-02T01:27:30.016+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 3.1.3 using maven.
[2025-05-02T01:27:30.038+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-02T01:27:30.040+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-02T01:27:30.084+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-02T01:27:30.085+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-02T01:27:30.086+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api added as a dependency
[2025-05-02T01:27:30.087+0000] {spark_submit.py:649} INFO - org.apache.derby#derby added as a dependency
[2025-05-02T01:27:30.088+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore added as a dependency
[2025-05-02T01:27:30.089+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec added as a dependency
[2025-05-02T01:27:30.090+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common added as a dependency
[2025-05-02T01:27:30.091+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde added as a dependency
[2025-05-02T01:27:30.092+0000] {spark_submit.py:649} INFO - com.google.guava#guava added as a dependency
[2025-05-02T01:27:30.093+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api added as a dependency
[2025-05-02T01:27:30.094+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime added as a dependency
[2025-05-02T01:27:30.095+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-daf3dd70-9e2f-499e-a653-203a895746ea;1.0
[2025-05-02T01:27:30.096+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T01:27:30.142+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-api;2.10.0 in central
[2025-05-02T01:27:30.170+0000] {spark_submit.py:649} INFO - found org.apache.derby#derby;10.14.1.0 in central
[2025-05-02T01:27:30.235+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-metastore;3.1.3 in central
[2025-05-02T01:27:30.371+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-serde;3.1.3 in central
[2025-05-02T01:27:30.462+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-common;3.1.3 in central
[2025-05-02T01:27:30.529+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-classification;3.1.3 in central
[2025-05-02T01:27:30.587+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.10 in central
[2025-05-02T01:27:30.647+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-upgrade-acid;3.1.3 in central
[2025-05-02T01:27:30.693+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-shims;3.1.3 in central
[2025-05-02T01:27:30.731+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-common;3.1.3 in central
[2025-05-02T01:27:30.780+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 in central
[2025-05-02T01:27:30.824+0000] {spark_submit.py:649} INFO - found com.google.guava#guava;19.0 in central
[2025-05-02T01:27:30.869+0000] {spark_submit.py:649} INFO - found commons-lang#commons-lang;2.6 in central
[2025-05-02T01:27:30.902+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libthrift;0.9.3 in central
[2025-05-02T01:27:30.948+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpclient;4.5.13 in central
[2025-05-02T01:27:31.000+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpcore;4.4.13 in central
[2025-05-02T01:27:31.047+0000] {spark_submit.py:649} INFO - found commons-logging#commons-logging;1.2 in central
[2025-05-02T01:27:31.107+0000] {spark_submit.py:649} INFO - found commons-codec#commons-codec;1.15 in central
[2025-05-02T01:27:31.157+0000] {spark_submit.py:649} INFO - found org.apache.zookeeper#zookeeper;3.4.6 in central
[2025-05-02T01:27:31.208+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-log4j12;1.6.1 in central
[2025-05-02T01:27:31.248+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.16 in central
[2025-05-02T01:27:31.292+0000] {spark_submit.py:649} INFO - found jline#jline;2.12 in central
[2025-05-02T01:27:31.330+0000] {spark_submit.py:649} INFO - found io.netty#netty;3.7.0.Final in central
[2025-05-02T01:27:31.385+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-core;2.17.1 in central
[2025-05-02T01:27:31.440+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-0.23;3.1.3 in central
[2025-05-02T01:27:31.506+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 in central
[2025-05-02T01:27:31.574+0000] {spark_submit.py:649} INFO - found javax.servlet#javax.servlet-api;3.1.0 in central
[2025-05-02T01:27:31.646+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-annotations;3.1.0 in central
[2025-05-02T01:27:31.685+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-servlet;4.0 in central
[2025-05-02T01:27:31.720+0000] {spark_submit.py:649} INFO - found com.google.inject#guice;4.0 in central
[2025-05-02T01:27:31.762+0000] {spark_submit.py:649} INFO - found javax.inject#javax.inject;1 in central
[2025-05-02T01:27:31.833+0000] {spark_submit.py:649} INFO - found aopalliance#aopalliance;1.0 in central
[2025-05-02T01:27:31.911+0000] {spark_submit.py:649} INFO - found com.google.protobuf#protobuf-java;2.5.0 in central
[2025-05-02T01:27:31.959+0000] {spark_submit.py:649} INFO - found commons-io#commons-io;2.6 in central
[2025-05-02T01:27:32.007+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-json;1.19 in central
[2025-05-02T01:27:32.056+0000] {spark_submit.py:649} INFO - found org.codehaus.jettison#jettison;1.1 in central
[2025-05-02T01:27:32.109+0000] {spark_submit.py:649} INFO - found com.sun.xml.bind#jaxb-impl;2.2.3-1 in central
[2025-05-02T01:27:32.167+0000] {spark_submit.py:649} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2025-05-02T01:27:32.203+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2025-05-02T01:27:32.238+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2025-05-02T01:27:32.274+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-jaxrs;1.9.13 in central
[2025-05-02T01:27:32.311+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-xc;1.9.13 in central
[2025-05-02T01:27:32.368+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-core;1.19 in central
[2025-05-02T01:27:32.403+0000] {spark_submit.py:649} INFO - found javax.ws.rs#jsr311-api;1.1.1 in central
[2025-05-02T01:27:32.434+0000] {spark_submit.py:649} INFO - found com.sun.jersey.contribs#jersey-guice;1.19 in central
[2025-05-02T01:27:32.470+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-servlet;1.19 in central
[2025-05-02T01:27:32.507+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-server;1.19 in central
[2025-05-02T01:27:32.579+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-common;3.1.0 in central
[2025-05-02T01:27:32.667+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-api;3.1.0 in central
[2025-05-02T01:27:32.805+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.12.0 in central
[2025-05-02T01:27:32.871+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-auth;3.1.0 in central
[2025-05-02T01:27:32.988+0000] {spark_submit.py:649} INFO - found com.nimbusds#nimbus-jose-jwt;4.41.1 in central
[2025-05-02T01:27:33.029+0000] {spark_submit.py:649} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-02T01:27:33.098+0000] {spark_submit.py:649} INFO - found net.minidev#json-smart;2.3 in central
[2025-05-02T01:27:33.137+0000] {spark_submit.py:649} INFO - found net.minidev#accessors-smart;1.2 in central
[2025-05-02T01:27:33.176+0000] {spark_submit.py:649} INFO - found org.ow2.asm#asm;5.0.4 in central
[2025-05-02T01:27:33.240+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-simplekdc;1.0.1 in central
[2025-05-02T01:27:33.283+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-client;1.0.1 in central
[2025-05-02T01:27:33.330+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-config;1.0.1 in central
[2025-05-02T01:27:33.382+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-core;1.0.1 in central
[2025-05-02T01:27:33.451+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-pkix;1.0.1 in central
[2025-05-02T01:27:33.540+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-asn1;1.0.1 in central
[2025-05-02T01:27:33.598+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-util;1.0.1 in central
[2025-05-02T01:27:33.650+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-common;1.0.1 in central
[2025-05-02T01:27:33.695+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-crypto;1.0.1 in central
[2025-05-02T01:27:33.754+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-util;1.0.1 in central
[2025-05-02T01:27:33.809+0000] {spark_submit.py:649} INFO - found org.apache.kerby#token-provider;1.0.1 in central
[2025-05-02T01:27:33.849+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-admin;1.0.1 in central
[2025-05-02T01:27:33.887+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-server;1.0.1 in central
[2025-05-02T01:27:33.923+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-identity;1.0.1 in central
[2025-05-02T01:27:33.959+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-xdr;1.0.1 in central
[2025-05-02T01:27:33.992+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-compress;1.19 in central
[2025-05-02T01:27:34.030+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util;9.3.19.v20170502 in central
[2025-05-02T01:27:34.088+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-client;1.19 in central
[2025-05-02T01:27:34.133+0000] {spark_submit.py:649} INFO - found commons-cli#commons-cli;1.2 in central
[2025-05-02T01:27:34.191+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.17 in central
[2025-05-02T01:27:34.234+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-core;2.12.0 in central
[2025-05-02T01:27:34.283+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-databind;2.12.0 in central
[2025-05-02T01:27:34.324+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 in central
[2025-05-02T01:27:34.374+0000] {spark_submit.py:649} INFO - found jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central
[2025-05-02T01:27:34.408+0000] {spark_submit.py:649} INFO - found jakarta.activation#jakarta.activation-api;1.2.1 in central
[2025-05-02T01:27:34.447+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 in central
[2025-05-02T01:27:34.481+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 in central
[2025-05-02T01:27:34.514+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 in central
[2025-05-02T01:27:34.557+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-common;3.1.0 in central
[2025-05-02T01:27:34.613+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-registry;3.1.0 in central
[2025-05-02T01:27:34.693+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-common;3.1.0 in central
[2025-05-02T01:27:34.771+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-math3;3.1.1 in central
[2025-05-02T01:27:34.854+0000] {spark_submit.py:649} INFO - found commons-net#commons-net;3.6 in central
[2025-05-02T01:27:34.892+0000] {spark_submit.py:649} INFO - found commons-collections#commons-collections;3.2.2 in central
[2025-05-02T01:27:34.942+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-server;9.3.20.v20170531 in central
[2025-05-02T01:27:35.002+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-http;9.3.20.v20170531 in central
[2025-05-02T01:27:35.049+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-io;9.3.20.v20170531 in central
[2025-05-02T01:27:35.085+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 in central
[2025-05-02T01:27:35.120+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-security;9.3.20.v20170531 in central
[2025-05-02T01:27:35.159+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 in central
[2025-05-02T01:27:35.205+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-xml;9.3.20.v20170531 in central
[2025-05-02T01:27:35.345+0000] {spark_submit.py:649} INFO - found commons-beanutils#commons-beanutils;1.9.3 in central
[2025-05-02T01:27:35.407+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-configuration2;2.1.1 in central
[2025-05-02T01:27:35.510+0000] {spark_submit.py:649} INFO - found org.apache.avro#avro;1.8.2 in central
[2025-05-02T01:27:35.584+0000] {spark_submit.py:649} INFO - found com.thoughtworks.paranamer#paranamer;2.7 in central
[2025-05-02T01:27:35.621+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.4 in central
[2025-05-02T01:27:35.680+0000] {spark_submit.py:649} INFO - found org.tukaani#xz;1.5 in central
[2025-05-02T01:27:35.723+0000] {spark_submit.py:649} INFO - found com.google.re2j#re2j;1.1 in central
[2025-05-02T01:27:35.790+0000] {spark_submit.py:649} INFO - found com.google.code.gson#gson;2.2.4 in central
[2025-05-02T01:27:35.844+0000] {spark_submit.py:649} INFO - found com.jcraft#jsch;0.1.54 in central
[2025-05-02T01:27:35.887+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-02T01:27:35.933+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core4;4.1.0-incubating in central
[2025-05-02T01:27:36.051+0000] {spark_submit.py:649} INFO - found org.codehaus.woodstox#stax2-api;3.1.4 in central
[2025-05-02T01:27:36.100+0000] {spark_submit.py:649} INFO - found com.fasterxml.woodstox#woodstox-core;5.0.3 in central
[2025-05-02T01:27:36.138+0000] {spark_submit.py:649} INFO - found commons-daemon#commons-daemon;1.0.13 in central
[2025-05-02T01:27:36.184+0000] {spark_submit.py:649} INFO - found dnsjava#dnsjava;2.1.7 in central
[2025-05-02T01:27:36.208+0000] {spark_submit.py:649} INFO - found org.fusesource.leveldbjni#leveldbjni-all;1.8 in central
[2025-05-02T01:27:36.238+0000] {spark_submit.py:649} INFO - found org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 in central
[2025-05-02T01:27:36.263+0000] {spark_submit.py:649} INFO - found org.ehcache#ehcache;3.3.1 in central
[2025-05-02T01:27:36.289+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP-java7;2.4.12 in central
[2025-05-02T01:27:36.328+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 in central
[2025-05-02T01:27:36.405+0000] {spark_submit.py:649} INFO - found de.ruedigermoeller#fst;2.50 in central
[2025-05-02T01:27:36.432+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#java-util;1.9.0 in central
[2025-05-02T01:27:36.460+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#json-io;2.5.1 in central
[2025-05-02T01:27:36.490+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 in central
[2025-05-02T01:27:36.640+0000] {spark_submit.py:649} INFO - found javax.servlet.jsp#jsp-api;2.1 in central
[2025-05-02T01:27:36.682+0000] {spark_submit.py:649} INFO - found com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 in central
[2025-05-02T01:27:36.711+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-scheduler;3.1.3 in central
[2025-05-02T01:27:36.736+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-storage-api;2.7.0 in central
[2025-05-02T01:27:36.748+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-lang3;3.9 in central
[2025-05-02T01:27:36.770+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-core;1.5.8 in central
[2025-05-02T01:27:36.789+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-shims;1.5.8 in central
[2025-05-02T01:27:36.812+0000] {spark_submit.py:649} INFO - found io.airlift#aircompressor;0.10 in central
[2025-05-02T01:27:36.831+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 in central
[2025-05-02T01:27:36.855+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-client;9.3.20.v20170531 in central
[2025-05-02T01:27:36.874+0000] {spark_submit.py:649} INFO - found joda-time#joda-time;2.9.9 in central
[2025-05-02T01:27:36.906+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-1.2-api;2.17.1 in central
[2025-05-02T01:27:36.929+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-web;2.17.1 in central
[2025-05-02T01:27:36.948+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant;1.9.1 in central
[2025-05-02T01:27:36.966+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant-launcher;1.9.1 in central
[2025-05-02T01:27:36.984+0000] {spark_submit.py:649} INFO - found net.sf.jpam#jpam;1.1 in central
[2025-05-02T01:27:37.003+0000] {spark_submit.py:649} INFO - found com.tdunning#json;1.8 in central
[2025-05-02T01:27:37.021+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.1.0 in central
[2025-05-02T01:27:37.040+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-jvm;3.1.0 in central
[2025-05-02T01:27:37.060+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-json;3.1.0 in central
[2025-05-02T01:27:37.082+0000] {spark_submit.py:649} INFO - found com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in central
[2025-05-02T01:27:37.107+0000] {spark_submit.py:649} INFO - found javolution#javolution;5.5.1 in central
[2025-05-02T01:27:37.137+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-service-rpc;3.1.3 in central
[2025-05-02T01:27:37.157+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libfb303;0.9.3 in central
[2025-05-02T01:27:37.177+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-vector;0.8.0 in central
[2025-05-02T01:27:37.196+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-format;0.8.0 in central
[2025-05-02T01:27:37.212+0000] {spark_submit.py:649} INFO - found com.vlkan#flatbuffers;1.2.0-3f79e055 in central
[2025-05-02T01:27:37.231+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-memory;0.8.0 in central
[2025-05-02T01:27:37.256+0000] {spark_submit.py:649} INFO - found io.netty#netty-buffer;4.1.17.Final in central
[2025-05-02T01:27:37.280+0000] {spark_submit.py:649} INFO - found io.netty#netty-common;4.1.17.Final in central
[2025-05-02T01:27:37.326+0000] {spark_submit.py:649} INFO - found com.carrotsearch#hppc;0.7.2 in central
[2025-05-02T01:27:37.348+0000] {spark_submit.py:649} INFO - found net.sf.opencsv#opencsv;2.3 in central
[2025-05-02T01:27:37.367+0000] {spark_submit.py:649} INFO - found org.apache.parquet#parquet-hadoop-bundle;1.10.0 in central
[2025-05-02T01:27:37.396+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-standalone-metastore;3.1.3 in central
[2025-05-02T01:27:37.435+0000] {spark_submit.py:649} INFO - found com.jolbox#bonecp;0.8.0.RELEASE in central
[2025-05-02T01:27:37.460+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP;2.6.1 in central
[2025-05-02T01:27:37.482+0000] {spark_submit.py:649} INFO - found commons-dbcp#commons-dbcp;1.4 in central
[2025-05-02T01:27:37.502+0000] {spark_submit.py:649} INFO - found commons-pool#commons-pool;1.5.4 in central
[2025-05-02T01:27:37.537+0000] {spark_submit.py:649} INFO - found org.antlr#antlr-runtime;3.5.2 in central
[2025-05-02T01:27:37.587+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-api-jdo;4.2.4 in central
[2025-05-02T01:27:37.605+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-core;4.1.17 in central
[2025-05-02T01:27:37.626+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-rdbms;4.1.19 in central
[2025-05-02T01:27:37.643+0000] {spark_submit.py:649} INFO - found org.datanucleus#javax.jdo;3.2.0-m3 in central
[2025-05-02T01:27:37.658+0000] {spark_submit.py:649} INFO - found javax.transaction#transaction-api;1.1 in central
[2025-05-02T01:27:37.674+0000] {spark_submit.py:649} INFO - found sqlline#sqlline;1.3.0 in central
[2025-05-02T01:27:37.709+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-client;2.0.0-alpha4 in central
[2025-05-02T01:27:37.726+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 in central
[2025-05-02T01:27:37.749+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 in central
[2025-05-02T01:27:37.765+0000] {spark_submit.py:649} INFO - found org.apache.yetus#audience-annotations;0.5.0 in central
[2025-05-02T01:27:37.780+0000] {spark_submit.py:649} INFO - found junit#junit;4.11 in central
[2025-05-02T01:27:37.794+0000] {spark_submit.py:649} INFO - found org.hamcrest#hamcrest-core;1.3 in central
[2025-05-02T01:27:37.814+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol;2.0.0-alpha4 in central
[2025-05-02T01:27:37.847+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 in central
[2025-05-02T01:27:37.862+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 in central
[2025-05-02T01:27:37.879+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core;3.2.0-incubating in central
[2025-05-02T01:27:37.893+0000] {spark_submit.py:649} INFO - found org.jruby.jcodings#jcodings;1.0.18 in central
[2025-05-02T01:27:37.908+0000] {spark_submit.py:649} INFO - found org.jruby.joni#joni;2.1.11 in central
[2025-05-02T01:27:37.921+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.2.1 in central
[2025-05-02T01:27:37.943+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-crypto;1.0.0 in central
[2025-05-02T01:27:37.968+0000] {spark_submit.py:649} INFO - found javax.jdo#jdo-api;3.0.1 in central
[2025-05-02T01:27:37.980+0000] {spark_submit.py:649} INFO - found javax.transaction#jta;1.1 in central
[2025-05-02T01:27:37.993+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-api;0.6.0 in central
[2025-05-02T01:27:38.008+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-core;0.6.0 in central
[2025-05-02T01:27:38.029+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-assistedinject;3.0 in central
[2025-05-02T01:27:38.056+0000] {spark_submit.py:649} INFO - found it.unimi.dsi#fastutil;6.5.6 in central
[2025-05-02T01:27:38.074+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-common;0.6.0-incubating in central
[2025-05-02T01:27:38.095+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-core;0.6.0-incubating in central
[2025-05-02T01:27:38.117+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-api;0.6.0-incubating in central
[2025-05-02T01:27:38.140+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-api;0.6.0-incubating in central
[2025-05-02T01:27:38.159+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-zookeeper;0.6.0-incubating in central
[2025-05-02T01:27:38.186+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-core;0.6.0-incubating in central
[2025-05-02T01:27:38.213+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 in central
[2025-05-02T01:27:38.238+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-exec;3.1.3 in central
[2025-05-02T01:27:38.261+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-tez;3.1.3 in central
[2025-05-02T01:27:38.288+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-client;3.1.3 in central
[2025-05-02T01:27:38.310+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-common;3.1.3 in central
[2025-05-02T01:27:38.399+0000] {spark_submit.py:649} INFO - found org.antlr#ST4;4.0.4 in central
[2025-05-02T01:27:38.430+0000] {spark_submit.py:649} INFO - found org.apache.ivy#ivy;2.4.0 in central
[2025-05-02T01:27:38.445+0000] {spark_submit.py:649} INFO - found org.codehaus.groovy#groovy-all;2.4.11 in central
[2025-05-02T01:27:38.477+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-core;1.16.0 in central
[2025-05-02T01:27:38.494+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-linq4j;1.16.0 in central
[2025-05-02T01:27:38.530+0000] {spark_submit.py:649} INFO - found com.esri.geometry#esri-geometry-api;2.0.0 in central
[2025-05-02T01:27:38.547+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.1 in central
[2025-05-02T01:27:38.561+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#sketches-core;0.9.0 in central
[2025-05-02T01:27:38.578+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#memory;0.9.0 in central
[2025-05-02T01:27:38.593+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#janino;2.7.6 in central
[2025-05-02T01:27:38.608+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#commons-compiler;2.7.6 in central
[2025-05-02T01:27:38.626+0000] {spark_submit.py:649} INFO - found org.apache.calcite.avatica#avatica;1.11.0 in central
[2025-05-02T01:27:38.646+0000] {spark_submit.py:649} INFO - found stax#stax-api;1.0.1 in central
[2025-05-02T01:27:38.682+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-02T01:27:38.693+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.8.2 in central
[2025-05-02T01:27:38.711+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-02T01:27:38.723+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.36 in central
[2025-05-02T01:27:38.957+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve 8696ms :: artifacts dl 176ms
[2025-05-02T01:27:38.958+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-02T01:27:38.959+0000] {spark_submit.py:649} INFO - aopalliance#aopalliance;1.0 from central in [default]
[2025-05-02T01:27:38.960+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-api;0.6.0 from central in [default]
[2025-05-02T01:27:38.961+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-core;0.6.0 from central in [default]
[2025-05-02T01:27:38.962+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 from central in [default]
[2025-05-02T01:27:38.963+0000] {spark_submit.py:649} INFO - com.carrotsearch#hppc;0.7.2 from central in [default]
[2025-05-02T01:27:38.963+0000] {spark_submit.py:649} INFO - com.cedarsoftware#java-util;1.9.0 from central in [default]
[2025-05-02T01:27:38.964+0000] {spark_submit.py:649} INFO - com.cedarsoftware#json-io;2.5.1 from central in [default]
[2025-05-02T01:27:38.965+0000] {spark_submit.py:649} INFO - com.esri.geometry#esri-geometry-api;2.0.0 from central in [default]
[2025-05-02T01:27:38.966+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-annotations;2.12.0 from central in [default]
[2025-05-02T01:27:38.967+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-core;2.12.0 from central in [default]
[2025-05-02T01:27:38.967+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-databind;2.12.0 from central in [default]
[2025-05-02T01:27:38.968+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 from central in [default]
[2025-05-02T01:27:38.969+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 from central in [default]
[2025-05-02T01:27:38.970+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 from central in [default]
[2025-05-02T01:27:38.970+0000] {spark_submit.py:649} INFO - com.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]
[2025-05-02T01:27:38.971+0000] {spark_submit.py:649} INFO - com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from central in [default]
[2025-05-02T01:27:38.972+0000] {spark_submit.py:649} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-02T01:27:38.973+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.1 from central in [default]
[2025-05-02T01:27:38.974+0000] {spark_submit.py:649} INFO - com.google.code.gson#gson;2.2.4 from central in [default]
[2025-05-02T01:27:38.975+0000] {spark_submit.py:649} INFO - com.google.guava#guava;19.0 from central in [default]
[2025-05-02T01:27:38.975+0000] {spark_submit.py:649} INFO - com.google.inject#guice;4.0 from central in [default]
[2025-05-02T01:27:38.976+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-assistedinject;3.0 from central in [default]
[2025-05-02T01:27:38.977+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-servlet;4.0 from central in [default]
[2025-05-02T01:27:38.978+0000] {spark_submit.py:649} INFO - com.google.protobuf#protobuf-java;2.5.0 from central in [default]
[2025-05-02T01:27:38.980+0000] {spark_submit.py:649} INFO - com.google.re2j#re2j;1.1 from central in [default]
[2025-05-02T01:27:38.981+0000] {spark_submit.py:649} INFO - com.jcraft#jsch;0.1.54 from central in [default]
[2025-05-02T01:27:38.981+0000] {spark_submit.py:649} INFO - com.jolbox#bonecp;0.8.0.RELEASE from central in [default]
[2025-05-02T01:27:38.982+0000] {spark_submit.py:649} INFO - com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 from central in [default]
[2025-05-02T01:27:38.983+0000] {spark_submit.py:649} INFO - com.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]
[2025-05-02T01:27:38.984+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-client;1.19 from central in [default]
[2025-05-02T01:27:38.984+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-core;1.19 from central in [default]
[2025-05-02T01:27:38.985+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-json;1.19 from central in [default]
[2025-05-02T01:27:38.986+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-server;1.19 from central in [default]
[2025-05-02T01:27:38.987+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-servlet;1.19 from central in [default]
[2025-05-02T01:27:38.988+0000] {spark_submit.py:649} INFO - com.sun.jersey.contribs#jersey-guice;1.19 from central in [default]
[2025-05-02T01:27:38.989+0000] {spark_submit.py:649} INFO - com.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]
[2025-05-02T01:27:38.990+0000] {spark_submit.py:649} INFO - com.tdunning#json;1.8 from central in [default]
[2025-05-02T01:27:38.990+0000] {spark_submit.py:649} INFO - com.thoughtworks.paranamer#paranamer;2.7 from central in [default]
[2025-05-02T01:27:38.991+0000] {spark_submit.py:649} INFO - com.vlkan#flatbuffers;1.2.0-3f79e055 from central in [default]
[2025-05-02T01:27:38.992+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#memory;0.9.0 from central in [default]
[2025-05-02T01:27:38.992+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#sketches-core;0.9.0 from central in [default]
[2025-05-02T01:27:38.993+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP;2.6.1 from central in [default]
[2025-05-02T01:27:38.994+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP-java7;2.4.12 from central in [default]
[2025-05-02T01:27:38.995+0000] {spark_submit.py:649} INFO - commons-beanutils#commons-beanutils;1.9.3 from central in [default]
[2025-05-02T01:27:38.996+0000] {spark_submit.py:649} INFO - commons-cli#commons-cli;1.2 from central in [default]
[2025-05-02T01:27:38.997+0000] {spark_submit.py:649} INFO - commons-codec#commons-codec;1.15 from central in [default]
[2025-05-02T01:27:38.998+0000] {spark_submit.py:649} INFO - commons-collections#commons-collections;3.2.2 from central in [default]
[2025-05-02T01:27:38.998+0000] {spark_submit.py:649} INFO - commons-daemon#commons-daemon;1.0.13 from central in [default]
[2025-05-02T01:27:38.999+0000] {spark_submit.py:649} INFO - commons-dbcp#commons-dbcp;1.4 from central in [default]
[2025-05-02T01:27:39.000+0000] {spark_submit.py:649} INFO - commons-io#commons-io;2.6 from central in [default]
[2025-05-02T01:27:39.001+0000] {spark_submit.py:649} INFO - commons-lang#commons-lang;2.6 from central in [default]
[2025-05-02T01:27:39.001+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.2 from central in [default]
[2025-05-02T01:27:39.002+0000] {spark_submit.py:649} INFO - commons-net#commons-net;3.6 from central in [default]
[2025-05-02T01:27:39.003+0000] {spark_submit.py:649} INFO - commons-pool#commons-pool;1.5.4 from central in [default]
[2025-05-02T01:27:39.003+0000] {spark_submit.py:649} INFO - de.ruedigermoeller#fst;2.50 from central in [default]
[2025-05-02T01:27:39.004+0000] {spark_submit.py:649} INFO - dnsjava#dnsjava;2.1.7 from central in [default]
[2025-05-02T01:27:39.005+0000] {spark_submit.py:649} INFO - io.airlift#aircompressor;0.10 from central in [default]
[2025-05-02T01:27:39.005+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.2.1 from central in [default]
[2025-05-02T01:27:39.006+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-json;3.1.0 from central in [default]
[2025-05-02T01:27:39.007+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-jvm;3.1.0 from central in [default]
[2025-05-02T01:27:39.007+0000] {spark_submit.py:649} INFO - io.netty#netty;3.7.0.Final from central in [default]
[2025-05-02T01:27:39.008+0000] {spark_submit.py:649} INFO - io.netty#netty-buffer;4.1.17.Final from central in [default]
[2025-05-02T01:27:39.009+0000] {spark_submit.py:649} INFO - io.netty#netty-common;4.1.17.Final from central in [default]
[2025-05-02T01:27:39.010+0000] {spark_submit.py:649} INFO - it.unimi.dsi#fastutil;6.5.6 from central in [default]
[2025-05-02T01:27:39.010+0000] {spark_submit.py:649} INFO - jakarta.activation#jakarta.activation-api;1.2.1 from central in [default]
[2025-05-02T01:27:39.011+0000] {spark_submit.py:649} INFO - jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]
[2025-05-02T01:27:39.012+0000] {spark_submit.py:649} INFO - javax.inject#javax.inject;1 from central in [default]
[2025-05-02T01:27:39.013+0000] {spark_submit.py:649} INFO - javax.jdo#jdo-api;3.0.1 from central in [default]
[2025-05-02T01:27:39.014+0000] {spark_submit.py:649} INFO - javax.servlet#javax.servlet-api;3.1.0 from central in [default]
[2025-05-02T01:27:39.015+0000] {spark_submit.py:649} INFO - javax.servlet.jsp#jsp-api;2.1 from central in [default]
[2025-05-02T01:27:39.016+0000] {spark_submit.py:649} INFO - javax.transaction#jta;1.1 from central in [default]
[2025-05-02T01:27:39.016+0000] {spark_submit.py:649} INFO - javax.transaction#transaction-api;1.1 from central in [default]
[2025-05-02T01:27:39.017+0000] {spark_submit.py:649} INFO - javax.ws.rs#jsr311-api;1.1.1 from central in [default]
[2025-05-02T01:27:39.018+0000] {spark_submit.py:649} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2025-05-02T01:27:39.018+0000] {spark_submit.py:649} INFO - javolution#javolution;5.5.1 from central in [default]
[2025-05-02T01:27:39.019+0000] {spark_submit.py:649} INFO - jline#jline;2.12 from central in [default]
[2025-05-02T01:27:39.020+0000] {spark_submit.py:649} INFO - joda-time#joda-time;2.9.9 from central in [default]
[2025-05-02T01:27:39.021+0000] {spark_submit.py:649} INFO - junit#junit;4.11 from central in [default]
[2025-05-02T01:27:39.021+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.17 from central in [default]
[2025-05-02T01:27:39.022+0000] {spark_submit.py:649} INFO - net.minidev#accessors-smart;1.2 from central in [default]
[2025-05-02T01:27:39.022+0000] {spark_submit.py:649} INFO - net.minidev#json-smart;2.3 from central in [default]
[2025-05-02T01:27:39.023+0000] {spark_submit.py:649} INFO - net.sf.jpam#jpam;1.1 from central in [default]
[2025-05-02T01:27:39.024+0000] {spark_submit.py:649} INFO - net.sf.opencsv#opencsv;2.3 from central in [default]
[2025-05-02T01:27:39.025+0000] {spark_submit.py:649} INFO - org.antlr#ST4;4.0.4 from central in [default]
[2025-05-02T01:27:39.026+0000] {spark_submit.py:649} INFO - org.antlr#antlr-runtime;3.5.2 from central in [default]
[2025-05-02T01:27:39.027+0000] {spark_submit.py:649} INFO - org.apache.ant#ant;1.9.1 from central in [default]
[2025-05-02T01:27:39.027+0000] {spark_submit.py:649} INFO - org.apache.ant#ant-launcher;1.9.1 from central in [default]
[2025-05-02T01:27:39.028+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-format;0.8.0 from central in [default]
[2025-05-02T01:27:39.029+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-memory;0.8.0 from central in [default]
[2025-05-02T01:27:39.030+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-vector;0.8.0 from central in [default]
[2025-05-02T01:27:39.031+0000] {spark_submit.py:649} INFO - org.apache.avro#avro;1.8.2 from central in [default]
[2025-05-02T01:27:39.032+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-core;1.16.0 from central in [default]
[2025-05-02T01:27:39.033+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-linq4j;1.16.0 from central in [default]
[2025-05-02T01:27:39.033+0000] {spark_submit.py:649} INFO - org.apache.calcite.avatica#avatica;1.11.0 from central in [default]
[2025-05-02T01:27:39.034+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-compress;1.19 from central in [default]
[2025-05-02T01:27:39.035+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-configuration2;2.1.1 from central in [default]
[2025-05-02T01:27:39.036+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-crypto;1.0.0 from central in [default]
[2025-05-02T01:27:39.037+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.9 from central in [default]
[2025-05-02T01:27:39.037+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-math3;3.1.1 from central in [default]
[2025-05-02T01:27:39.038+0000] {spark_submit.py:649} INFO - org.apache.derby#derby;10.14.1.0 from central in [default]
[2025-05-02T01:27:39.039+0000] {spark_submit.py:649} INFO - org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 from central in [default]
[2025-05-02T01:27:39.040+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-annotations;3.1.0 from central in [default]
[2025-05-02T01:27:39.041+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-auth;3.1.0 from central in [default]
[2025-05-02T01:27:39.041+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-02T01:27:39.042+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-02T01:27:39.043+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-common;3.1.0 from central in [default]
[2025-05-02T01:27:39.044+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-api;3.1.0 from central in [default]
[2025-05-02T01:27:39.045+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-common;3.1.0 from central in [default]
[2025-05-02T01:27:39.045+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-registry;3.1.0 from central in [default]
[2025-05-02T01:27:39.046+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 from central in [default]
[2025-05-02T01:27:39.047+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-common;3.1.0 from central in [default]
[2025-05-02T01:27:39.048+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 from central in [default]
[2025-05-02T01:27:39.049+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 from central in [default]
[2025-05-02T01:27:39.050+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-client;2.0.0-alpha4 from central in [default]
[2025-05-02T01:27:39.051+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol;2.0.0-alpha4 from central in [default]
[2025-05-02T01:27:39.051+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 from central in [default]
[2025-05-02T01:27:39.052+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 from central in [default]
[2025-05-02T01:27:39.053+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 from central in [default]
[2025-05-02T01:27:39.054+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 from central in [default]
[2025-05-02T01:27:39.055+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-classification;3.1.3 from central in [default]
[2025-05-02T01:27:39.056+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common;3.1.3 from central in [default]
[2025-05-02T01:27:39.056+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec;3.1.3 from central in [default]
[2025-05-02T01:27:39.057+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-client;3.1.3 from central in [default]
[2025-05-02T01:27:39.058+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-common;3.1.3 from central in [default]
[2025-05-02T01:27:39.059+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-tez;3.1.3 from central in [default]
[2025-05-02T01:27:39.060+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore;3.1.3 from central in [default]
[2025-05-02T01:27:39.061+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde;3.1.3 from central in [default]
[2025-05-02T01:27:39.062+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-service-rpc;3.1.3 from central in [default]
[2025-05-02T01:27:39.062+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-shims;3.1.3 from central in [default]
[2025-05-02T01:27:39.063+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-standalone-metastore;3.1.3 from central in [default]
[2025-05-02T01:27:39.065+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-storage-api;2.7.0 from central in [default]
[2025-05-02T01:27:39.065+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-upgrade-acid;3.1.3 from central in [default]
[2025-05-02T01:27:39.066+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-0.23;3.1.3 from central in [default]
[2025-05-02T01:27:39.067+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-common;3.1.3 from central in [default]
[2025-05-02T01:27:39.068+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-scheduler;3.1.3 from central in [default]
[2025-05-02T01:27:39.068+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core;3.2.0-incubating from central in [default]
[2025-05-02T01:27:39.069+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
[2025-05-02T01:27:39.069+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpclient;4.5.13 from central in [default]
[2025-05-02T01:27:39.070+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpcore;4.4.13 from central in [default]
[2025-05-02T01:27:39.071+0000] {spark_submit.py:649} INFO - org.apache.ivy#ivy;2.4.0 from central in [default]
[2025-05-02T01:27:39.071+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-admin;1.0.1 from central in [default]
[2025-05-02T01:27:39.072+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-client;1.0.1 from central in [default]
[2025-05-02T01:27:39.073+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-common;1.0.1 from central in [default]
[2025-05-02T01:27:39.073+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-core;1.0.1 from central in [default]
[2025-05-02T01:27:39.074+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-crypto;1.0.1 from central in [default]
[2025-05-02T01:27:39.075+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-identity;1.0.1 from central in [default]
[2025-05-02T01:27:39.075+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-server;1.0.1 from central in [default]
[2025-05-02T01:27:39.076+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]
[2025-05-02T01:27:39.077+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-util;1.0.1 from central in [default]
[2025-05-02T01:27:39.078+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-asn1;1.0.1 from central in [default]
[2025-05-02T01:27:39.079+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-config;1.0.1 from central in [default]
[2025-05-02T01:27:39.080+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-pkix;1.0.1 from central in [default]
[2025-05-02T01:27:39.081+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-util;1.0.1 from central in [default]
[2025-05-02T01:27:39.082+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-xdr;1.0.1 from central in [default]
[2025-05-02T01:27:39.083+0000] {spark_submit.py:649} INFO - org.apache.kerby#token-provider;1.0.1 from central in [default]
[2025-05-02T01:27:39.084+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-1.2-api;2.17.1 from central in [default]
[2025-05-02T01:27:39.085+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api;2.10.0 from central in [default]
[2025-05-02T01:27:39.086+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-core;2.17.1 from central in [default]
[2025-05-02T01:27:39.087+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 from central in [default]
[2025-05-02T01:27:39.090+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-web;2.17.1 from central in [default]
[2025-05-02T01:27:39.092+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-core;1.5.8 from central in [default]
[2025-05-02T01:27:39.096+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-shims;1.5.8 from central in [default]
[2025-05-02T01:27:39.097+0000] {spark_submit.py:649} INFO - org.apache.parquet#parquet-hadoop-bundle;1.10.0 from central in [default]
[2025-05-02T01:27:39.101+0000] {spark_submit.py:649} INFO - org.apache.thrift#libfb303;0.9.3 from central in [default]
[2025-05-02T01:27:39.102+0000] {spark_submit.py:649} INFO - org.apache.thrift#libthrift;0.9.3 from central in [default]
[2025-05-02T01:27:39.103+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-api;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.105+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-common;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.108+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-core;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.109+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-api;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.114+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-core;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.116+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-zookeeper;0.6.0-incubating from central in [default]
[2025-05-02T01:27:39.120+0000] {spark_submit.py:649} INFO - org.apache.yetus#audience-annotations;0.5.0 from central in [default]
[2025-05-02T01:27:39.122+0000] {spark_submit.py:649} INFO - org.apache.zookeeper#zookeeper;3.4.6 from central in [default]
[2025-05-02T01:27:39.123+0000] {spark_submit.py:649} INFO - org.codehaus.groovy#groovy-all;2.4.11 from central in [default]
[2025-05-02T01:27:39.125+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2025-05-02T01:27:39.129+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]
[2025-05-02T01:27:39.132+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2025-05-02T01:27:39.134+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-xc;1.9.13 from central in [default]
[2025-05-02T01:27:39.136+0000] {spark_submit.py:649} INFO - org.codehaus.janino#commons-compiler;2.7.6 from central in [default]
[2025-05-02T01:27:39.139+0000] {spark_submit.py:649} INFO - org.codehaus.janino#janino;2.7.6 from central in [default]
[2025-05-02T01:27:39.143+0000] {spark_submit.py:649} INFO - org.codehaus.jettison#jettison;1.1 from central in [default]
[2025-05-02T01:27:39.146+0000] {spark_submit.py:649} INFO - org.codehaus.woodstox#stax2-api;3.1.4 from central in [default]
[2025-05-02T01:27:39.147+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]
[2025-05-02T01:27:39.148+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-core;4.1.17 from central in [default]
[2025-05-02T01:27:39.149+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]
[2025-05-02T01:27:39.150+0000] {spark_submit.py:649} INFO - org.datanucleus#javax.jdo;3.2.0-m3 from central in [default]
[2025-05-02T01:27:39.151+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-client;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.152+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-http;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.154+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-io;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.155+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.156+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-security;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.157+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-server;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.158+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.159+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util;9.3.19.v20170502 from central in [default]
[2025-05-02T01:27:39.161+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 from central in [default]
[2025-05-02T01:27:39.163+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.166+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-xml;9.3.20.v20170531 from central in [default]
[2025-05-02T01:27:39.167+0000] {spark_submit.py:649} INFO - org.ehcache#ehcache;3.3.1 from central in [default]
[2025-05-02T01:27:39.168+0000] {spark_submit.py:649} INFO - org.fusesource.leveldbjni#leveldbjni-all;1.8 from central in [default]
[2025-05-02T01:27:39.170+0000] {spark_submit.py:649} INFO - org.hamcrest#hamcrest-core;1.3 from central in [default]
[2025-05-02T01:27:39.172+0000] {spark_submit.py:649} INFO - org.jruby.jcodings#jcodings;1.0.18 from central in [default]
[2025-05-02T01:27:39.173+0000] {spark_submit.py:649} INFO - org.jruby.joni#joni;2.1.11 from central in [default]
[2025-05-02T01:27:39.174+0000] {spark_submit.py:649} INFO - org.ow2.asm#asm;5.0.4 from central in [default]
[2025-05-02T01:27:39.175+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.36 from central in [default]
[2025-05-02T01:27:39.176+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-log4j12;1.6.1 from central in [default]
[2025-05-02T01:27:39.179+0000] {spark_submit.py:649} INFO - org.tukaani#xz;1.5 from central in [default]
[2025-05-02T01:27:39.180+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
[2025-05-02T01:27:39.182+0000] {spark_submit.py:649} INFO - sqlline#sqlline;1.3.0 from central in [default]
[2025-05-02T01:27:39.183+0000] {spark_submit.py:649} INFO - stax#stax-api;1.0.1 from central in [default]
[2025-05-02T01:27:39.184+0000] {spark_submit.py:649} INFO - :: evicted modules:
[2025-05-02T01:27:39.185+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.10 by [org.slf4j#slf4j-api;1.7.36] in [default]
[2025-05-02T01:27:39.186+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.16 by [log4j#log4j;1.2.17] in [default]
[2025-05-02T01:27:39.187+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-02T01:27:39.188+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.4 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T01:27:39.188+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.4 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]
[2025-05-02T01:27:39.189+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-02T01:27:39.190+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.0.4 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-02T01:27:39.191+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.0 by [io.dropwizard.metrics#metrics-core;3.2.1] in [default]
[2025-05-02T01:27:39.191+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.2 by [io.dropwizard.metrics#metrics-core;3.1.0] in [default]
[2025-05-02T01:27:39.192+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.2 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-02T01:27:39.193+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.2 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T01:27:39.195+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.6 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T01:27:39.202+0000] {spark_submit.py:649} INFO - com.google.inject#guice;3.0 by [com.google.inject#guice;4.0] in [default]
[2025-05-02T01:27:39.204+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;2.0.1 by [com.google.code.findbugs#jsr305;3.0.0] in [default]
[2025-05-02T01:27:39.205+0000] {spark_submit.py:649} INFO - com.google.guava#guava;14.0.1 by [com.google.guava#guava;19.0] in [default]
[2025-05-02T01:27:39.207+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:39.208+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-02T01:27:39.209+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-02T01:27:39.210+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:39.211+0000] {spark_submit.py:649} INFO - |      default     |  224  |   0   |   0   |   15  ||  209  |   0   |
[2025-05-02T01:27:39.213+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T01:27:39.214+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-daf3dd70-9e2f-499e-a653-203a895746ea
[2025-05-02T01:27:39.216+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T01:27:39.217+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 209 already retrieved (0kB/120ms)
[2025-05-02T01:27:40.432+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:40 INFO IsolatedClientLoader: Downloaded metastore jars to /tmp/hive-v3_1-24e51972-a46d-40e8-9ea1-46b3b73b5cf2
[2025-05-02T01:27:41.261+0000] {spark_submit.py:649} INFO - Hive Session ID = bc1924fe-e742-403f-a12f-a389672d48f5
[2025-05-02T01:27:41.397+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:41 INFO HiveClientImpl: Warehouse location for Hive client (version 3.1.3) is file:/opt/***/spark-warehouse
[2025-05-02T01:27:42.524+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:42 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-05-02T01:27:42.562+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:42 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-05-02T01:27:42.564+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:42 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-05-02T01:27:44.587+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:44 INFO InMemoryFileIndex: It took 148 ms to list leaf files for 1 paths.
[2025-05-02T01:27:46.817+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:46 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
[2025-05-02T01:27:47.106+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:47 INFO DeltaLog: Loading version 0.
[2025-05-02T01:27:48.209+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:48 INFO Snapshot: [tableId=fa860813-daf3-4046-be05-c396bb34df06] Created snapshot Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log, version=0, metadata=Metadata(7ee2c826-febc-4225-9876-9688319b91cc,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746146696394)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json; isDirectory=false; length=6036; replication=1; blocksize=33554432; modification_time=1746146720639; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=6167abc40085b1bbc93d5c273db01fac versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@5325be85,1746146720639), checksumOpt=Some(VersionChecksum(Some(a5f04c07-1585-427a-961a-33c535d2bc31),31301816,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(7ee2c826-febc-4225-9876-9688319b91cc,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746146696394)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-2cd9a2fd-3b86-4eb7-9653-5e4bd91a2550-c000.snappy.parquet,Map(),31301816,1746146714000,false,{"numRecords":111602,"minValues":{"ID":"00011a5a-4cca-c00f-1fe2-a33ff195","CLAIMID":"0001da0a-1087-7b47-dc4e-c62c3df4","CHARGEID":0,"PATIENTID":"00732e11-5e4d-37b7-01f8-929a2553","TYPE":"CHARGE","AMOUNT":0.0,"METHOD":"CASH","FROMDATE":"1931-04-21T11:59:06.000Z","TODATE":"1931-04-21T12:19:24.000Z","PLACEOFSERVICE":"00ffb204-07b4-3c72-a678-fa252e6c","PROCEDURECODE":3,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":1,"NOTES":"0.25 ML Leuprolide Acetate 30 MG","UNITAMOUNT":0.0,"TRANSFEROUTID":1,"TRANSFERTYPE":"1","PAYMENTS":0.0,"ADJUSTMENTS":0.0,"TRANSFERS":0.01,"OUTSTANDING":0.0,"APPOINTMENTID":"0004591c-7253-3a60-3435-8bd8dd25","PATIENTINSURANCEID":"00732e11-5e4d-37b7-01f8-929a2553","FEESCHEDULEID":1,"PROVIDERID":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"ffffad8f-7a4a-8675-09fa-0ada11bd","CLAIMID":"ffff94bb-e888-c037-6e14-76215741","CHARGEID":143657,"PATIENTID":"fb164202-4e38-04a0-470a-b7229db1","TYPE":"TRANSFEROUT","AMOUNT":103958.4,"METHOD":"ECHECK","FROMDATE":"2024-11-04T08:21:17.000Z","TODATE":"2024-11-05T00:34:10.000Z","PLACEOFSERVICE":"fe30f2b4-9bc8-346e-afba-4455d176","PROCEDURECODE":456191000124101,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":20,"NOTES":"zoster vaccine  live","UNITAMOUNT":103958.4,"TRANSFEROUTID":143655,"TRANSFERTYPE":"p","PAYMENTS":91678.0,"ADJUSTMENTS":0.0,"TRANSFERS":31187.52,"OUTSTANDING":91678.0,"APPOINTMENTID":"fffa37f5-08f3-5c8a-09e0-a379d113","PATIENTINSURANCEID":"fb164202-4e38-04a0-470a-b7229db1","FEESCHEDULEID":1,"PROVIDERID":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"CLAIMID":0,"CHARGEID":0,"PATIENTID":0,"TYPE":0,"AMOUNT":60590,"METHOD":69967,"FROMDATE":0,"TODATE":0,"PLACEOFSERVICE":0,"PROCEDURECODE":0,"MODIFIER1":111602,"MODIFIER2":111602,"DIAGNOSISREF1":0,"DIAGNOSISREF2":74224,"DIAGNOSISREF3":101620,"DIAGNOSISREF4":108971,"UNITS":0,"DEPARTMENTID":0,"NOTES":0,"UNITAMOUNT":0,"TRANSFEROUTID":92647,"TRANSFERTYPE":60590,"PAYMENTS":0,"ADJUSTMENTS":0,"TRANSFERS":73692,"OUTSTANDING":0,"APPOINTMENTID":0,"LINENOTE":111602,"PATIENTINSURANCEID":7254,"FEESCHEDULEID":0,"PROVIDERID":0}},null,null,None,None,None))))))
[2025-05-02T01:27:48.229+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:48 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log, version=0, metadata=Metadata(7ee2c826-febc-4225-9876-9688319b91cc,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746146696394)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json; isDirectory=false; length=6036; replication=1; blocksize=33554432; modification_time=1746146720639; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=6167abc40085b1bbc93d5c273db01fac versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@5325be85,1746146720639), checksumOpt=Some(VersionChecksum(Some(a5f04c07-1585-427a-961a-33c535d2bc31),31301816,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(7ee2c826-febc-4225-9876-9688319b91cc,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"CLAIMID","type":"string","nullable":true,"metadata":{}},{"name":"CHARGEID","type":"long","nullable":true,"metadata":{}},{"name":"PATIENTID","type":"string","nullable":true,"metadata":{}},{"name":"TYPE","type":"string","nullable":true,"metadata":{}},{"name":"AMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"METHOD","type":"string","nullable":true,"metadata":{}},{"name":"FROMDATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"TODATE","type":"timestamp","nullable":true,"metadata":{}},{"name":"PLACEOFSERVICE","type":"string","nullable":true,"metadata":{}},{"name":"PROCEDURECODE","type":"long","nullable":true,"metadata":{}},{"name":"MODIFIER1","type":"string","nullable":true,"metadata":{}},{"name":"MODIFIER2","type":"string","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF1","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF2","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF3","type":"long","nullable":true,"metadata":{}},{"name":"DIAGNOSISREF4","type":"long","nullable":true,"metadata":{}},{"name":"UNITS","type":"long","nullable":true,"metadata":{}},{"name":"DEPARTMENTID","type":"long","nullable":true,"metadata":{}},{"name":"NOTES","type":"string","nullable":true,"metadata":{}},{"name":"UNITAMOUNT","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFEROUTID","type":"long","nullable":true,"metadata":{}},{"name":"TRANSFERTYPE","type":"string","nullable":true,"metadata":{}},{"name":"PAYMENTS","type":"double","nullable":true,"metadata":{}},{"name":"ADJUSTMENTS","type":"double","nullable":true,"metadata":{}},{"name":"TRANSFERS","type":"double","nullable":true,"metadata":{}},{"name":"OUTSTANDING","type":"double","nullable":true,"metadata":{}},{"name":"APPOINTMENTID","type":"string","nullable":true,"metadata":{}},{"name":"LINENOTE","type":"string","nullable":true,"metadata":{}},{"name":"PATIENTINSURANCEID","type":"string","nullable":true,"metadata":{}},{"name":"FEESCHEDULEID","type":"long","nullable":true,"metadata":{}},{"name":"PROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"SUPERVISINGPROVIDERID","type":"string","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746146696394)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-2cd9a2fd-3b86-4eb7-9653-5e4bd91a2550-c000.snappy.parquet,Map(),31301816,1746146714000,false,{"numRecords":111602,"minValues":{"ID":"00011a5a-4cca-c00f-1fe2-a33ff195","CLAIMID":"0001da0a-1087-7b47-dc4e-c62c3df4","CHARGEID":0,"PATIENTID":"00732e11-5e4d-37b7-01f8-929a2553","TYPE":"CHARGE","AMOUNT":0.0,"METHOD":"CASH","FROMDATE":"1931-04-21T11:59:06.000Z","TODATE":"1931-04-21T12:19:24.000Z","PLACEOFSERVICE":"00ffb204-07b4-3c72-a678-fa252e6c","PROCEDURECODE":3,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":1,"NOTES":"0.25 ML Leuprolide Acetate 30 MG","UNITAMOUNT":0.0,"TRANSFEROUTID":1,"TRANSFERTYPE":"1","PAYMENTS":0.0,"ADJUSTMENTS":0.0,"TRANSFERS":0.01,"OUTSTANDING":0.0,"APPOINTMENTID":"0004591c-7253-3a60-3435-8bd8dd25","PATIENTINSURANCEID":"00732e11-5e4d-37b7-01f8-929a2553","FEESCHEDULEID":1,"PROVIDERID":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"ffffad8f-7a4a-8675-09fa-0ada11bd","CLAIMID":"ffff94bb-e888-c037-6e14-76215741","CHARGEID":143657,"PATIENTID":"fb164202-4e38-04a0-470a-b7229db1","TYPE":"TRANSFEROUT","AMOUNT":103958.4,"METHOD":"ECHECK","FROMDATE":"2024-11-04T08:21:17.000Z","TODATE":"2024-11-05T00:34:10.000Z","PLACEOFSERVICE":"fe30f2b4-9bc8-346e-afba-4455d176","PROCEDURECODE":456191000124101,"DIAGNOSISREF1":1,"DIAGNOSISREF2":2,"DIAGNOSISREF3":3,"DIAGNOSISREF4":4,"UNITS":1,"DEPARTMENTID":20,"NOTES":"zoster vaccine  live","UNITAMOUNT":103958.4,"TRANSFEROUTID":143655,"TRANSFERTYPE":"p","PAYMENTS":91678.0,"ADJUSTMENTS":0.0,"TRANSFERS":31187.52,"OUTSTANDING":91678.0,"APPOINTMENTID":"fffa37f5-08f3-5c8a-09e0-a379d113","PATIENTINSURANCEID":"fb164202-4e38-04a0-470a-b7229db1","FEESCHEDULEID":1,"PROVIDERID":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"CLAIMID":0,"CHARGEID":0,"PATIENTID":0,"TYPE":0,"AMOUNT":60590,"METHOD":69967,"FROMDATE":0,"TODATE":0,"PLACEOFSERVICE":0,"PROCEDURECODE":0,"MODIFIER1":111602,"MODIFIER2":111602,"DIAGNOSISREF1":0,"DIAGNOSISREF2":74224,"DIAGNOSISREF3":101620,"DIAGNOSISREF4":108971,"UNITS":0,"DEPARTMENTID":0,"NOTES":0,"UNITAMOUNT":0,"TRANSFEROUTID":92647,"TRANSFERTYPE":60590,"PAYMENTS":0,"ADJUSTMENTS":0,"TRANSFERS":73692,"OUTSTANDING":0,"APPOINTMENTID":0,"LINENOTE":111602,"PATIENTINSURANCEID":7254,"FEESCHEDULEID":0,"PROVIDERID":0}},null,null,None,None,None))))))
[2025-05-02T01:27:50.378+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO MergeIntoCommand: DELTA: MERGE operation - materialize source
[2025-05-02T01:27:50.392+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO MergeIntoCommand: DELTA: Done
[2025-05-02T01:27:50.396+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO MergeIntoCommand: DELTA: MERGE operation - writing new files for only inserts
[2025-05-02T01:27:50.410+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO Snapshot: DELTA: Compute snapshot for version: 0
[2025-05-02T01:27:50.531+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 204.7 KiB, free 434.2 MiB)
[2025-05-02T01:27:50.636+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 434.2 MiB)
[2025-05-02T01:27:50.640+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ***-scheduler:40993 (size: 35.8 KiB, free: 434.4 MiB)
[2025-05-02T01:27:50.648+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:50 INFO SparkContext: Created broadcast 0 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:27:52.296+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:52 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 6036)
[2025-05-02T01:27:54.535+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:54 INFO DataSourceStrategy: Pruning directories with:
[2025-05-02T01:27:54.541+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:54 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T01:27:54.547+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:54 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T01:27:54.795+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-05-02T01:27:56.561+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO CodeGenerator: Code generated in 788.360039 ms
[2025-05-02T01:27:56.570+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 205.0 KiB, free 434.0 MiB)
[2025-05-02T01:27:56.596+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.9 MiB)
[2025-05-02T01:27:56.598+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ***-scheduler:40993 (size: 35.9 KiB, free: 434.3 MiB)
[2025-05-02T01:27:56.601+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO SparkContext: Created broadcast 1 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:27:56.671+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4200340 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T01:27:57.175+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Registering RDD 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 0
[2025-05-02T01:27:57.188+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Got map stage job 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T01:27:57.189+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Final stage: ShuffleMapStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:27:57.191+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T01:27:57.192+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:27:57.199+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:27:57.405+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 105.9 KiB, free 433.8 MiB)
[2025-05-02T01:27:57.411+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.8 MiB)
[2025-05-02T01:27:57.414+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ***-scheduler:40993 (size: 32.6 KiB, free: 434.3 MiB)
[2025-05-02T01:27:57.416+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:27:57.454+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T01:27:57.458+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-02T01:27:57.570+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10901 bytes)
[2025-05-02T01:27:57.601+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-02T01:27:58.269+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO CodeGenerator: Code generated in 361.265178 ms
[2025-05-02T01:27:58.347+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO CodeGenerator: Code generated in 33.269272 ms
[2025-05-02T01:27:58.375+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/_delta_log/00000000000000000000.json, range: 0-6036, partition values: [0]
[2025-05-02T01:27:58.608+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO CodeGenerator: Code generated in 189.048381 ms
[2025-05-02T01:27:58.862+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO CodeGenerator: Code generated in 15.249088 ms
[2025-05-02T01:27:58.891+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO CodeGenerator: Code generated in 18.272953 ms
[2025-05-02T01:27:58.978+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1927 bytes result sent to driver
[2025-05-02T01:27:59.001+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1457 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T01:27:59.005+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-02T01:27:59.020+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO DAGScheduler: ShuffleMapStage 0 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.786 s
[2025-05-02T01:27:59.022+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T01:27:59.023+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO DAGScheduler: running: Set()
[2025-05-02T01:27:59.024+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO DAGScheduler: waiting: Set()
[2025-05-02T01:27:59.025+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO DAGScheduler: failed: Set()
[2025-05-02T01:27:59.887+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-02T01:27:59.888+0000] {spark_submit.py:649} INFO - 25/05/02 01:27:59 INFO CodeGenerator: Code generated in 451.441813 ms
[2025-05-02T01:28:00.033+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO CodeGenerator: Code generated in 102.893541 ms
[2025-05-02T01:28:00.805+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO CodeGenerator: Code generated in 113.143881 ms
[2025-05-02T01:28:00.826+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Registering RDD 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2025-05-02T01:28:00.828+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Got map stage job 1 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-02T01:28:00.828+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Final stage: ShuffleMapStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:28:00.829+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-05-02T01:28:00.836+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:00.837+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:28:00.963+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 605.6 KiB, free 433.2 MiB)
[2025-05-02T01:28:00.973+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 138.8 KiB, free 433.1 MiB)
[2025-05-02T01:28:00.976+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ***-scheduler:40993 (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-02T01:28:00.979+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:00.983+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T01:28:00.985+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 50 tasks resource profile 0
[2025-05-02T01:28:01.003+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 1) (***-scheduler, executor driver, partition 23, NODE_LOCAL, 10195 bytes)
[2025-05-02T01:28:01.006+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO Executor: Running task 23.0 in stage 2.0 (TID 1)
[2025-05-02T01:28:01.185+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ***-scheduler:40993 in memory (size: 32.6 KiB, free: 434.2 MiB)
[2025-05-02T01:28:01.314+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:01.319+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms
[2025-05-02T01:28:01.497+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO CodeGenerator: Code generated in 167.624756 ms
[2025-05-02T01:28:01.562+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO CodeGenerator: Code generated in 38.885097 ms
[2025-05-02T01:28:01.612+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:01 INFO CodeGenerator: Code generated in 19.175986 ms
[2025-05-02T01:28:02.145+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:02 INFO CodeGenerator: Code generated in 252.633499 ms
[2025-05-02T01:28:03.008+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-02T01:28:03.009+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 799.582725 ms
[2025-05-02T01:28:03.040+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO MemoryStore: Block rdd_10_23 stored as values in memory (estimated size 1614.0 B, free 433.2 MiB)
[2025-05-02T01:28:03.042+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO BlockManagerInfo: Added rdd_10_23 in memory on ***-scheduler:40993 (size: 1614.0 B, free: 434.2 MiB)
[2025-05-02T01:28:03.187+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 136.261531 ms
[2025-05-02T01:28:03.240+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 10.703643 ms
[2025-05-02T01:28:03.315+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 18.91643 ms
[2025-05-02T01:28:03.329+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 10.404905 ms
[2025-05-02T01:28:03.373+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 27.961815 ms
[2025-05-02T01:28:03.407+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 14.821629 ms
[2025-05-02T01:28:03.441+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 16.408505 ms
[2025-05-02T01:28:03.462+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO CodeGenerator: Code generated in 13.677565 ms
[2025-05-02T01:28:03.471+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO Executor: Finished task 23.0 in stage 2.0 (TID 1). 5308 bytes result sent to driver
[2025-05-02T01:28:03.474+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 2) (***-scheduler, executor driver, partition 42, NODE_LOCAL, 10195 bytes)
[2025-05-02T01:28:03.475+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 1) in 2478 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T01:28:03.477+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO Executor: Running task 42.0 in stage 2.0 (TID 2)
[2025-05-02T01:28:03.540+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:03.541+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:03.743+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO MemoryStore: Block rdd_10_42 stored as values in memory (estimated size 872.0 B, free 433.2 MiB)
[2025-05-02T01:28:03.745+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO BlockManagerInfo: Added rdd_10_42 in memory on ***-scheduler:40993 (size: 872.0 B, free: 434.2 MiB)
[2025-05-02T01:28:03.824+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO Executor: Finished task 42.0 in stage 2.0 (TID 2). 5308 bytes result sent to driver
[2025-05-02T01:28:03.829+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:03.830+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-05-02T01:28:03.832+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 2) in 357 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T01:28:03.878+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:03.880+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:04.031+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO MemoryStore: Block rdd_10_0 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:04.033+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO BlockManagerInfo: Added rdd_10_0 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:04.159+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 5351 bytes result sent to driver
[2025-05-02T01:28:04.164+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:04.167+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-05-02T01:28:04.169+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 339 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T01:28:04.227+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:04.229+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2025-05-02T01:28:04.333+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO MemoryStore: Block rdd_10_1 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:04.335+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO BlockManagerInfo: Added rdd_10_1 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:04.416+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 5308 bytes result sent to driver
[2025-05-02T01:28:04.419+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:04.421+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-05-02T01:28:04.423+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 258 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T01:28:04.478+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:04.480+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:04.572+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO MemoryStore: Block rdd_10_2 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:04.574+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO BlockManagerInfo: Added rdd_10_2 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:04.652+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 5308 bytes result sent to driver
[2025-05-02T01:28:04.656+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:04.658+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 239 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T01:28:04.659+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-05-02T01:28:04.703+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:04.704+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:04.803+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO MemoryStore: Block rdd_10_3 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:04.805+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO BlockManagerInfo: Added rdd_10_3 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:04.869+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 5308 bytes result sent to driver
[2025-05-02T01:28:04.871+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:04.873+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-05-02T01:28:04.874+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 218 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T01:28:04.913+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:04.914+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-05-02T01:28:04.991+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO MemoryStore: Block rdd_10_4 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:04.994+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:04 INFO BlockManagerInfo: Added rdd_10_4 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.052+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 5308 bytes result sent to driver
[2025-05-02T01:28:05.055+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.056+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 185 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T01:28:05.057+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-05-02T01:28:05.093+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.095+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:05.171+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO MemoryStore: Block rdd_10_5 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:05.174+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO BlockManagerInfo: Added rdd_10_5 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.235+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 5308 bytes result sent to driver
[2025-05-02T01:28:05.237+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.239+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-05-02T01:28:05.240+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 185 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T01:28:05.270+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.272+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:05.355+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO MemoryStore: Block rdd_10_6 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:05.358+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO BlockManagerInfo: Added rdd_10_6 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.404+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 5308 bytes result sent to driver
[2025-05-02T01:28:05.407+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.408+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 171 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T01:28:05.410+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-05-02T01:28:05.435+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.437+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:05.501+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO MemoryStore: Block rdd_10_7 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:05.502+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO BlockManagerInfo: Added rdd_10_7 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.543+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 5308 bytes result sent to driver
[2025-05-02T01:28:05.545+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.546+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 140 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T01:28:05.547+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-05-02T01:28:05.575+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.576+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:05.648+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO MemoryStore: Block rdd_10_8 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:05.651+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO BlockManagerInfo: Added rdd_10_8 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.703+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 5351 bytes result sent to driver
[2025-05-02T01:28:05.706+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.707+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-05-02T01:28:05.710+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 163 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T01:28:05.737+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.739+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:05.859+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO MemoryStore: Block rdd_10_9 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:05.861+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO BlockManagerInfo: Added rdd_10_9 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:05.927+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 5308 bytes result sent to driver
[2025-05-02T01:28:05.930+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:05.932+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-05-02T01:28:05.933+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 226 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T01:28:05.968+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:05.969+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.072+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_10 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.074+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_10 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.141+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 5308 bytes result sent to driver
[2025-05-02T01:28:06.144+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.146+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-05-02T01:28:06.147+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 216 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T01:28:06.181+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.182+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.262+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_11 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.263+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_11 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.329+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 5308 bytes result sent to driver
[2025-05-02T01:28:06.331+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 15) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.333+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 189 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T01:28:06.335+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 12.0 in stage 2.0 (TID 15)
[2025-05-02T01:28:06.373+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.375+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.452+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_12 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.454+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_12 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.513+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 12.0 in stage 2.0 (TID 15). 5308 bytes result sent to driver
[2025-05-02T01:28:06.515+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 16) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.516+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 13.0 in stage 2.0 (TID 16)
[2025-05-02T01:28:06.518+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 15) in 186 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T01:28:06.544+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.546+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.604+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_13 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.605+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_13 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.665+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 13.0 in stage 2.0 (TID 16). 5308 bytes result sent to driver
[2025-05-02T01:28:06.666+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 17) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.668+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 14.0 in stage 2.0 (TID 17)
[2025-05-02T01:28:06.669+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 16) in 153 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T01:28:06.690+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.692+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.746+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_14 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.747+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_14 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.795+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 14.0 in stage 2.0 (TID 17). 5308 bytes result sent to driver
[2025-05-02T01:28:06.797+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 18) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.798+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 17) in 133 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T01:28:06.800+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 15.0 in stage 2.0 (TID 18)
[2025-05-02T01:28:06.832+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.833+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:06.885+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO MemoryStore: Block rdd_10_15 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:06.887+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO BlockManagerInfo: Added rdd_10_15 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:06.922+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Finished task 15.0 in stage 2.0 (TID 18). 5308 bytes result sent to driver
[2025-05-02T01:28:06.924+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 19) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:06.925+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 18) in 129 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T01:28:06.927+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO Executor: Running task 16.0 in stage 2.0 (TID 19)
[2025-05-02T01:28:06.945+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:06.946+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.028+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_16 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.030+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_16 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.106+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 16.0 in stage 2.0 (TID 19). 5351 bytes result sent to driver
[2025-05-02T01:28:07.109+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 20) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.111+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 17.0 in stage 2.0 (TID 20)
[2025-05-02T01:28:07.112+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 19) in 187 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T01:28:07.139+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:07.140+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.244+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_17 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.247+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_17 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.305+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 17.0 in stage 2.0 (TID 20). 5351 bytes result sent to driver
[2025-05-02T01:28:07.308+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 21) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.310+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 18.0 in stage 2.0 (TID 21)
[2025-05-02T01:28:07.311+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 20) in 201 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T01:28:07.336+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:07.337+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.408+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_18 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.410+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_18 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.478+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 18.0 in stage 2.0 (TID 21). 5308 bytes result sent to driver
[2025-05-02T01:28:07.481+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 22) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.482+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 21) in 174 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T01:28:07.484+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 19.0 in stage 2.0 (TID 22)
[2025-05-02T01:28:07.510+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:07.511+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.576+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_19 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.577+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_19 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.635+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 19.0 in stage 2.0 (TID 22). 5308 bytes result sent to driver
[2025-05-02T01:28:07.638+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 23) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.639+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 22) in 158 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T01:28:07.641+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 20.0 in stage 2.0 (TID 23)
[2025-05-02T01:28:07.667+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:07.669+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.744+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_20 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.746+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_20 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.800+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 20.0 in stage 2.0 (TID 23). 5308 bytes result sent to driver
[2025-05-02T01:28:07.802+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 24) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.804+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 21.0 in stage 2.0 (TID 24)
[2025-05-02T01:28:07.806+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 23) in 167 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T01:28:07.835+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:07.837+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:07.913+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO MemoryStore: Block rdd_10_21 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:07.915+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO BlockManagerInfo: Added rdd_10_21 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:07.981+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Finished task 21.0 in stage 2.0 (TID 24). 5308 bytes result sent to driver
[2025-05-02T01:28:07.984+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 25) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:07.986+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO Executor: Running task 22.0 in stage 2.0 (TID 25)
[2025-05-02T01:28:07.987+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:07 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 24) in 184 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T01:28:08.012+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.014+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.081+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_22 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.083+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_22 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.132+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 22.0 in stage 2.0 (TID 25). 5308 bytes result sent to driver
[2025-05-02T01:28:08.134+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 26) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.135+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 24.0 in stage 2.0 (TID 26)
[2025-05-02T01:28:08.137+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 25) in 151 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T01:28:08.159+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.160+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.237+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_24 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.239+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_24 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.292+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 24.0 in stage 2.0 (TID 26). 5308 bytes result sent to driver
[2025-05-02T01:28:08.294+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 27) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.296+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 25.0 in stage 2.0 (TID 27)
[2025-05-02T01:28:08.297+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 26) in 162 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T01:28:08.321+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.323+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.381+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_25 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.382+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_25 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.422+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 25.0 in stage 2.0 (TID 27). 5308 bytes result sent to driver
[2025-05-02T01:28:08.425+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 28) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.426+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 27) in 132 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T01:28:08.428+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 26.0 in stage 2.0 (TID 28)
[2025-05-02T01:28:08.451+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.452+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.518+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_26 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.520+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_26 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.572+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 26.0 in stage 2.0 (TID 28). 5308 bytes result sent to driver
[2025-05-02T01:28:08.575+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 29) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.577+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 28) in 153 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T01:28:08.578+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 27.0 in stage 2.0 (TID 29)
[2025-05-02T01:28:08.614+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.615+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.701+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_27 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.703+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_27 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.763+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 27.0 in stage 2.0 (TID 29). 5351 bytes result sent to driver
[2025-05-02T01:28:08.765+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 30) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.767+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 29) in 193 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T01:28:08.768+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 28.0 in stage 2.0 (TID 30)
[2025-05-02T01:28:08.813+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.814+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:08.909+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO MemoryStore: Block rdd_10_28 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:08.911+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO BlockManagerInfo: Added rdd_10_28 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:08.966+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Finished task 28.0 in stage 2.0 (TID 30). 5308 bytes result sent to driver
[2025-05-02T01:28:08.968+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 31) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:08.970+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO Executor: Running task 29.0 in stage 2.0 (TID 31)
[2025-05-02T01:28:08.971+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 30) in 205 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T01:28:08.997+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:08.998+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.053+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_29 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.056+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_29 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.106+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 29.0 in stage 2.0 (TID 31). 5308 bytes result sent to driver
[2025-05-02T01:28:09.109+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 32) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.110+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 31) in 142 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T01:28:09.111+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 30.0 in stage 2.0 (TID 32)
[2025-05-02T01:28:09.144+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.146+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.219+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_30 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.221+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_30 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.269+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 30.0 in stage 2.0 (TID 32). 5308 bytes result sent to driver
[2025-05-02T01:28:09.271+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 33) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.273+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 31.0 in stage 2.0 (TID 33)
[2025-05-02T01:28:09.274+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 32) in 164 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T01:28:09.303+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.304+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.374+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_31 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.376+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_31 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.428+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 31.0 in stage 2.0 (TID 33). 5308 bytes result sent to driver
[2025-05-02T01:28:09.430+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 34) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.432+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 32.0 in stage 2.0 (TID 34)
[2025-05-02T01:28:09.433+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 33) in 161 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T01:28:09.460+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.461+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.547+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_32 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.548+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_32 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.594+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 32.0 in stage 2.0 (TID 34). 5308 bytes result sent to driver
[2025-05-02T01:28:09.597+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 35) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.598+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 34) in 168 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T01:28:09.600+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 33.0 in stage 2.0 (TID 35)
[2025-05-02T01:28:09.624+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.625+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.685+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_33 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.686+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_33 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.729+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 33.0 in stage 2.0 (TID 35). 5308 bytes result sent to driver
[2025-05-02T01:28:09.730+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 36) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.732+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 34.0 in stage 2.0 (TID 36)
[2025-05-02T01:28:09.733+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 35) in 136 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T01:28:09.752+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.753+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T01:28:09.811+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_34 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.813+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_34 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.853+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 34.0 in stage 2.0 (TID 36). 5308 bytes result sent to driver
[2025-05-02T01:28:09.856+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 37) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.857+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 35.0 in stage 2.0 (TID 37)
[2025-05-02T01:28:09.859+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 36) in 126 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T01:28:09.877+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:09.878+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:09.943+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO MemoryStore: Block rdd_10_35 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:09.944+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO BlockManagerInfo: Added rdd_10_35 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:09.981+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Finished task 35.0 in stage 2.0 (TID 37). 5308 bytes result sent to driver
[2025-05-02T01:28:09.983+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 38) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:09.984+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO Executor: Running task 36.0 in stage 2.0 (TID 38)
[2025-05-02T01:28:09.986+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:09 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 37) in 130 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T01:28:10.006+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.007+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.073+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_36 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.074+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_36 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.131+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 36.0 in stage 2.0 (TID 38). 5351 bytes result sent to driver
[2025-05-02T01:28:10.134+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 39) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.136+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 37.0 in stage 2.0 (TID 39)
[2025-05-02T01:28:10.137+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 38) in 153 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T01:28:10.161+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.163+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.268+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_37 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.271+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_37 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.343+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 37.0 in stage 2.0 (TID 39). 5351 bytes result sent to driver
[2025-05-02T01:28:10.346+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 40) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.348+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 39) in 214 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T01:28:10.349+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 38.0 in stage 2.0 (TID 40)
[2025-05-02T01:28:10.374+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.376+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.451+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_38 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.452+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_38 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.507+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 38.0 in stage 2.0 (TID 40). 5308 bytes result sent to driver
[2025-05-02T01:28:10.510+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 41) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.511+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 40) in 165 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T01:28:10.512+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 39.0 in stage 2.0 (TID 41)
[2025-05-02T01:28:10.539+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.541+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.607+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_39 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.608+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_39 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.656+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 39.0 in stage 2.0 (TID 41). 5308 bytes result sent to driver
[2025-05-02T01:28:10.659+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 42) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.661+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 41) in 151 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T01:28:10.663+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 40.0 in stage 2.0 (TID 42)
[2025-05-02T01:28:10.696+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.698+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.760+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_40 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.761+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_40 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.818+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 40.0 in stage 2.0 (TID 42). 5308 bytes result sent to driver
[2025-05-02T01:28:10.820+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 43) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.822+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 41.0 in stage 2.0 (TID 43)
[2025-05-02T01:28:10.823+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 42) in 163 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T01:28:10.848+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.850+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:10.929+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO MemoryStore: Block rdd_10_41 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:10.930+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO BlockManagerInfo: Added rdd_10_41 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:10.970+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Finished task 41.0 in stage 2.0 (TID 43). 5308 bytes result sent to driver
[2025-05-02T01:28:10.973+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 44) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:10.975+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO Executor: Running task 43.0 in stage 2.0 (TID 44)
[2025-05-02T01:28:10.976+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 43) in 155 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T01:28:10.996+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:10.998+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T01:28:11.054+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_43 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.055+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_43 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.098+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 43.0 in stage 2.0 (TID 44). 5308 bytes result sent to driver
[2025-05-02T01:28:11.100+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 45) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.102+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 44) in 129 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T01:28:11.104+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 44.0 in stage 2.0 (TID 45)
[2025-05-02T01:28:11.125+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.127+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:11.195+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_44 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.197+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_44 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.232+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 44.0 in stage 2.0 (TID 45). 5308 bytes result sent to driver
[2025-05-02T01:28:11.233+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 46) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.234+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 45) in 134 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T01:28:11.236+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 45.0 in stage 2.0 (TID 46)
[2025-05-02T01:28:11.257+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.258+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T01:28:11.314+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_45 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.315+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_45 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.351+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 45.0 in stage 2.0 (TID 46). 5308 bytes result sent to driver
[2025-05-02T01:28:11.353+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 47) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.355+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 46) in 122 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T01:28:11.356+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 46.0 in stage 2.0 (TID 47)
[2025-05-02T01:28:11.375+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.376+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T01:28:11.430+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_46 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.432+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_46 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.473+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 46.0 in stage 2.0 (TID 47). 5308 bytes result sent to driver
[2025-05-02T01:28:11.475+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 48) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.477+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 47) in 124 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T01:28:11.479+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 47.0 in stage 2.0 (TID 48)
[2025-05-02T01:28:11.502+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.504+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:11.568+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_47 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.570+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_47 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.636+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 47.0 in stage 2.0 (TID 48). 5394 bytes result sent to driver
[2025-05-02T01:28:11.640+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 49) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.641+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 48) in 166 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T01:28:11.644+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 48.0 in stage 2.0 (TID 49)
[2025-05-02T01:28:11.672+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.674+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:11.764+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_48 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.765+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_48 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.835+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 48.0 in stage 2.0 (TID 49). 5308 bytes result sent to driver
[2025-05-02T01:28:11.839+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 50) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T01:28:11.840+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Running task 49.0 in stage 2.0 (TID 50)
[2025-05-02T01:28:11.842+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 49) in 202 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T01:28:11.865+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:11.867+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T01:28:11.931+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO MemoryStore: Block rdd_10_49 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T01:28:11.933+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO BlockManagerInfo: Added rdd_10_49 in memory on ***-scheduler:40993 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T01:28:11.975+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO Executor: Finished task 49.0 in stage 2.0 (TID 50). 5308 bytes result sent to driver
[2025-05-02T01:28:11.977+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 50) in 140 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T01:28:11.979+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-02T01:28:11.980+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO DAGScheduler: ShuffleMapStage 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 11.121 s
[2025-05-02T01:28:11.982+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T01:28:11.983+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO DAGScheduler: running: Set()
[2025-05-02T01:28:11.984+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO DAGScheduler: waiting: Set()
[2025-05-02T01:28:11.985+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:11 INFO DAGScheduler: failed: Set()
[2025-05-02T01:28:12.064+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:28:12.071+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Got job 2 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T01:28:12.072+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:28:12.073+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2025-05-02T01:28:12.074+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:12.077+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:28:12.099+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 537.2 KiB, free 432.7 MiB)
[2025-05-02T01:28:12.103+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 125.4 KiB, free 432.6 MiB)
[2025-05-02T01:28:12.105+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ***-scheduler:40993 (size: 125.4 KiB, free: 434.1 MiB)
[2025-05-02T01:28:12.107+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:12.110+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T01:28:12.112+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-05-02T01:28:12.115+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 51) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-02T01:28:12.117+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 51)
[2025-05-02T01:28:12.161+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO ShuffleBlockFetcherIterator: Getting 50 (5.0 KiB) non-empty blocks including 50 (5.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:12.162+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T01:28:12.198+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO CodeGenerator: Code generated in 14.445836 ms
[2025-05-02T01:28:12.246+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO CodeGenerator: Code generated in 35.86102 ms
[2025-05-02T01:28:12.281+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO Executor: Finished task 0.0 in stage 5.0 (TID 51). 7311 bytes result sent to driver
[2025-05-02T01:28:12.283+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 51) in 170 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T01:28:12.285+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-05-02T01:28:12.286+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: ResultStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.204 s
[2025-05-02T01:28:12.290+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T01:28:12.291+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-05-02T01:28:12.295+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO DAGScheduler: Job 2 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.230054 s
[2025-05-02T01:28:12.417+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO CodeGenerator: Code generated in 87.954371 ms
[2025-05-02T01:28:12.423+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:12 INFO Snapshot: DELTA: Done
[2025-05-02T01:28:13.328+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO CodeGenerator: Code generated in 276.273787 ms
[2025-05-02T01:28:13.390+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:28:13.394+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Got job 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-02T01:28:13.395+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:28:13.396+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-05-02T01:28:13.397+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:13.399+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[18] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:28:13.417+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 687.2 KiB, free 431.9 MiB)
[2025-05-02T01:28:13.424+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 155.3 KiB, free 431.7 MiB)
[2025-05-02T01:28:13.427+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ***-scheduler:40993 (size: 155.3 KiB, free: 433.9 MiB)
[2025-05-02T01:28:13.429+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:13.431+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 7 (MapPartitionsRDD[18] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T01:28:13.432+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSchedulerImpl: Adding task set 7.0 with 50 tasks resource profile 0
[2025-05-02T01:28:13.439+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 52) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:13.441+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Running task 0.0 in stage 7.0 (TID 52)
[2025-05-02T01:28:13.483+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManager: Found block rdd_10_0 locally
[2025-05-02T01:28:13.820+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ***-scheduler:40993 in memory (size: 125.4 KiB, free: 434.0 MiB)
[2025-05-02T01:28:13.869+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO CodeGenerator: Code generated in 382.897782 ms
[2025-05-02T01:28:13.882+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Finished task 0.0 in stage 7.0 (TID 52). 4224 bytes result sent to driver
[2025-05-02T01:28:13.885+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 53) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:13.886+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 52) in 448 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T01:28:13.890+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Running task 1.0 in stage 7.0 (TID 53)
[2025-05-02T01:28:13.913+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManager: Found block rdd_10_1 locally
[2025-05-02T01:28:13.918+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Finished task 1.0 in stage 7.0 (TID 53). 4181 bytes result sent to driver
[2025-05-02T01:28:13.920+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 54) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:13.922+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Running task 2.0 in stage 7.0 (TID 54)
[2025-05-02T01:28:13.923+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 53) in 39 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T01:28:13.962+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManager: Found block rdd_10_2 locally
[2025-05-02T01:28:13.966+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Finished task 2.0 in stage 7.0 (TID 54). 4224 bytes result sent to driver
[2025-05-02T01:28:13.968+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 55) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:13.970+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 54) in 50 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T01:28:13.972+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Running task 3.0 in stage 7.0 (TID 55)
[2025-05-02T01:28:13.992+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO BlockManager: Found block rdd_10_3 locally
[2025-05-02T01:28:13.997+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO Executor: Finished task 3.0 in stage 7.0 (TID 55). 4181 bytes result sent to driver
[2025-05-02T01:28:13.999+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:13 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 56) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.001+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 4.0 in stage 7.0 (TID 56)
[2025-05-02T01:28:14.002+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 55) in 33 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T01:28:14.022+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_4 locally
[2025-05-02T01:28:14.027+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 4.0 in stage 7.0 (TID 56). 4181 bytes result sent to driver
[2025-05-02T01:28:14.029+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 57) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.031+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 56) in 32 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T01:28:14.033+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 5.0 in stage 7.0 (TID 57)
[2025-05-02T01:28:14.055+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_5 locally
[2025-05-02T01:28:14.060+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 5.0 in stage 7.0 (TID 57). 4181 bytes result sent to driver
[2025-05-02T01:28:14.062+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 58) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.063+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 6.0 in stage 7.0 (TID 58)
[2025-05-02T01:28:14.065+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 57) in 35 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T01:28:14.087+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_6 locally
[2025-05-02T01:28:14.092+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 6.0 in stage 7.0 (TID 58). 4181 bytes result sent to driver
[2025-05-02T01:28:14.094+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 59) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.095+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 58) in 33 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T01:28:14.097+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 7.0 in stage 7.0 (TID 59)
[2025-05-02T01:28:14.117+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_7 locally
[2025-05-02T01:28:14.123+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 7.0 in stage 7.0 (TID 59). 4181 bytes result sent to driver
[2025-05-02T01:28:14.125+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 60) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.127+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 59) in 33 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T01:28:14.129+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 8.0 in stage 7.0 (TID 60)
[2025-05-02T01:28:14.152+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_8 locally
[2025-05-02T01:28:14.157+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 8.0 in stage 7.0 (TID 60). 4181 bytes result sent to driver
[2025-05-02T01:28:14.159+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 61) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.161+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 9.0 in stage 7.0 (TID 61)
[2025-05-02T01:28:14.162+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 60) in 37 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T01:28:14.185+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_9 locally
[2025-05-02T01:28:14.192+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 9.0 in stage 7.0 (TID 61). 4181 bytes result sent to driver
[2025-05-02T01:28:14.194+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 62) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.196+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 61) in 37 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T01:28:14.197+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 10.0 in stage 7.0 (TID 62)
[2025-05-02T01:28:14.223+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_10 locally
[2025-05-02T01:28:14.228+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 10.0 in stage 7.0 (TID 62). 4181 bytes result sent to driver
[2025-05-02T01:28:14.230+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 63) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.232+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 11.0 in stage 7.0 (TID 63)
[2025-05-02T01:28:14.234+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 62) in 38 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T01:28:14.256+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_11 locally
[2025-05-02T01:28:14.261+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 11.0 in stage 7.0 (TID 63). 4181 bytes result sent to driver
[2025-05-02T01:28:14.263+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 64) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.265+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 12.0 in stage 7.0 (TID 64)
[2025-05-02T01:28:14.266+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 63) in 35 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T01:28:14.291+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_12 locally
[2025-05-02T01:28:14.297+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 12.0 in stage 7.0 (TID 64). 4181 bytes result sent to driver
[2025-05-02T01:28:14.298+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 65) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.300+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 13.0 in stage 7.0 (TID 65)
[2025-05-02T01:28:14.301+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 64) in 37 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T01:28:14.320+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_13 locally
[2025-05-02T01:28:14.325+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 13.0 in stage 7.0 (TID 65). 4181 bytes result sent to driver
[2025-05-02T01:28:14.326+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 66) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.328+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 14.0 in stage 7.0 (TID 66)
[2025-05-02T01:28:14.329+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 65) in 29 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T01:28:14.349+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_14 locally
[2025-05-02T01:28:14.354+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 14.0 in stage 7.0 (TID 66). 4181 bytes result sent to driver
[2025-05-02T01:28:14.356+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 67) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.358+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 15.0 in stage 7.0 (TID 67)
[2025-05-02T01:28:14.360+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 66) in 32 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T01:28:14.387+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_15 locally
[2025-05-02T01:28:14.393+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 15.0 in stage 7.0 (TID 67). 4181 bytes result sent to driver
[2025-05-02T01:28:14.394+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 68) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.396+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 16.0 in stage 7.0 (TID 68)
[2025-05-02T01:28:14.397+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 67) in 40 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T01:28:14.415+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_16 locally
[2025-05-02T01:28:14.418+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 16.0 in stage 7.0 (TID 68). 4181 bytes result sent to driver
[2025-05-02T01:28:14.420+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 69) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.422+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 17.0 in stage 7.0 (TID 69)
[2025-05-02T01:28:14.423+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 68) in 28 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T01:28:14.441+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_17 locally
[2025-05-02T01:28:14.445+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 17.0 in stage 7.0 (TID 69). 4181 bytes result sent to driver
[2025-05-02T01:28:14.447+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 70) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.448+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 69) in 28 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T01:28:14.449+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 18.0 in stage 7.0 (TID 70)
[2025-05-02T01:28:14.468+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_18 locally
[2025-05-02T01:28:14.471+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 18.0 in stage 7.0 (TID 70). 4181 bytes result sent to driver
[2025-05-02T01:28:14.473+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 71) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.475+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 19.0 in stage 7.0 (TID 71)
[2025-05-02T01:28:14.476+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 70) in 28 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T01:28:14.493+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_19 locally
[2025-05-02T01:28:14.497+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 19.0 in stage 7.0 (TID 71). 4181 bytes result sent to driver
[2025-05-02T01:28:14.499+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 72) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.500+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 20.0 in stage 7.0 (TID 72)
[2025-05-02T01:28:14.501+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 71) in 27 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T01:28:14.519+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_20 locally
[2025-05-02T01:28:14.524+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 20.0 in stage 7.0 (TID 72). 4181 bytes result sent to driver
[2025-05-02T01:28:14.526+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 73) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.527+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 21.0 in stage 7.0 (TID 73)
[2025-05-02T01:28:14.528+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 72) in 29 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T01:28:14.547+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_21 locally
[2025-05-02T01:28:14.551+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 21.0 in stage 7.0 (TID 73). 4181 bytes result sent to driver
[2025-05-02T01:28:14.553+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 74) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.554+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 22.0 in stage 7.0 (TID 74)
[2025-05-02T01:28:14.556+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 73) in 29 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T01:28:14.576+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_22 locally
[2025-05-02T01:28:14.580+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 22.0 in stage 7.0 (TID 74). 4181 bytes result sent to driver
[2025-05-02T01:28:14.582+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 75) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.583+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 23.0 in stage 7.0 (TID 75)
[2025-05-02T01:28:14.584+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 74) in 30 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T01:28:14.600+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_23 locally
[2025-05-02T01:28:14.605+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 23.0 in stage 7.0 (TID 75). 4349 bytes result sent to driver
[2025-05-02T01:28:14.607+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 76) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.608+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 24.0 in stage 7.0 (TID 76)
[2025-05-02T01:28:14.610+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 75) in 27 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T01:28:14.625+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_24 locally
[2025-05-02T01:28:14.628+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 24.0 in stage 7.0 (TID 76). 4181 bytes result sent to driver
[2025-05-02T01:28:14.630+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 77) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.631+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 25.0 in stage 7.0 (TID 77)
[2025-05-02T01:28:14.632+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 76) in 24 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T01:28:14.648+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_25 locally
[2025-05-02T01:28:14.651+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 25.0 in stage 7.0 (TID 77). 4181 bytes result sent to driver
[2025-05-02T01:28:14.653+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 78) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.654+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 26.0 in stage 7.0 (TID 78)
[2025-05-02T01:28:14.656+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 77) in 25 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T01:28:14.670+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_26 locally
[2025-05-02T01:28:14.674+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 26.0 in stage 7.0 (TID 78). 4181 bytes result sent to driver
[2025-05-02T01:28:14.675+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 79) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.677+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 78) in 24 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T01:28:14.678+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 27.0 in stage 7.0 (TID 79)
[2025-05-02T01:28:14.698+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_27 locally
[2025-05-02T01:28:14.701+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 27.0 in stage 7.0 (TID 79). 4181 bytes result sent to driver
[2025-05-02T01:28:14.703+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 80) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.704+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 28.0 in stage 7.0 (TID 80)
[2025-05-02T01:28:14.706+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 79) in 30 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T01:28:14.729+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_28 locally
[2025-05-02T01:28:14.739+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 28.0 in stage 7.0 (TID 80). 4224 bytes result sent to driver
[2025-05-02T01:28:14.741+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 81) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.743+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 29.0 in stage 7.0 (TID 81)
[2025-05-02T01:28:14.744+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 80) in 40 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T01:28:14.760+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_29 locally
[2025-05-02T01:28:14.763+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 29.0 in stage 7.0 (TID 81). 4181 bytes result sent to driver
[2025-05-02T01:28:14.764+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 82) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.766+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 30.0 in stage 7.0 (TID 82)
[2025-05-02T01:28:14.767+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 81) in 25 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T01:28:14.785+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_30 locally
[2025-05-02T01:28:14.789+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 30.0 in stage 7.0 (TID 82). 4181 bytes result sent to driver
[2025-05-02T01:28:14.791+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 83) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.792+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 31.0 in stage 7.0 (TID 83)
[2025-05-02T01:28:14.793+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 82) in 27 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T01:28:14.809+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_31 locally
[2025-05-02T01:28:14.813+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 31.0 in stage 7.0 (TID 83). 4181 bytes result sent to driver
[2025-05-02T01:28:14.815+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 84) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.816+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 32.0 in stage 7.0 (TID 84)
[2025-05-02T01:28:14.817+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 83) in 26 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T01:28:14.838+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_32 locally
[2025-05-02T01:28:14.843+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 32.0 in stage 7.0 (TID 84). 4181 bytes result sent to driver
[2025-05-02T01:28:14.845+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 85) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.846+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 33.0 in stage 7.0 (TID 85)
[2025-05-02T01:28:14.846+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 84) in 31 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T01:28:14.876+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ***-scheduler:40993 in memory (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-02T01:28:14.881+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_33 locally
[2025-05-02T01:28:14.887+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 33.0 in stage 7.0 (TID 85). 4181 bytes result sent to driver
[2025-05-02T01:28:14.889+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 86) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.891+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 34.0 in stage 7.0 (TID 86)
[2025-05-02T01:28:14.903+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 85) in 45 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T01:28:14.915+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_34 locally
[2025-05-02T01:28:14.920+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 34.0 in stage 7.0 (TID 86). 4181 bytes result sent to driver
[2025-05-02T01:28:14.923+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 87) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.925+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 35.0 in stage 7.0 (TID 87)
[2025-05-02T01:28:14.927+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 86) in 38 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T01:28:14.947+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_35 locally
[2025-05-02T01:28:14.951+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 35.0 in stage 7.0 (TID 87). 4181 bytes result sent to driver
[2025-05-02T01:28:14.953+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 88) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.955+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 36.0 in stage 7.0 (TID 88)
[2025-05-02T01:28:14.956+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 87) in 33 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T01:28:14.978+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO BlockManager: Found block rdd_10_36 locally
[2025-05-02T01:28:14.982+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Finished task 36.0 in stage 7.0 (TID 88). 4181 bytes result sent to driver
[2025-05-02T01:28:14.983+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 89) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:14.984+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO Executor: Running task 37.0 in stage 7.0 (TID 89)
[2025-05-02T01:28:14.985+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:14 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 88) in 32 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T01:28:15.013+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_37 locally
[2025-05-02T01:28:15.018+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 37.0 in stage 7.0 (TID 89). 4181 bytes result sent to driver
[2025-05-02T01:28:15.020+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 90) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.021+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 38.0 in stage 7.0 (TID 90)
[2025-05-02T01:28:15.023+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 89) in 39 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T01:28:15.044+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_38 locally
[2025-05-02T01:28:15.048+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 38.0 in stage 7.0 (TID 90). 4181 bytes result sent to driver
[2025-05-02T01:28:15.050+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 91) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.052+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 39.0 in stage 7.0 (TID 91)
[2025-05-02T01:28:15.053+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 90) in 32 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T01:28:15.072+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_39 locally
[2025-05-02T01:28:15.076+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 39.0 in stage 7.0 (TID 91). 4181 bytes result sent to driver
[2025-05-02T01:28:15.078+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 92) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.080+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 91) in 30 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T01:28:15.081+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 40.0 in stage 7.0 (TID 92)
[2025-05-02T01:28:15.102+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_40 locally
[2025-05-02T01:28:15.106+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 40.0 in stage 7.0 (TID 92). 4181 bytes result sent to driver
[2025-05-02T01:28:15.108+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 93) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.110+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 41.0 in stage 7.0 (TID 93)
[2025-05-02T01:28:15.111+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 92) in 31 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T01:28:15.129+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_41 locally
[2025-05-02T01:28:15.132+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 41.0 in stage 7.0 (TID 93). 4181 bytes result sent to driver
[2025-05-02T01:28:15.134+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 94) (***-scheduler, executor driver, partition 42, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.136+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 93) in 28 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T01:28:15.137+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 42.0 in stage 7.0 (TID 94)
[2025-05-02T01:28:15.158+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_42 locally
[2025-05-02T01:28:15.162+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 42.0 in stage 7.0 (TID 94). 4224 bytes result sent to driver
[2025-05-02T01:28:15.164+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 95) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.166+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 43.0 in stage 7.0 (TID 95)
[2025-05-02T01:28:15.167+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 94) in 32 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T01:28:15.193+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_43 locally
[2025-05-02T01:28:15.196+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 43.0 in stage 7.0 (TID 95). 4181 bytes result sent to driver
[2025-05-02T01:28:15.198+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 96) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.199+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 44.0 in stage 7.0 (TID 96)
[2025-05-02T01:28:15.201+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 95) in 36 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T01:28:15.221+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_44 locally
[2025-05-02T01:28:15.226+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 44.0 in stage 7.0 (TID 96). 4181 bytes result sent to driver
[2025-05-02T01:28:15.228+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 97) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.230+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 45.0 in stage 7.0 (TID 97)
[2025-05-02T01:28:15.231+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 96) in 32 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T01:28:15.249+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_45 locally
[2025-05-02T01:28:15.254+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 45.0 in stage 7.0 (TID 97). 4181 bytes result sent to driver
[2025-05-02T01:28:15.256+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 98) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.257+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 46.0 in stage 7.0 (TID 98)
[2025-05-02T01:28:15.258+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 97) in 30 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T01:28:15.279+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_46 locally
[2025-05-02T01:28:15.283+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 46.0 in stage 7.0 (TID 98). 4181 bytes result sent to driver
[2025-05-02T01:28:15.285+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 99) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.286+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 47.0 in stage 7.0 (TID 99)
[2025-05-02T01:28:15.287+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 98) in 30 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T01:28:15.308+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_47 locally
[2025-05-02T01:28:15.311+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 47.0 in stage 7.0 (TID 99). 4181 bytes result sent to driver
[2025-05-02T01:28:15.313+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 100) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.314+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 48.0 in stage 7.0 (TID 100)
[2025-05-02T01:28:15.315+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 99) in 30 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T01:28:15.336+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_48 locally
[2025-05-02T01:28:15.341+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 48.0 in stage 7.0 (TID 100). 4181 bytes result sent to driver
[2025-05-02T01:28:15.343+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 101) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T01:28:15.344+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 100) in 31 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T01:28:15.345+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Running task 49.0 in stage 7.0 (TID 101)
[2025-05-02T01:28:15.365+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO BlockManager: Found block rdd_10_49 locally
[2025-05-02T01:28:15.369+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO Executor: Finished task 49.0 in stage 7.0 (TID 101). 4181 bytes result sent to driver
[2025-05-02T01:28:15.371+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 101) in 28 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T01:28:15.372+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-05-02T01:28:15.373+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO DAGScheduler: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.970 s
[2025-05-02T01:28:15.374+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T01:28:15.375+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-05-02T01:28:15.376+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO DAGScheduler: Job 3 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.982608 s
[2025-05-02T01:28:15.412+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:15 INFO CodeGenerator: Code generated in 26.328317 ms
[2025-05-02T01:28:16.027+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T01:28:16.029+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T01:28:16.041+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceStrategy: Pushed Filters: IsNotNull(HASH)
[2025-05-02T01:28:16.043+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(HASH#135)
[2025-05-02T01:28:16.245+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-05-02T01:28:16.395+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO CodeGenerator: Code generated in 28.632909 ms
[2025-05-02T01:28:16.432+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 206.5 KiB, free 432.9 MiB)
[2025-05-02T01:28:16.463+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.3 KiB, free 432.9 MiB)
[2025-05-02T01:28:16.464+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ***-scheduler:40993 (size: 36.3 KiB, free: 434.1 MiB)
[2025-05-02T01:28:16.466+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T01:28:16.687+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO CodeGenerator: Code generated in 118.232724 ms
[2025-05-02T01:28:16.693+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO CodeGenerator: Code generated in 265.642312 ms
[2025-05-02T01:28:16.710+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 205.0 KiB, free 432.7 MiB)
[2025-05-02T01:28:16.748+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 432.6 MiB)
[2025-05-02T01:28:16.752+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ***-scheduler:40993 (size: 35.9 KiB, free: 434.1 MiB)
[2025-05-02T01:28:16.755+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO SparkContext: Created broadcast 7 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:28:16.818+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 54665438 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T01:28:16.821+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO CodeGenerator: Code generated in 10.185695 ms
[2025-05-02T01:28:16.899+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO CodeGenerator: Code generated in 48.070386 ms
[2025-05-02T01:28:16.923+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Registering RDD 22 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2025-05-02T01:28:16.925+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Got map stage job 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T01:28:16.927+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:28:16.931+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T01:28:16.934+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:16.936+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[22] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:28:16.940+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 35496120 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T01:28:16.943+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 91.0 KiB, free 432.5 MiB)
[2025-05-02T01:28:16.947+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 30.7 KiB, free 432.5 MiB)
[2025-05-02T01:28:16.949+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ***-scheduler:40993 (size: 30.7 KiB, free: 434.1 MiB)
[2025-05-02T01:28:16.951+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:16.953+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[22] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T01:28:16.956+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-05-02T01:28:16.960+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 102) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10840 bytes)
[2025-05-02T01:28:16.962+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 102)
[2025-05-02T01:28:17.062+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T01:28:17.065+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-05-02T01:28:17.067+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-05-02T01:28:17.068+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T01:28:17.070+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:17.073+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-05-02T01:28:17.076+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 20.5 KiB, free 432.5 MiB)
[2025-05-02T01:28:17.078+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 432.5 MiB)
[2025-05-02T01:28:17.080+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ***-scheduler:40993 (size: 8.4 KiB, free: 434.1 MiB)
[2025-05-02T01:28:17.082+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:17.083+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-05-02T01:28:17.085+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-05-02T01:28:17.187+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO CodeGenerator: Code generated in 175.856324 ms
[2025-05-02T01:28:17.253+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO CodeGenerator: Code generated in 46.563954 ms
[2025-05-02T01:28:17.333+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO CodeGenerator: Code generated in 40.03338 ms
[2025-05-02T01:28:17.350+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO FileScanRDD: Reading File path: s3a://medical-bucket/raw/transactional/medical-data-sample/claims_transactions.csv, range: 0-50471134, partition values: [empty row]
[2025-05-02T01:28:17.400+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:17 INFO CodeGenerator: Code generated in 35.767047 ms
[2025-05-02T01:28:19.929+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ***-scheduler:40993 in memory (size: 155.3 KiB, free: 434.2 MiB)
[2025-05-02T01:28:28.692+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO Executor: Finished task 0.0 in stage 8.0 (TID 102). 2770 bytes result sent to driver
[2025-05-02T01:28:28.703+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 103) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10929 bytes)
[2025-05-02T01:28:28.707+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 102) in 11738 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T01:28:28.708+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO Executor: Running task 0.0 in stage 9.0 (TID 103)
[2025-05-02T01:28:28.710+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-05-02T01:28:28.711+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 11.773 s
[2025-05-02T01:28:28.713+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T01:28:28.714+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO DAGScheduler: running: Set(ResultStage 9)
[2025-05-02T01:28:28.717+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO DAGScheduler: waiting: Set()
[2025-05-02T01:28:28.719+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO DAGScheduler: failed: Set()
[2025-05-02T01:28:28.791+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO CodeGenerator: Code generated in 18.897715 ms
[2025-05-02T01:28:28.795+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:28 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_claims_transactions/part-00000-2cd9a2fd-3b86-4eb7-9653-5e4bd91a2550-c000.snappy.parquet, range: 0-31301816, partition values: [empty row]
[2025-05-02T01:28:29.162+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:29 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-02T01:28:30.219+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:30 INFO FilterCompat: Filtering using predicate: noteq(HASH, null)
[2025-05-02T01:28:30.508+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:30 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-02T01:28:31.505+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:31 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-05-02T01:28:32.567+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ***-scheduler:40993 in memory (size: 30.7 KiB, free: 434.2 MiB)
[2025-05-02T01:28:33.194+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO MemoryStore: Block taskresult_103 stored as bytes in memory (estimated size 23.6 MiB, free 409.9 MiB)
[2025-05-02T01:28:33.200+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO BlockManagerInfo: Added taskresult_103 in memory on ***-scheduler:40993 (size: 23.6 MiB, free: 410.7 MiB)
[2025-05-02T01:28:33.225+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO Executor: Finished task 0.0 in stage 9.0 (TID 103). 24703708 bytes result sent via BlockManager)
[2025-05-02T01:28:33.373+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO TransportClientFactory: Successfully created connection to ***-scheduler/172.18.0.4:40993 after 12 ms (0 ms spent in bootstraps)
[2025-05-02T01:28:33.899+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 103) in 5201 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T01:28:33.923+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-05-02T01:28:33.926+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO DAGScheduler: ResultStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 16.831 s
[2025-05-02T01:28:33.927+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T01:28:33.929+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-05-02T01:28:33.931+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 15.946951 s
[2025-05-02T01:28:33.932+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO BlockManagerInfo: Removed taskresult_103 on ***-scheduler:40993 in memory (size: 23.6 MiB, free: 434.2 MiB)
[2025-05-02T01:28:33.981+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:33 INFO CodeGenerator: Code generated in 26.975682 ms
[2025-05-02T01:28:35.031+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 116.0 MiB, free 317.4 MiB)
[2025-05-02T01:28:36.223+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 313.4 MiB)
[2025-05-02T01:28:36.232+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 430.2 MiB)
[2025-05-02T01:28:36.241+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece1 stored as bytes in memory (estimated size 4.0 MiB, free 309.4 MiB)
[2025-05-02T01:28:36.243+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece1 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 426.2 MiB)
[2025-05-02T01:28:36.253+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece2 stored as bytes in memory (estimated size 4.0 MiB, free 305.4 MiB)
[2025-05-02T01:28:36.255+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece2 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 422.2 MiB)
[2025-05-02T01:28:36.266+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece3 stored as bytes in memory (estimated size 4.0 MiB, free 301.4 MiB)
[2025-05-02T01:28:36.268+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece3 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 418.2 MiB)
[2025-05-02T01:28:36.277+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece4 stored as bytes in memory (estimated size 4.0 MiB, free 297.4 MiB)
[2025-05-02T01:28:36.279+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece4 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 414.2 MiB)
[2025-05-02T01:28:36.289+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece5 stored as bytes in memory (estimated size 4.0 MiB, free 293.4 MiB)
[2025-05-02T01:28:36.291+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece5 in memory on ***-scheduler:40993 (size: 4.0 MiB, free: 410.2 MiB)
[2025-05-02T01:28:36.298+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO MemoryStore: Block broadcast_10_piece6 stored as bytes in memory (estimated size 2.5 MiB, free 290.9 MiB)
[2025-05-02T01:28:36.300+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO BlockManagerInfo: Added broadcast_10_piece6 in memory on ***-scheduler:40993 (size: 2.5 MiB, free: 407.8 MiB)
[2025-05-02T01:28:36.307+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T01:28:36.389+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 30690953, minimum partition size: 1048576
[2025-05-02T01:28:36.835+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:36 INFO CodeGenerator: Code generated in 196.660594 ms
[2025-05-02T01:28:37.195+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T01:28:37.199+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Got job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T01:28:37.204+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T01:28:37.206+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-05-02T01:28:37.208+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Missing parents: List()
[2025-05-02T01:28:37.210+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T01:28:37.353+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 488.0 KiB, free 290.5 MiB)
[2025-05-02T01:28:37.369+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 165.8 KiB, free 290.3 MiB)
[2025-05-02T01:28:37.372+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ***-scheduler:40993 (size: 165.8 KiB, free: 407.6 MiB)
[2025-05-02T01:28:37.382+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-05-02T01:28:37.387+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T01:28:37.389+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-05-02T01:28:37.394+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 104) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-02T01:28:37.399+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO Executor: Running task 0.0 in stage 11.0 (TID 104)
[2025-05-02T01:28:37.696+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO ShuffleBlockFetcherIterator: Getting 1 (29.3 MiB) non-empty blocks including 1 (29.3 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T01:28:37.699+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2025-05-02T01:28:37.901+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:37 INFO CodeGenerator: Code generated in 196.803719 ms
[2025-05-02T01:28:38.068+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:38 INFO CodeGenerator: Code generated in 84.590524 ms
[2025-05-02T01:28:38.863+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:38 INFO CodeGenerator: Code generated in 467.312842 ms
[2025-05-02T01:28:38.965+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:38 INFO CodeGenerator: Code generated in 20.242674 ms
[2025-05-02T01:28:38.984+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:38 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T01:28:39.001+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:39 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T01:28:39.129+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-05-02T01:28:39.179+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-05-02T01:28:39.182+0000] {spark_submit.py:649} INFO - {
[2025-05-02T01:28:39.186+0000] {spark_submit.py:649} INFO - "type" : "struct",
[2025-05-02T01:28:39.188+0000] {spark_submit.py:649} INFO - "fields" : [ {
[2025-05-02T01:28:39.190+0000] {spark_submit.py:649} INFO - "name" : "ID",
[2025-05-02T01:28:39.192+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.194+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.195+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.197+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.199+0000] {spark_submit.py:649} INFO - "name" : "CLAIMID",
[2025-05-02T01:28:39.201+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.203+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.205+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.207+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.209+0000] {spark_submit.py:649} INFO - "name" : "CHARGEID",
[2025-05-02T01:28:39.211+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.213+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.215+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.217+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.219+0000] {spark_submit.py:649} INFO - "name" : "PATIENTID",
[2025-05-02T01:28:39.220+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.222+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.224+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.226+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.228+0000] {spark_submit.py:649} INFO - "name" : "TYPE",
[2025-05-02T01:28:39.230+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.231+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.233+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.236+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.237+0000] {spark_submit.py:649} INFO - "name" : "AMOUNT",
[2025-05-02T01:28:39.239+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.241+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.243+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.245+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.246+0000] {spark_submit.py:649} INFO - "name" : "METHOD",
[2025-05-02T01:28:39.248+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.250+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.252+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.254+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.257+0000] {spark_submit.py:649} INFO - "name" : "FROMDATE",
[2025-05-02T01:28:39.259+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T01:28:39.261+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.263+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.265+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.267+0000] {spark_submit.py:649} INFO - "name" : "TODATE",
[2025-05-02T01:28:39.269+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T01:28:39.271+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.274+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.276+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.278+0000] {spark_submit.py:649} INFO - "name" : "PLACEOFSERVICE",
[2025-05-02T01:28:39.280+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.284+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.286+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.289+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.291+0000] {spark_submit.py:649} INFO - "name" : "PROCEDURECODE",
[2025-05-02T01:28:39.293+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.295+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.297+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.298+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.300+0000] {spark_submit.py:649} INFO - "name" : "MODIFIER1",
[2025-05-02T01:28:39.301+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.303+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.305+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.307+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.308+0000] {spark_submit.py:649} INFO - "name" : "MODIFIER2",
[2025-05-02T01:28:39.310+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.312+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.314+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.315+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.317+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF1",
[2025-05-02T01:28:39.319+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.321+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.323+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.325+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.327+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF2",
[2025-05-02T01:28:39.328+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.330+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.331+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.333+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.335+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF3",
[2025-05-02T01:28:39.337+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.339+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.341+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.342+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.344+0000] {spark_submit.py:649} INFO - "name" : "DIAGNOSISREF4",
[2025-05-02T01:28:39.345+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.347+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.348+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.350+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.352+0000] {spark_submit.py:649} INFO - "name" : "UNITS",
[2025-05-02T01:28:39.354+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.355+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.357+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.358+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.360+0000] {spark_submit.py:649} INFO - "name" : "DEPARTMENTID",
[2025-05-02T01:28:39.361+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.363+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.364+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.366+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.367+0000] {spark_submit.py:649} INFO - "name" : "NOTES",
[2025-05-02T01:28:39.369+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.370+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.372+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.374+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.376+0000] {spark_submit.py:649} INFO - "name" : "UNITAMOUNT",
[2025-05-02T01:28:39.378+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.379+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.380+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.382+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.383+0000] {spark_submit.py:649} INFO - "name" : "TRANSFEROUTID",
[2025-05-02T01:28:39.385+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.387+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.389+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.390+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.392+0000] {spark_submit.py:649} INFO - "name" : "TRANSFERTYPE",
[2025-05-02T01:28:39.394+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.396+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.397+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.399+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.401+0000] {spark_submit.py:649} INFO - "name" : "PAYMENTS",
[2025-05-02T01:28:39.402+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.404+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.407+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.409+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.410+0000] {spark_submit.py:649} INFO - "name" : "ADJUSTMENTS",
[2025-05-02T01:28:39.412+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.414+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.416+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.418+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.420+0000] {spark_submit.py:649} INFO - "name" : "TRANSFERS",
[2025-05-02T01:28:39.422+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.424+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.426+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.428+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.429+0000] {spark_submit.py:649} INFO - "name" : "OUTSTANDING",
[2025-05-02T01:28:39.431+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T01:28:39.443+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.445+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.447+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.449+0000] {spark_submit.py:649} INFO - "name" : "APPOINTMENTID",
[2025-05-02T01:28:39.451+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.453+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.455+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.457+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.459+0000] {spark_submit.py:649} INFO - "name" : "LINENOTE",
[2025-05-02T01:28:39.460+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.462+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.464+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.467+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.471+0000] {spark_submit.py:649} INFO - "name" : "PATIENTINSURANCEID",
[2025-05-02T01:28:39.473+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.477+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.479+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.481+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.483+0000] {spark_submit.py:649} INFO - "name" : "FEESCHEDULEID",
[2025-05-02T01:28:39.486+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T01:28:39.489+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.491+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.493+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.495+0000] {spark_submit.py:649} INFO - "name" : "PROVIDERID",
[2025-05-02T01:28:39.497+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.503+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.505+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.507+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.509+0000] {spark_submit.py:649} INFO - "name" : "SUPERVISINGPROVIDERID",
[2025-05-02T01:28:39.511+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.513+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.514+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.516+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T01:28:39.518+0000] {spark_submit.py:649} INFO - "name" : "HASH",
[2025-05-02T01:28:39.520+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T01:28:39.522+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T01:28:39.523+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T01:28:39.525+0000] {spark_submit.py:649} INFO - } ]
[2025-05-02T01:28:39.526+0000] {spark_submit.py:649} INFO - }
[2025-05-02T01:28:39.528+0000] {spark_submit.py:649} INFO - and corresponding Parquet message type:
[2025-05-02T01:28:39.529+0000] {spark_submit.py:649} INFO - message spark_schema {
[2025-05-02T01:28:39.530+0000] {spark_submit.py:649} INFO - optional binary ID (STRING);
[2025-05-02T01:28:39.532+0000] {spark_submit.py:649} INFO - optional binary CLAIMID (STRING);
[2025-05-02T01:28:39.535+0000] {spark_submit.py:649} INFO - optional int64 CHARGEID;
[2025-05-02T01:28:39.537+0000] {spark_submit.py:649} INFO - optional binary PATIENTID (STRING);
[2025-05-02T01:28:39.538+0000] {spark_submit.py:649} INFO - optional binary TYPE (STRING);
[2025-05-02T01:28:39.541+0000] {spark_submit.py:649} INFO - optional double AMOUNT;
[2025-05-02T01:28:39.543+0000] {spark_submit.py:649} INFO - optional binary METHOD (STRING);
[2025-05-02T01:28:39.544+0000] {spark_submit.py:649} INFO - optional int96 FROMDATE;
[2025-05-02T01:28:39.546+0000] {spark_submit.py:649} INFO - optional int96 TODATE;
[2025-05-02T01:28:39.550+0000] {spark_submit.py:649} INFO - optional binary PLACEOFSERVICE (STRING);
[2025-05-02T01:28:39.552+0000] {spark_submit.py:649} INFO - optional int64 PROCEDURECODE;
[2025-05-02T01:28:39.554+0000] {spark_submit.py:649} INFO - optional binary MODIFIER1 (STRING);
[2025-05-02T01:28:39.556+0000] {spark_submit.py:649} INFO - optional binary MODIFIER2 (STRING);
[2025-05-02T01:28:39.558+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF1;
[2025-05-02T01:28:39.560+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF2;
[2025-05-02T01:28:39.562+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF3;
[2025-05-02T01:28:39.564+0000] {spark_submit.py:649} INFO - optional int64 DIAGNOSISREF4;
[2025-05-02T01:28:39.566+0000] {spark_submit.py:649} INFO - optional int64 UNITS;
[2025-05-02T01:28:39.569+0000] {spark_submit.py:649} INFO - optional int64 DEPARTMENTID;
[2025-05-02T01:28:39.571+0000] {spark_submit.py:649} INFO - optional binary NOTES (STRING);
[2025-05-02T01:28:39.573+0000] {spark_submit.py:649} INFO - optional double UNITAMOUNT;
[2025-05-02T01:28:39.575+0000] {spark_submit.py:649} INFO - optional int64 TRANSFEROUTID;
[2025-05-02T01:28:39.577+0000] {spark_submit.py:649} INFO - optional binary TRANSFERTYPE (STRING);
[2025-05-02T01:28:39.578+0000] {spark_submit.py:649} INFO - optional double PAYMENTS;
[2025-05-02T01:28:39.580+0000] {spark_submit.py:649} INFO - optional double ADJUSTMENTS;
[2025-05-02T01:28:39.582+0000] {spark_submit.py:649} INFO - optional double TRANSFERS;
[2025-05-02T01:28:39.584+0000] {spark_submit.py:649} INFO - optional double OUTSTANDING;
[2025-05-02T01:28:39.586+0000] {spark_submit.py:649} INFO - optional binary APPOINTMENTID (STRING);
[2025-05-02T01:28:39.588+0000] {spark_submit.py:649} INFO - optional binary LINENOTE (STRING);
[2025-05-02T01:28:39.590+0000] {spark_submit.py:649} INFO - optional binary PATIENTINSURANCEID (STRING);
[2025-05-02T01:28:39.592+0000] {spark_submit.py:649} INFO - optional int64 FEESCHEDULEID;
[2025-05-02T01:28:39.594+0000] {spark_submit.py:649} INFO - optional binary PROVIDERID (STRING);
[2025-05-02T01:28:39.596+0000] {spark_submit.py:649} INFO - optional binary SUPERVISINGPROVIDERID (STRING);
[2025-05-02T01:28:39.598+0000] {spark_submit.py:649} INFO - optional binary HASH (STRING);
[2025-05-02T01:28:39.599+0000] {spark_submit.py:649} INFO - }
[2025-05-02T01:28:39.601+0000] {spark_submit.py:649} INFO - 
[2025-05-02T01:28:39.603+0000] {spark_submit.py:649} INFO - 
[2025-05-02T01:28:40.167+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:40 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-05-02T01:28:40.574+0000] {spark_submit.py:649} INFO - 25/05/02 01:28:40 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ***-scheduler:40993 in memory (size: 8.4 KiB, free: 407.6 MiB)
[2025-05-02T01:30:37.579+0000] {spark_submit.py:649} INFO - 25/05/02 01:30:34 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray((104,11,0,Vector(AccumulableInfo(489,None,Some(69),None,false,true,None), AccumulableInfo(493,None,Some(209715008),None,false,true,None), AccumulableInfo(495,None,Some(0),None,false,true,None), AccumulableInfo(496,None,Some(1),None,false,true,None), AccumulableInfo(497,None,Some(0),None,false,true,None), AccumulableInfo(498,None,Some(0),None,false,true,None), AccumulableInfo(499,None,Some(29326957),None,false,true,None), AccumulableInfo(500,None,Some(0),None,false,true,None), AccumulableInfo(501,None,Some(111602),None,false,true,None), AccumulableInfo(502,None,Some(0),None,false,true,None), AccumulableInfo(503,None,Some(0),None,false,true,None), AccumulableInfo(504,None,Some(0),None,false,true,None), AccumulableInfo(505,None,Some(0),None,false,true,None), AccumulableInfo(506,None,Some(0),None,false,true,None), AccumulableInfo(507,None,Some(0),None,false,true,None), AccumulableInfo(508,None,Some(0),None,false,true,None), AccumulableInfo(509,None,Some(0),None,false,true,None), AccumulableInfo(510,None,Some(0),None,false,true,None), AccumulableInfo(511,None,Some(0),None,false,true,None), AccumulableInfo(371,Some(records read),Some(111602),None,true,true,Some(sql)), AccumulableInfo(369,Some(local bytes read),Some(29326957),None,true,true,Some(sql)), AccumulableInfo(370,Some(fetch wait time),Some(0),None,true,true,Some(sql)), AccumulableInfo(366,Some(local blocks read),Some(1),None,true,true,Some(sql)), AccumulableInfo(478,Some(time in aggregation build),Some(3148),None,true,true,Some(sql)), AccumulableInfo(476,Some(peak memory),Some(88080304),None,true,true,Some(sql)), AccumulableInfo(475,Some(number of output rows),Some(5235),None,true,true,Some(sql)), AccumulableInfo(479,Some(avg hash probes per key),Some(16),None,true,true,Some(sql)), AccumulableInfo(1,Some(number of source rows),Some(5236),None,true,true,Some(sql)), AccumulableInfo(474,Some(number of output rows),Some(5236),None,true,true,Some(sql))))),Map((11,0) -> org.apache.spark.executor.ExecutorMetrics@57ffbe57)) by listener AppStatusListener took 3.714870739s.
[2025-05-02T01:30:57.222+0000] {spark_submit.py:649} INFO - 25/05/02 01:30:57 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray((104,11,0,Vector(AccumulableInfo(489,None,Some(69),None,false,true,None), AccumulableInfo(493,None,Some(209715008),None,false,true,None), AccumulableInfo(495,None,Some(0),None,false,true,None), AccumulableInfo(496,None,Some(1),None,false,true,None), AccumulableInfo(497,None,Some(0),None,false,true,None), AccumulableInfo(498,None,Some(0),None,false,true,None), AccumulableInfo(499,None,Some(29326957),None,false,true,None), AccumulableInfo(500,None,Some(0),None,false,true,None), AccumulableInfo(501,None,Some(111602),None,false,true,None), AccumulableInfo(502,None,Some(0),None,false,true,None), AccumulableInfo(503,None,Some(0),None,false,true,None), AccumulableInfo(504,None,Some(0),None,false,true,None), AccumulableInfo(505,None,Some(0),None,false,true,None), AccumulableInfo(506,None,Some(0),None,false,true,None), AccumulableInfo(507,None,Some(0),None,false,true,None), AccumulableInfo(508,None,Some(0),None,false,true,None), AccumulableInfo(509,None,Some(0),None,false,true,None), AccumulableInfo(510,None,Some(0),None,false,true,None), AccumulableInfo(511,None,Some(0),None,false,true,None), AccumulableInfo(371,Some(records read),Some(111602),None,true,true,Some(sql)), AccumulableInfo(369,Some(local bytes read),Some(29326957),None,true,true,Some(sql)), AccumulableInfo(370,Some(fetch wait time),Some(0),None,true,true,Some(sql)), AccumulableInfo(366,Some(local blocks read),Some(1),None,true,true,Some(sql)), AccumulableInfo(478,Some(time in aggregation build),Some(3148),None,true,true,Some(sql)), AccumulableInfo(476,Some(peak memory),Some(88080304),None,true,true,Some(sql)), AccumulableInfo(475,Some(number of output rows),Some(5235),None,true,true,Some(sql)), AccumulableInfo(479,Some(avg hash probes per key),Some(16),None,true,true,Some(sql)), AccumulableInfo(1,Some(number of source rows),Some(5236),None,true,true,Some(sql)), AccumulableInfo(474,Some(number of output rows),Some(5236),None,true,true,Some(sql))))),Map((11,0) -> org.apache.spark.executor.ExecutorMetrics@57ffbe57)) by listener SQLAppStatusListener took 6.047427811s.
[2025-05-02T01:29:25.615+0000] {job.py:239} ERROR - Job heartbeat failed with error
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/MySQLdb/connections.py", line 200, in __init__
    super().__init__(*args, **kwargs2)
MySQLdb.OperationalError: (2005, "Unknown server host 'mysql' (-3)")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py", line 233, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/local_task_job_runner.py", line 284, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2360, in refresh_from_db
    _refresh_from_db(task_instance=self, session=session, lock_for_update=lock_for_update)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 873, in _refresh_from_db
    ti = TaskInstance.get_task_instance(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2346, in get_task_instance
    return query.one_or_none()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/MySQLdb/connections.py", line 200, in __init__
    super().__init__(*args, **kwargs2)
sqlalchemy.exc.OperationalError: (MySQLdb.OperationalError) (2005, "Unknown server host 'mysql' (-3)")
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-05-02T01:30:57.695+0000] {job.py:254} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-05-02T01:32:05.232+0000] {spark_submit.py:649} INFO - 25/05/02 01:32:03 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray((104,11,0,Vector(AccumulableInfo(489,None,Some(214),None,false,true,None), AccumulableInfo(493,None,Some(209715008),None,false,true,None), AccumulableInfo(495,None,Some(0),None,false,true,None), AccumulableInfo(496,None,Some(1),None,false,true,None), AccumulableInfo(497,None,Some(0),None,false,true,None), AccumulableInfo(498,None,Some(0),None,false,true,None), AccumulableInfo(499,None,Some(29326957),None,false,true,None), AccumulableInfo(500,None,Some(0),None,false,true,None), AccumulableInfo(501,None,Some(111602),None,false,true,None), AccumulableInfo(502,None,Some(0),None,false,true,None), AccumulableInfo(503,None,Some(0),None,false,true,None), AccumulableInfo(504,None,Some(0),None,false,true,None), AccumulableInfo(505,None,Some(0),None,false,true,None), AccumulableInfo(506,None,Some(0),None,false,true,None), AccumulableInfo(507,None,Some(0),None,false,true,None), AccumulableInfo(508,None,Some(0),None,false,true,None), AccumulableInfo(509,None,Some(0),None,false,true,None), AccumulableInfo(510,None,Some(0),None,false,true,None), AccumulableInfo(511,None,Some(0),None,false,true,None), AccumulableInfo(371,Some(records read),Some(111602),None,true,true,Some(sql)), AccumulableInfo(369,Some(local bytes read),Some(29326957),None,true,true,Some(sql)), AccumulableInfo(370,Some(fetch wait time),Some(0),None,true,true,Some(sql)), AccumulableInfo(366,Some(local blocks read),Some(1),None,true,true,Some(sql)), AccumulableInfo(472,Some(duration),Some(136071),None,true,true,Some(sql)), AccumulableInfo(478,Some(time in aggregation build),Some(3148),None,true,true,Some(sql)), AccumulableInfo(476,Some(peak memory),Some(88080304),None,true,true,Some(sql)), AccumulableInfo(475,Some(number of output rows),Some(111602),None,true,true,Some(sql)), AccumulableInfo(479,Some(avg hash probes per key),Some(16),None,true,true,Some(sql)), AccumulableInfo(1,Some(number of source rows),Some(111602),None,true,true,Some(sql)), AccumulableInfo(474,Some(number of output rows),Some(111602),None,true,true,Some(sql))))),Map((11,0) -> org.apache.spark.executor.ExecutorMetrics@1b5206fc)) by listener AppStatusListener took 3.314321761s.
[2025-05-02T01:32:45.218+0000] {spark_submit.py:649} INFO - 25/05/02 01:32:13 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@323c191d)) by listener AppStatusListener took 2.164415851s.
[2025-05-02T01:33:46.010+0000] {spark_submit.py:649} INFO - 25/05/02 01:32:29 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@6c663dc7)) by listener AppStatusListener took 7.515842856s.
