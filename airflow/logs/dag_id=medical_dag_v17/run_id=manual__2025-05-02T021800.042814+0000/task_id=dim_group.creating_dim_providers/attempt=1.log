[2025-05-02T02:38:50.249+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-05-02T02:38:50.271+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: medical_dag_v17.dim_group.creating_dim_providers manual__2025-05-02T02:18:00.042814+00:00 [queued]>
[2025-05-02T02:38:50.286+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: medical_dag_v17.dim_group.creating_dim_providers manual__2025-05-02T02:18:00.042814+00:00 [queued]>
[2025-05-02T02:38:50.287+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-05-02T02:38:50.310+0000] {taskinstance.py:2890} INFO - Executing <Task(SparkSubmitOperator): dim_group.creating_dim_providers> on 2025-05-02 02:18:00.042814+00:00
[2025-05-02T02:38:50.320+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'medical_dag_v17', 'dim_group.creating_dim_providers', 'manual__2025-05-02T02:18:00.042814+00:00', '--job-id', '183', '--raw', '--subdir', 'DAGS_FOLDER/raw_enriched_dag.py', '--cfg-path', '/tmp/tmpiaxi5qfi']
[2025-05-02T02:38:50.323+0000] {standard_task_runner.py:105} INFO - Job 183: Subtask dim_group.creating_dim_providers
[2025-05-02T02:38:50.324+0000] {logging_mixin.py:190} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=11540) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-05-02T02:38:50.326+0000] {standard_task_runner.py:72} INFO - Started process 11541 to run task
[2025-05-02T02:38:50.386+0000] {task_command.py:467} INFO - Running <TaskInstance: medical_dag_v17.dim_group.creating_dim_providers manual__2025-05-02T02:18:00.042814+00:00 [running]> on host airflow-scheduler
[2025-05-02T02:38:50.497+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='medical_dag_v17' AIRFLOW_CTX_TASK_ID='dim_group.creating_dim_providers' AIRFLOW_CTX_EXECUTION_DATE='2025-05-02T02:18:00.042814+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-02T02:18:00.042814+00:00'
[2025-05-02T02:38:50.498+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-05-02T02:38:50.547+0000] {base.py:84} INFO - Retrieving connection 'spark_conn'
[2025-05-02T02:38:50.549+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local --packages io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4 --name arrow-spark --deploy-mode client /opt/etl/enriched_curated/creating_dim_providers.py
[2025-05-02T02:38:53.354+0000] {spark_submit.py:649} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-05-02T02:38:53.485+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-02T02:38:53.487+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-02T02:38:53.492+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12 added as a dependency
[2025-05-02T02:38:53.493+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-05-02T02:38:53.493+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-71b4fddf-1f2d-425b-95c1-23e28a63a750;1.0
[2025-05-02T02:38:53.494+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T02:38:53.708+0000] {spark_submit.py:649} INFO - found io.delta#delta-spark_2.12;3.3.0 in central
[2025-05-02T02:38:53.765+0000] {spark_submit.py:649} INFO - found io.delta#delta-storage;3.3.0 in central
[2025-05-02T02:38:53.804+0000] {spark_submit.py:649} INFO - found org.antlr#antlr4-runtime;4.9.3 in central
[2025-05-02T02:38:53.884+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2025-05-02T02:38:53.913+0000] {spark_submit.py:649} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2025-05-02T02:38:53.947+0000] {spark_submit.py:649} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-05-02T02:38:53.981+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve 471ms :: artifacts dl 17ms
[2025-05-02T02:38:53.983+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-02T02:38:53.984+0000] {spark_submit.py:649} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2025-05-02T02:38:53.985+0000] {spark_submit.py:649} INFO - io.delta#delta-spark_2.12;3.3.0 from central in [default]
[2025-05-02T02:38:53.986+0000] {spark_submit.py:649} INFO - io.delta#delta-storage;3.3.0 from central in [default]
[2025-05-02T02:38:53.987+0000] {spark_submit.py:649} INFO - org.antlr#antlr4-runtime;4.9.3 from central in [default]
[2025-05-02T02:38:53.988+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2025-05-02T02:38:53.988+0000] {spark_submit.py:649} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-05-02T02:38:53.989+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:38:53.990+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-02T02:38:53.991+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-02T02:38:53.992+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:38:53.992+0000] {spark_submit.py:649} INFO - |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
[2025-05-02T02:38:53.993+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:38:53.994+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-71b4fddf-1f2d-425b-95c1-23e28a63a750
[2025-05-02T02:38:53.995+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T02:38:54.006+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 6 already retrieved (0kB/12ms)
[2025-05-02T02:38:54.421+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-02T02:38:59.505+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SparkContext: Running Spark version 3.5.3
[2025-05-02T02:38:59.506+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SparkContext: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-02T02:38:59.507+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SparkContext: Java version 17.0.14
[2025-05-02T02:38:59.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceUtils: ==============================================================
[2025-05-02T02:38:59.532+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-02T02:38:59.533+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceUtils: ==============================================================
[2025-05-02T02:38:59.534+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SparkContext: Submitted application: MyETLPipeline
[2025-05-02T02:38:59.559+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-02T02:38:59.567+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceProfile: Limiting resource is cpu
[2025-05-02T02:38:59.568+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-02T02:38:59.649+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SecurityManager: Changing view acls to: ***
[2025-05-02T02:38:59.650+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SecurityManager: Changing modify acls to: ***
[2025-05-02T02:38:59.651+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SecurityManager: Changing view acls groups to:
[2025-05-02T02:38:59.652+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SecurityManager: Changing modify acls groups to:
[2025-05-02T02:38:59.653+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-05-02T02:38:59.949+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO Utils: Successfully started service 'sparkDriver' on port 40269.
[2025-05-02T02:38:59.993+0000] {spark_submit.py:649} INFO - 25/05/02 02:38:59 INFO SparkEnv: Registering MapOutputTracker
[2025-05-02T02:39:00.031+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-02T02:39:00.054+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-02T02:39:00.055+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-02T02:39:00.059+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-02T02:39:00.093+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a9b8008-bb1c-4755-8d0c-eeb8ed3f509e
[2025-05-02T02:39:00.111+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-02T02:39:00.131+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-02T02:39:00.324+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-02T02:39:00.417+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-02T02:39:00.469+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at spark://***-scheduler:40269/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:00.470+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://***-scheduler:40269/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746153539498
[2025-05-02T02:39:00.471+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at spark://***-scheduler:40269/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:00.472+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at spark://***-scheduler:40269/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746153539498
[2025-05-02T02:39:00.473+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://***-scheduler:40269/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746153539498
[2025-05-02T02:39:00.473+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://***-scheduler:40269/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746153539498
[2025-05-02T02:39:00.474+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:00.476+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T02:39:00.514+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746153539498
[2025-05-02T02:39:00.515+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T02:39:00.524+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar at file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:00.525+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Copying /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-storage-3.3.0.jar
[2025-05-02T02:39:00.530+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746153539498
[2025-05-02T02:39:00.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Copying /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T02:39:00.540+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746153539498
[2025-05-02T02:39:00.541+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:00 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T02:39:01.100+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746153539498
[2025-05-02T02:39:01.101+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T02:39:01.186+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Starting executor ID driver on host ***-scheduler
[2025-05-02T02:39:01.187+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: OS info Linux, 5.15.153.1-microsoft-standard-WSL2, amd64
[2025-05-02T02:39:01.188+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Java version 17.0.14
[2025-05-02T02:39:01.195+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-05-02T02:39:01.196+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@74beeae1 for default.
[2025-05-02T02:39:01.208+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:01.229+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-storage-3.3.0.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-storage-3.3.0.jar
[2025-05-02T02:39:01.233+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746153539498
[2025-05-02T02:39:01.240+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T02:39:01.244+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:01.249+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/io.delta_delta-spark_2.12-3.3.0.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T02:39:01.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746153539498
[2025-05-02T02:39:01.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T02:39:01.259+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746153539498
[2025-05-02T02:39:01.260+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T02:39:01.264+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746153539498
[2025-05-02T02:39:01.415+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T02:39:01.421+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/io.delta_delta-spark_2.12-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:01.467+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO TransportClientFactory: Successfully created connection to ***-scheduler/172.18.0.3:40269 after 32 ms (0 ms spent in bootstraps)
[2025-05-02T02:39:01.474+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/io.delta_delta-spark_2.12-3.3.0.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp296488139033022526.tmp
[2025-05-02T02:39:01.529+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp296488139033022526.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-spark_2.12-3.3.0.jar
[2025-05-02T02:39:01.536+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-spark_2.12-3.3.0.jar to class loader default
[2025-05-02T02:39:01.537+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1746153539498
[2025-05-02T02:39:01.537+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp18310371119070385686.tmp
[2025-05-02T02:39:01.541+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp18310371119070385686.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-05-02T02:39:01.545+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to class loader default
[2025-05-02T02:39:01.546+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1746153539498
[2025-05-02T02:39:01.547+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp1265064728640368232.tmp
[2025-05-02T02:39:01.553+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp1265064728640368232.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-05-02T02:39:01.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.apache.hadoop_hadoop-aws-3.3.4.jar to class loader default
[2025-05-02T02:39:01.559+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746153539498
[2025-05-02T02:39:01.560+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp15711876963332855894.tmp
[2025-05-02T02:39:01.562+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp15711876963332855894.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.antlr_antlr4-runtime-4.9.3.jar
[2025-05-02T02:39:01.567+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/org.antlr_antlr4-runtime-4.9.3.jar to class loader default
[2025-05-02T02:39:01.567+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/io.delta_delta-storage-3.3.0.jar with timestamp 1746153539498
[2025-05-02T02:39:01.568+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/io.delta_delta-storage-3.3.0.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp3516108893000715789.tmp
[2025-05-02T02:39:01.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp3516108893000715789.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-storage-3.3.0.jar
[2025-05-02T02:39:01.574+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/io.delta_delta-storage-3.3.0.jar to class loader default
[2025-05-02T02:39:01.575+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Executor: Fetching spark://***-scheduler:40269/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1746153539498
[2025-05-02T02:39:01.576+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:01 INFO Utils: Fetching spark://***-scheduler:40269/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp2359826580220311991.tmp
[2025-05-02T02:39:02.995+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:02 INFO Utils: /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/fetchFileTemp2359826580220311991.tmp has been previously copied to /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2025-05-02T02:39:03.040+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO Executor: Adding file:/tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/userFiles-cf765b55-81d9-4d04-9e40-1cba171f791c/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to class loader default
[2025-05-02T02:39:03.060+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43081.
[2025-05-02T02:39:03.061+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO NettyBlockTransferService: Server created on ***-scheduler:43081
[2025-05-02T02:39:03.064+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-02T02:39:03.075+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ***-scheduler, 43081, None)
[2025-05-02T02:39:03.082+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO BlockManagerMasterEndpoint: Registering block manager ***-scheduler:43081 with 434.4 MiB RAM, BlockManagerId(driver, ***-scheduler, 43081, None)
[2025-05-02T02:39:03.086+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ***-scheduler, 43081, None)
[2025-05-02T02:39:03.089+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ***-scheduler, 43081, None)
[2025-05-02T02:39:03.771+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-02T02:39:03.774+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:03 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-05-02T02:39:05.090+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:05 INFO HiveConf: Found configuration file null
[2025-05-02T02:39:05.102+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:05 INFO HiveUtils: Initializing HiveMetastoreConnection version 3.1.3 using maven.
[2025-05-02T02:39:05.114+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-02T02:39:05.116+0000] {spark_submit.py:649} INFO - https://maven-central.storage-download.googleapis.com/maven2/ added as a remote repository with the name: repo-1
[2025-05-02T02:39:05.129+0000] {spark_submit.py:649} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-05-02T02:39:05.130+0000] {spark_submit.py:649} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-05-02T02:39:05.131+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api added as a dependency
[2025-05-02T02:39:05.132+0000] {spark_submit.py:649} INFO - org.apache.derby#derby added as a dependency
[2025-05-02T02:39:05.133+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore added as a dependency
[2025-05-02T02:39:05.134+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec added as a dependency
[2025-05-02T02:39:05.134+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common added as a dependency
[2025-05-02T02:39:05.135+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde added as a dependency
[2025-05-02T02:39:05.136+0000] {spark_submit.py:649} INFO - com.google.guava#guava added as a dependency
[2025-05-02T02:39:05.137+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api added as a dependency
[2025-05-02T02:39:05.137+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime added as a dependency
[2025-05-02T02:39:05.138+0000] {spark_submit.py:649} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-2f5dfa83-a718-4453-983e-d7e536d9f3f9;1.0
[2025-05-02T02:39:05.139+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T02:39:05.160+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-api;2.10.0 in central
[2025-05-02T02:39:05.180+0000] {spark_submit.py:649} INFO - found org.apache.derby#derby;10.14.1.0 in central
[2025-05-02T02:39:05.205+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-metastore;3.1.3 in central
[2025-05-02T02:39:05.247+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-serde;3.1.3 in central
[2025-05-02T02:39:05.291+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-common;3.1.3 in central
[2025-05-02T02:39:05.321+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-classification;3.1.3 in central
[2025-05-02T02:39:05.343+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.10 in central
[2025-05-02T02:39:05.374+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-upgrade-acid;3.1.3 in central
[2025-05-02T02:39:05.407+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-shims;3.1.3 in central
[2025-05-02T02:39:05.431+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-common;3.1.3 in central
[2025-05-02T02:39:05.457+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 in central
[2025-05-02T02:39:05.480+0000] {spark_submit.py:649} INFO - found com.google.guava#guava;19.0 in central
[2025-05-02T02:39:05.507+0000] {spark_submit.py:649} INFO - found commons-lang#commons-lang;2.6 in central
[2025-05-02T02:39:05.532+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libthrift;0.9.3 in central
[2025-05-02T02:39:05.556+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpclient;4.5.13 in central
[2025-05-02T02:39:05.579+0000] {spark_submit.py:649} INFO - found org.apache.httpcomponents#httpcore;4.4.13 in central
[2025-05-02T02:39:05.606+0000] {spark_submit.py:649} INFO - found commons-logging#commons-logging;1.2 in central
[2025-05-02T02:39:05.639+0000] {spark_submit.py:649} INFO - found commons-codec#commons-codec;1.15 in central
[2025-05-02T02:39:05.667+0000] {spark_submit.py:649} INFO - found org.apache.zookeeper#zookeeper;3.4.6 in central
[2025-05-02T02:39:05.690+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-log4j12;1.6.1 in central
[2025-05-02T02:39:05.711+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.16 in central
[2025-05-02T02:39:05.732+0000] {spark_submit.py:649} INFO - found jline#jline;2.12 in central
[2025-05-02T02:39:05.756+0000] {spark_submit.py:649} INFO - found io.netty#netty;3.7.0.Final in central
[2025-05-02T02:39:05.787+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-core;2.17.1 in central
[2025-05-02T02:39:05.813+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-0.23;3.1.3 in central
[2025-05-02T02:39:05.853+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 in central
[2025-05-02T02:39:05.899+0000] {spark_submit.py:649} INFO - found javax.servlet#javax.servlet-api;3.1.0 in central
[2025-05-02T02:39:05.943+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-annotations;3.1.0 in central
[2025-05-02T02:39:05.967+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-servlet;4.0 in central
[2025-05-02T02:39:05.991+0000] {spark_submit.py:649} INFO - found com.google.inject#guice;4.0 in central
[2025-05-02T02:39:06.015+0000] {spark_submit.py:649} INFO - found javax.inject#javax.inject;1 in central
[2025-05-02T02:39:06.044+0000] {spark_submit.py:649} INFO - found aopalliance#aopalliance;1.0 in central
[2025-05-02T02:39:06.075+0000] {spark_submit.py:649} INFO - found com.google.protobuf#protobuf-java;2.5.0 in central
[2025-05-02T02:39:06.105+0000] {spark_submit.py:649} INFO - found commons-io#commons-io;2.6 in central
[2025-05-02T02:39:06.131+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-json;1.19 in central
[2025-05-02T02:39:06.156+0000] {spark_submit.py:649} INFO - found org.codehaus.jettison#jettison;1.1 in central
[2025-05-02T02:39:06.185+0000] {spark_submit.py:649} INFO - found com.sun.xml.bind#jaxb-impl;2.2.3-1 in central
[2025-05-02T02:39:06.210+0000] {spark_submit.py:649} INFO - found javax.xml.bind#jaxb-api;2.2.11 in central
[2025-05-02T02:39:06.234+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
[2025-05-02T02:39:06.265+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
[2025-05-02T02:39:06.288+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-jaxrs;1.9.13 in central
[2025-05-02T02:39:06.312+0000] {spark_submit.py:649} INFO - found org.codehaus.jackson#jackson-xc;1.9.13 in central
[2025-05-02T02:39:06.343+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-core;1.19 in central
[2025-05-02T02:39:06.366+0000] {spark_submit.py:649} INFO - found javax.ws.rs#jsr311-api;1.1.1 in central
[2025-05-02T02:39:06.388+0000] {spark_submit.py:649} INFO - found com.sun.jersey.contribs#jersey-guice;1.19 in central
[2025-05-02T02:39:06.412+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-servlet;1.19 in central
[2025-05-02T02:39:06.436+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-server;1.19 in central
[2025-05-02T02:39:06.467+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-common;3.1.0 in central
[2025-05-02T02:39:06.508+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-api;3.1.0 in central
[2025-05-02T02:39:06.583+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-annotations;2.12.0 in central
[2025-05-02T02:39:06.616+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-auth;3.1.0 in central
[2025-05-02T02:39:06.690+0000] {spark_submit.py:649} INFO - found com.nimbusds#nimbus-jose-jwt;4.41.1 in central
[2025-05-02T02:39:06.716+0000] {spark_submit.py:649} INFO - found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2025-05-02T02:39:06.741+0000] {spark_submit.py:649} INFO - found net.minidev#json-smart;2.3 in central
[2025-05-02T02:39:06.767+0000] {spark_submit.py:649} INFO - found net.minidev#accessors-smart;1.2 in central
[2025-05-02T02:39:06.793+0000] {spark_submit.py:649} INFO - found org.ow2.asm#asm;5.0.4 in central
[2025-05-02T02:39:06.836+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-simplekdc;1.0.1 in central
[2025-05-02T02:39:06.861+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-client;1.0.1 in central
[2025-05-02T02:39:06.890+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-config;1.0.1 in central
[2025-05-02T02:39:06.920+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-core;1.0.1 in central
[2025-05-02T02:39:06.948+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-pkix;1.0.1 in central
[2025-05-02T02:39:06.981+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-asn1;1.0.1 in central
[2025-05-02T02:39:07.014+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-util;1.0.1 in central
[2025-05-02T02:39:07.043+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-common;1.0.1 in central
[2025-05-02T02:39:07.075+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-crypto;1.0.1 in central
[2025-05-02T02:39:07.120+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-util;1.0.1 in central
[2025-05-02T02:39:07.150+0000] {spark_submit.py:649} INFO - found org.apache.kerby#token-provider;1.0.1 in central
[2025-05-02T02:39:07.182+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-admin;1.0.1 in central
[2025-05-02T02:39:07.209+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-server;1.0.1 in central
[2025-05-02T02:39:07.236+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerb-identity;1.0.1 in central
[2025-05-02T02:39:07.265+0000] {spark_submit.py:649} INFO - found org.apache.kerby#kerby-xdr;1.0.1 in central
[2025-05-02T02:39:07.291+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-compress;1.19 in central
[2025-05-02T02:39:07.317+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util;9.3.19.v20170502 in central
[2025-05-02T02:39:07.349+0000] {spark_submit.py:649} INFO - found com.sun.jersey#jersey-client;1.19 in central
[2025-05-02T02:39:07.372+0000] {spark_submit.py:649} INFO - found commons-cli#commons-cli;1.2 in central
[2025-05-02T02:39:07.407+0000] {spark_submit.py:649} INFO - found log4j#log4j;1.2.17 in central
[2025-05-02T02:39:07.431+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-core;2.12.0 in central
[2025-05-02T02:39:07.454+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.core#jackson-databind;2.12.0 in central
[2025-05-02T02:39:07.480+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 in central
[2025-05-02T02:39:07.506+0000] {spark_submit.py:649} INFO - found jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central
[2025-05-02T02:39:07.535+0000] {spark_submit.py:649} INFO - found jakarta.activation#jakarta.activation-api;1.2.1 in central
[2025-05-02T02:39:07.560+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 in central
[2025-05-02T02:39:07.584+0000] {spark_submit.py:649} INFO - found com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 in central
[2025-05-02T02:39:07.606+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 in central
[2025-05-02T02:39:07.631+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-common;3.1.0 in central
[2025-05-02T02:39:07.664+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-registry;3.1.0 in central
[2025-05-02T02:39:07.733+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-common;3.1.0 in central
[2025-05-02T02:39:07.805+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-math3;3.1.1 in central
[2025-05-02T02:39:07.860+0000] {spark_submit.py:649} INFO - found commons-net#commons-net;3.6 in central
[2025-05-02T02:39:07.898+0000] {spark_submit.py:649} INFO - found commons-collections#commons-collections;3.2.2 in central
[2025-05-02T02:39:07.938+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-server;9.3.20.v20170531 in central
[2025-05-02T02:39:07.970+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-http;9.3.20.v20170531 in central
[2025-05-02T02:39:08.010+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-io;9.3.20.v20170531 in central
[2025-05-02T02:39:08.038+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 in central
[2025-05-02T02:39:08.066+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-security;9.3.20.v20170531 in central
[2025-05-02T02:39:08.102+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 in central
[2025-05-02T02:39:08.132+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-xml;9.3.20.v20170531 in central
[2025-05-02T02:39:08.230+0000] {spark_submit.py:649} INFO - found commons-beanutils#commons-beanutils;1.9.3 in central
[2025-05-02T02:39:08.269+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-configuration2;2.1.1 in central
[2025-05-02T02:39:08.323+0000] {spark_submit.py:649} INFO - found org.apache.avro#avro;1.8.2 in central
[2025-05-02T02:39:08.373+0000] {spark_submit.py:649} INFO - found com.thoughtworks.paranamer#paranamer;2.7 in central
[2025-05-02T02:39:08.402+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.4 in central
[2025-05-02T02:39:08.445+0000] {spark_submit.py:649} INFO - found org.tukaani#xz;1.5 in central
[2025-05-02T02:39:08.473+0000] {spark_submit.py:649} INFO - found com.google.re2j#re2j;1.1 in central
[2025-05-02T02:39:08.512+0000] {spark_submit.py:649} INFO - found com.google.code.gson#gson;2.2.4 in central
[2025-05-02T02:39:08.544+0000] {spark_submit.py:649} INFO - found com.jcraft#jsch;0.1.54 in central
[2025-05-02T02:39:08.571+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-05-02T02:39:08.601+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core4;4.1.0-incubating in central
[2025-05-02T02:39:08.664+0000] {spark_submit.py:649} INFO - found org.codehaus.woodstox#stax2-api;3.1.4 in central
[2025-05-02T02:39:08.692+0000] {spark_submit.py:649} INFO - found com.fasterxml.woodstox#woodstox-core;5.0.3 in central
[2025-05-02T02:39:08.715+0000] {spark_submit.py:649} INFO - found commons-daemon#commons-daemon;1.0.13 in central
[2025-05-02T02:39:08.744+0000] {spark_submit.py:649} INFO - found dnsjava#dnsjava;2.1.7 in central
[2025-05-02T02:39:08.770+0000] {spark_submit.py:649} INFO - found org.fusesource.leveldbjni#leveldbjni-all;1.8 in central
[2025-05-02T02:39:08.790+0000] {spark_submit.py:649} INFO - found org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 in central
[2025-05-02T02:39:08.810+0000] {spark_submit.py:649} INFO - found org.ehcache#ehcache;3.3.1 in central
[2025-05-02T02:39:08.833+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP-java7;2.4.12 in central
[2025-05-02T02:39:08.872+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 in central
[2025-05-02T02:39:08.971+0000] {spark_submit.py:649} INFO - found de.ruedigermoeller#fst;2.50 in central
[2025-05-02T02:39:08.994+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#java-util;1.9.0 in central
[2025-05-02T02:39:09.019+0000] {spark_submit.py:649} INFO - found com.cedarsoftware#json-io;2.5.1 in central
[2025-05-02T02:39:09.044+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 in central
[2025-05-02T02:39:09.195+0000] {spark_submit.py:649} INFO - found javax.servlet.jsp#jsp-api;2.1 in central
[2025-05-02T02:39:09.225+0000] {spark_submit.py:649} INFO - found com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 in central
[2025-05-02T02:39:09.251+0000] {spark_submit.py:649} INFO - found org.apache.hive.shims#hive-shims-scheduler;3.1.3 in central
[2025-05-02T02:39:09.269+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-storage-api;2.7.0 in central
[2025-05-02T02:39:09.278+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-lang3;3.9 in central
[2025-05-02T02:39:09.295+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-core;1.5.8 in central
[2025-05-02T02:39:09.313+0000] {spark_submit.py:649} INFO - found org.apache.orc#orc-shims;1.5.8 in central
[2025-05-02T02:39:09.332+0000] {spark_submit.py:649} INFO - found io.airlift#aircompressor;0.10 in central
[2025-05-02T02:39:09.348+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 in central
[2025-05-02T02:39:09.363+0000] {spark_submit.py:649} INFO - found org.eclipse.jetty#jetty-client;9.3.20.v20170531 in central
[2025-05-02T02:39:09.379+0000] {spark_submit.py:649} INFO - found joda-time#joda-time;2.9.9 in central
[2025-05-02T02:39:09.398+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-1.2-api;2.17.1 in central
[2025-05-02T02:39:09.416+0000] {spark_submit.py:649} INFO - found org.apache.logging.log4j#log4j-web;2.17.1 in central
[2025-05-02T02:39:09.429+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant;1.9.1 in central
[2025-05-02T02:39:09.443+0000] {spark_submit.py:649} INFO - found org.apache.ant#ant-launcher;1.9.1 in central
[2025-05-02T02:39:09.458+0000] {spark_submit.py:649} INFO - found net.sf.jpam#jpam;1.1 in central
[2025-05-02T02:39:09.472+0000] {spark_submit.py:649} INFO - found com.tdunning#json;1.8 in central
[2025-05-02T02:39:09.487+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.1.0 in central
[2025-05-02T02:39:09.502+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-jvm;3.1.0 in central
[2025-05-02T02:39:09.516+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-json;3.1.0 in central
[2025-05-02T02:39:09.531+0000] {spark_submit.py:649} INFO - found com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in central
[2025-05-02T02:39:09.555+0000] {spark_submit.py:649} INFO - found javolution#javolution;5.5.1 in central
[2025-05-02T02:39:09.577+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-service-rpc;3.1.3 in central
[2025-05-02T02:39:09.590+0000] {spark_submit.py:649} INFO - found org.apache.thrift#libfb303;0.9.3 in central
[2025-05-02T02:39:09.606+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-vector;0.8.0 in central
[2025-05-02T02:39:09.619+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-format;0.8.0 in central
[2025-05-02T02:39:09.632+0000] {spark_submit.py:649} INFO - found com.vlkan#flatbuffers;1.2.0-3f79e055 in central
[2025-05-02T02:39:09.645+0000] {spark_submit.py:649} INFO - found org.apache.arrow#arrow-memory;0.8.0 in central
[2025-05-02T02:39:09.663+0000] {spark_submit.py:649} INFO - found io.netty#netty-buffer;4.1.17.Final in central
[2025-05-02T02:39:09.679+0000] {spark_submit.py:649} INFO - found io.netty#netty-common;4.1.17.Final in central
[2025-05-02T02:39:09.698+0000] {spark_submit.py:649} INFO - found com.carrotsearch#hppc;0.7.2 in central
[2025-05-02T02:39:09.711+0000] {spark_submit.py:649} INFO - found net.sf.opencsv#opencsv;2.3 in central
[2025-05-02T02:39:09.723+0000] {spark_submit.py:649} INFO - found org.apache.parquet#parquet-hadoop-bundle;1.10.0 in central
[2025-05-02T02:39:09.742+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-standalone-metastore;3.1.3 in central
[2025-05-02T02:39:09.767+0000] {spark_submit.py:649} INFO - found com.jolbox#bonecp;0.8.0.RELEASE in central
[2025-05-02T02:39:09.785+0000] {spark_submit.py:649} INFO - found com.zaxxer#HikariCP;2.6.1 in central
[2025-05-02T02:39:09.798+0000] {spark_submit.py:649} INFO - found commons-dbcp#commons-dbcp;1.4 in central
[2025-05-02T02:39:09.810+0000] {spark_submit.py:649} INFO - found commons-pool#commons-pool;1.5.4 in central
[2025-05-02T02:39:09.830+0000] {spark_submit.py:649} INFO - found org.antlr#antlr-runtime;3.5.2 in central
[2025-05-02T02:39:09.859+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-api-jdo;4.2.4 in central
[2025-05-02T02:39:09.879+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-core;4.1.17 in central
[2025-05-02T02:39:09.892+0000] {spark_submit.py:649} INFO - found org.datanucleus#datanucleus-rdbms;4.1.19 in central
[2025-05-02T02:39:09.905+0000] {spark_submit.py:649} INFO - found org.datanucleus#javax.jdo;3.2.0-m3 in central
[2025-05-02T02:39:09.918+0000] {spark_submit.py:649} INFO - found javax.transaction#transaction-api;1.1 in central
[2025-05-02T02:39:09.930+0000] {spark_submit.py:649} INFO - found sqlline#sqlline;1.3.0 in central
[2025-05-02T02:39:09.951+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-client;2.0.0-alpha4 in central
[2025-05-02T02:39:09.965+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 in central
[2025-05-02T02:39:09.981+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 in central
[2025-05-02T02:39:09.994+0000] {spark_submit.py:649} INFO - found org.apache.yetus#audience-annotations;0.5.0 in central
[2025-05-02T02:39:10.006+0000] {spark_submit.py:649} INFO - found junit#junit;4.11 in central
[2025-05-02T02:39:10.019+0000] {spark_submit.py:649} INFO - found org.hamcrest#hamcrest-core;1.3 in central
[2025-05-02T02:39:10.036+0000] {spark_submit.py:649} INFO - found org.apache.hbase#hbase-protocol;2.0.0-alpha4 in central
[2025-05-02T02:39:10.058+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 in central
[2025-05-02T02:39:10.071+0000] {spark_submit.py:649} INFO - found org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 in central
[2025-05-02T02:39:10.086+0000] {spark_submit.py:649} INFO - found org.apache.htrace#htrace-core;3.2.0-incubating in central
[2025-05-02T02:39:10.099+0000] {spark_submit.py:649} INFO - found org.jruby.jcodings#jcodings;1.0.18 in central
[2025-05-02T02:39:10.113+0000] {spark_submit.py:649} INFO - found org.jruby.joni#joni;2.1.11 in central
[2025-05-02T02:39:10.123+0000] {spark_submit.py:649} INFO - found io.dropwizard.metrics#metrics-core;3.2.1 in central
[2025-05-02T02:39:10.140+0000] {spark_submit.py:649} INFO - found org.apache.commons#commons-crypto;1.0.0 in central
[2025-05-02T02:39:10.161+0000] {spark_submit.py:649} INFO - found javax.jdo#jdo-api;3.0.1 in central
[2025-05-02T02:39:10.171+0000] {spark_submit.py:649} INFO - found javax.transaction#jta;1.1 in central
[2025-05-02T02:39:10.183+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-api;0.6.0 in central
[2025-05-02T02:39:10.195+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-core;0.6.0 in central
[2025-05-02T02:39:10.213+0000] {spark_submit.py:649} INFO - found com.google.inject.extensions#guice-assistedinject;3.0 in central
[2025-05-02T02:39:10.236+0000] {spark_submit.py:649} INFO - found it.unimi.dsi#fastutil;6.5.6 in central
[2025-05-02T02:39:10.249+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-common;0.6.0-incubating in central
[2025-05-02T02:39:10.265+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-core;0.6.0-incubating in central
[2025-05-02T02:39:10.279+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-api;0.6.0-incubating in central
[2025-05-02T02:39:10.295+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-api;0.6.0-incubating in central
[2025-05-02T02:39:10.310+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-zookeeper;0.6.0-incubating in central
[2025-05-02T02:39:10.331+0000] {spark_submit.py:649} INFO - found org.apache.twill#twill-discovery-core;0.6.0-incubating in central
[2025-05-02T02:39:10.350+0000] {spark_submit.py:649} INFO - found co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 in central
[2025-05-02T02:39:10.370+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-exec;3.1.3 in central
[2025-05-02T02:39:10.389+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-tez;3.1.3 in central
[2025-05-02T02:39:10.411+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-client;3.1.3 in central
[2025-05-02T02:39:10.430+0000] {spark_submit.py:649} INFO - found org.apache.hive#hive-llap-common;3.1.3 in central
[2025-05-02T02:39:10.489+0000] {spark_submit.py:649} INFO - found org.antlr#ST4;4.0.4 in central
[2025-05-02T02:39:10.514+0000] {spark_submit.py:649} INFO - found org.apache.ivy#ivy;2.4.0 in central
[2025-05-02T02:39:10.535+0000] {spark_submit.py:649} INFO - found org.codehaus.groovy#groovy-all;2.4.11 in central
[2025-05-02T02:39:10.562+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-core;1.16.0 in central
[2025-05-02T02:39:10.579+0000] {spark_submit.py:649} INFO - found org.apache.calcite#calcite-linq4j;1.16.0 in central
[2025-05-02T02:39:10.603+0000] {spark_submit.py:649} INFO - found com.esri.geometry#esri-geometry-api;2.0.0 in central
[2025-05-02T02:39:10.616+0000] {spark_submit.py:649} INFO - found com.google.code.findbugs#jsr305;3.0.1 in central
[2025-05-02T02:39:10.629+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#sketches-core;0.9.0 in central
[2025-05-02T02:39:10.642+0000] {spark_submit.py:649} INFO - found com.yahoo.datasketches#memory;0.9.0 in central
[2025-05-02T02:39:10.659+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#janino;2.7.6 in central
[2025-05-02T02:39:10.673+0000] {spark_submit.py:649} INFO - found org.codehaus.janino#commons-compiler;2.7.6 in central
[2025-05-02T02:39:03.330+0000] {spark_submit.py:649} INFO - found org.apache.calcite.avatica#avatica;1.11.0 in central
[2025-05-02T02:39:03.347+0000] {spark_submit.py:649} INFO - found stax#stax-api;1.0.1 in central
[2025-05-02T02:39:03.366+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-05-02T02:39:03.374+0000] {spark_submit.py:649} INFO - found org.xerial.snappy#snappy-java;1.1.8.2 in central
[2025-05-02T02:39:03.389+0000] {spark_submit.py:649} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-05-02T02:39:03.397+0000] {spark_submit.py:649} INFO - found org.slf4j#slf4j-api;1.7.36 in central
[2025-05-02T02:39:03.562+0000] {spark_submit.py:649} INFO - :: resolution report :: resolve -1692ms :: artifacts dl 123ms
[2025-05-02T02:39:03.563+0000] {spark_submit.py:649} INFO - :: modules in use:
[2025-05-02T02:39:03.564+0000] {spark_submit.py:649} INFO - aopalliance#aopalliance;1.0 from central in [default]
[2025-05-02T02:39:03.565+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-api;0.6.0 from central in [default]
[2025-05-02T02:39:03.565+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-core;0.6.0 from central in [default]
[2025-05-02T02:39:03.566+0000] {spark_submit.py:649} INFO - co.cask.tephra#tephra-hbase-compat-1.0;0.6.0 from central in [default]
[2025-05-02T02:39:03.567+0000] {spark_submit.py:649} INFO - com.carrotsearch#hppc;0.7.2 from central in [default]
[2025-05-02T02:39:03.568+0000] {spark_submit.py:649} INFO - com.cedarsoftware#java-util;1.9.0 from central in [default]
[2025-05-02T02:39:03.569+0000] {spark_submit.py:649} INFO - com.cedarsoftware#json-io;2.5.1 from central in [default]
[2025-05-02T02:39:03.570+0000] {spark_submit.py:649} INFO - com.esri.geometry#esri-geometry-api;2.0.0 from central in [default]
[2025-05-02T02:39:03.570+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-annotations;2.12.0 from central in [default]
[2025-05-02T02:39:03.571+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-core;2.12.0 from central in [default]
[2025-05-02T02:39:03.572+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.core#jackson-databind;2.12.0 from central in [default]
[2025-05-02T02:39:03.573+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.0 from central in [default]
[2025-05-02T02:39:03.574+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.0 from central in [default]
[2025-05-02T02:39:03.575+0000] {spark_submit.py:649} INFO - com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.0 from central in [default]
[2025-05-02T02:39:03.576+0000] {spark_submit.py:649} INFO - com.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]
[2025-05-02T02:39:03.576+0000] {spark_submit.py:649} INFO - com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from central in [default]
[2025-05-02T02:39:03.577+0000] {spark_submit.py:649} INFO - com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2025-05-02T02:39:03.578+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.1 from central in [default]
[2025-05-02T02:39:03.579+0000] {spark_submit.py:649} INFO - com.google.code.gson#gson;2.2.4 from central in [default]
[2025-05-02T02:39:03.580+0000] {spark_submit.py:649} INFO - com.google.guava#guava;19.0 from central in [default]
[2025-05-02T02:39:03.581+0000] {spark_submit.py:649} INFO - com.google.inject#guice;4.0 from central in [default]
[2025-05-02T02:39:13.666+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-assistedinject;3.0 from central in [default]
[2025-05-02T02:39:13.667+0000] {spark_submit.py:649} INFO - com.google.inject.extensions#guice-servlet;4.0 from central in [default]
[2025-05-02T02:39:13.668+0000] {spark_submit.py:649} INFO - com.google.protobuf#protobuf-java;2.5.0 from central in [default]
[2025-05-02T02:39:13.669+0000] {spark_submit.py:649} INFO - com.google.re2j#re2j;1.1 from central in [default]
[2025-05-02T02:39:13.670+0000] {spark_submit.py:649} INFO - com.jcraft#jsch;0.1.54 from central in [default]
[2025-05-02T02:39:13.670+0000] {spark_submit.py:649} INFO - com.jolbox#bonecp;0.8.0.RELEASE from central in [default]
[2025-05-02T02:39:13.671+0000] {spark_submit.py:649} INFO - com.microsoft.sqlserver#mssql-jdbc;6.2.1.jre7 from central in [default]
[2025-05-02T02:39:13.672+0000] {spark_submit.py:649} INFO - com.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]
[2025-05-02T02:39:13.673+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-client;1.19 from central in [default]
[2025-05-02T02:39:13.674+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-core;1.19 from central in [default]
[2025-05-02T02:39:13.675+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-json;1.19 from central in [default]
[2025-05-02T02:39:13.676+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-server;1.19 from central in [default]
[2025-05-02T02:39:13.676+0000] {spark_submit.py:649} INFO - com.sun.jersey#jersey-servlet;1.19 from central in [default]
[2025-05-02T02:39:13.677+0000] {spark_submit.py:649} INFO - com.sun.jersey.contribs#jersey-guice;1.19 from central in [default]
[2025-05-02T02:39:13.678+0000] {spark_submit.py:649} INFO - com.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]
[2025-05-02T02:39:13.679+0000] {spark_submit.py:649} INFO - com.tdunning#json;1.8 from central in [default]
[2025-05-02T02:39:13.680+0000] {spark_submit.py:649} INFO - com.thoughtworks.paranamer#paranamer;2.7 from central in [default]
[2025-05-02T02:39:13.681+0000] {spark_submit.py:649} INFO - com.vlkan#flatbuffers;1.2.0-3f79e055 from central in [default]
[2025-05-02T02:39:13.682+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#memory;0.9.0 from central in [default]
[2025-05-02T02:39:13.682+0000] {spark_submit.py:649} INFO - com.yahoo.datasketches#sketches-core;0.9.0 from central in [default]
[2025-05-02T02:39:13.683+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP;2.6.1 from central in [default]
[2025-05-02T02:39:13.684+0000] {spark_submit.py:649} INFO - com.zaxxer#HikariCP-java7;2.4.12 from central in [default]
[2025-05-02T02:39:13.685+0000] {spark_submit.py:649} INFO - commons-beanutils#commons-beanutils;1.9.3 from central in [default]
[2025-05-02T02:39:13.686+0000] {spark_submit.py:649} INFO - commons-cli#commons-cli;1.2 from central in [default]
[2025-05-02T02:39:13.687+0000] {spark_submit.py:649} INFO - commons-codec#commons-codec;1.15 from central in [default]
[2025-05-02T02:39:13.688+0000] {spark_submit.py:649} INFO - commons-collections#commons-collections;3.2.2 from central in [default]
[2025-05-02T02:39:13.689+0000] {spark_submit.py:649} INFO - commons-daemon#commons-daemon;1.0.13 from central in [default]
[2025-05-02T02:39:13.690+0000] {spark_submit.py:649} INFO - commons-dbcp#commons-dbcp;1.4 from central in [default]
[2025-05-02T02:39:13.690+0000] {spark_submit.py:649} INFO - commons-io#commons-io;2.6 from central in [default]
[2025-05-02T02:39:13.691+0000] {spark_submit.py:649} INFO - commons-lang#commons-lang;2.6 from central in [default]
[2025-05-02T02:39:13.692+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.2 from central in [default]
[2025-05-02T02:39:13.693+0000] {spark_submit.py:649} INFO - commons-net#commons-net;3.6 from central in [default]
[2025-05-02T02:39:13.694+0000] {spark_submit.py:649} INFO - commons-pool#commons-pool;1.5.4 from central in [default]
[2025-05-02T02:39:13.694+0000] {spark_submit.py:649} INFO - de.ruedigermoeller#fst;2.50 from central in [default]
[2025-05-02T02:39:13.695+0000] {spark_submit.py:649} INFO - dnsjava#dnsjava;2.1.7 from central in [default]
[2025-05-02T02:39:13.696+0000] {spark_submit.py:649} INFO - io.airlift#aircompressor;0.10 from central in [default]
[2025-05-02T02:39:13.697+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.2.1 from central in [default]
[2025-05-02T02:39:13.698+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-json;3.1.0 from central in [default]
[2025-05-02T02:39:13.699+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-jvm;3.1.0 from central in [default]
[2025-05-02T02:39:13.699+0000] {spark_submit.py:649} INFO - io.netty#netty;3.7.0.Final from central in [default]
[2025-05-02T02:39:13.700+0000] {spark_submit.py:649} INFO - io.netty#netty-buffer;4.1.17.Final from central in [default]
[2025-05-02T02:39:13.701+0000] {spark_submit.py:649} INFO - io.netty#netty-common;4.1.17.Final from central in [default]
[2025-05-02T02:39:13.702+0000] {spark_submit.py:649} INFO - it.unimi.dsi#fastutil;6.5.6 from central in [default]
[2025-05-02T02:39:13.703+0000] {spark_submit.py:649} INFO - jakarta.activation#jakarta.activation-api;1.2.1 from central in [default]
[2025-05-02T02:39:13.703+0000] {spark_submit.py:649} INFO - jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]
[2025-05-02T02:39:13.704+0000] {spark_submit.py:649} INFO - javax.inject#javax.inject;1 from central in [default]
[2025-05-02T02:39:13.705+0000] {spark_submit.py:649} INFO - javax.jdo#jdo-api;3.0.1 from central in [default]
[2025-05-02T02:39:13.706+0000] {spark_submit.py:649} INFO - javax.servlet#javax.servlet-api;3.1.0 from central in [default]
[2025-05-02T02:39:13.707+0000] {spark_submit.py:649} INFO - javax.servlet.jsp#jsp-api;2.1 from central in [default]
[2025-05-02T02:39:13.708+0000] {spark_submit.py:649} INFO - javax.transaction#jta;1.1 from central in [default]
[2025-05-02T02:39:13.708+0000] {spark_submit.py:649} INFO - javax.transaction#transaction-api;1.1 from central in [default]
[2025-05-02T02:39:13.709+0000] {spark_submit.py:649} INFO - javax.ws.rs#jsr311-api;1.1.1 from central in [default]
[2025-05-02T02:39:13.710+0000] {spark_submit.py:649} INFO - javax.xml.bind#jaxb-api;2.2.11 from central in [default]
[2025-05-02T02:39:13.711+0000] {spark_submit.py:649} INFO - javolution#javolution;5.5.1 from central in [default]
[2025-05-02T02:39:13.711+0000] {spark_submit.py:649} INFO - jline#jline;2.12 from central in [default]
[2025-05-02T02:39:13.712+0000] {spark_submit.py:649} INFO - joda-time#joda-time;2.9.9 from central in [default]
[2025-05-02T02:39:13.713+0000] {spark_submit.py:649} INFO - junit#junit;4.11 from central in [default]
[2025-05-02T02:39:13.714+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.17 from central in [default]
[2025-05-02T02:39:13.714+0000] {spark_submit.py:649} INFO - net.minidev#accessors-smart;1.2 from central in [default]
[2025-05-02T02:39:13.715+0000] {spark_submit.py:649} INFO - net.minidev#json-smart;2.3 from central in [default]
[2025-05-02T02:39:13.716+0000] {spark_submit.py:649} INFO - net.sf.jpam#jpam;1.1 from central in [default]
[2025-05-02T02:39:13.717+0000] {spark_submit.py:649} INFO - net.sf.opencsv#opencsv;2.3 from central in [default]
[2025-05-02T02:39:13.717+0000] {spark_submit.py:649} INFO - org.antlr#ST4;4.0.4 from central in [default]
[2025-05-02T02:39:13.718+0000] {spark_submit.py:649} INFO - org.antlr#antlr-runtime;3.5.2 from central in [default]
[2025-05-02T02:39:13.719+0000] {spark_submit.py:649} INFO - org.apache.ant#ant;1.9.1 from central in [default]
[2025-05-02T02:39:13.719+0000] {spark_submit.py:649} INFO - org.apache.ant#ant-launcher;1.9.1 from central in [default]
[2025-05-02T02:39:13.720+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-format;0.8.0 from central in [default]
[2025-05-02T02:39:13.721+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-memory;0.8.0 from central in [default]
[2025-05-02T02:39:13.721+0000] {spark_submit.py:649} INFO - org.apache.arrow#arrow-vector;0.8.0 from central in [default]
[2025-05-02T02:39:13.722+0000] {spark_submit.py:649} INFO - org.apache.avro#avro;1.8.2 from central in [default]
[2025-05-02T02:39:13.723+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-core;1.16.0 from central in [default]
[2025-05-02T02:39:13.724+0000] {spark_submit.py:649} INFO - org.apache.calcite#calcite-linq4j;1.16.0 from central in [default]
[2025-05-02T02:39:13.725+0000] {spark_submit.py:649} INFO - org.apache.calcite.avatica#avatica;1.11.0 from central in [default]
[2025-05-02T02:39:13.725+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-compress;1.19 from central in [default]
[2025-05-02T02:39:13.726+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-configuration2;2.1.1 from central in [default]
[2025-05-02T02:39:13.727+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-crypto;1.0.0 from central in [default]
[2025-05-02T02:39:13.727+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.9 from central in [default]
[2025-05-02T02:39:13.728+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-math3;3.1.1 from central in [default]
[2025-05-02T02:39:13.729+0000] {spark_submit.py:649} INFO - org.apache.derby#derby;10.14.1.0 from central in [default]
[2025-05-02T02:39:13.730+0000] {spark_submit.py:649} INFO - org.apache.geronimo.specs#geronimo-jcache_1.0_spec;1.0-alpha-1 from central in [default]
[2025-05-02T02:39:13.730+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-annotations;3.1.0 from central in [default]
[2025-05-02T02:39:13.731+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-auth;3.1.0 from central in [default]
[2025-05-02T02:39:13.731+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-05-02T02:39:13.732+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-05-02T02:39:13.733+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-common;3.1.0 from central in [default]
[2025-05-02T02:39:13.733+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-api;3.1.0 from central in [default]
[2025-05-02T02:39:13.734+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-common;3.1.0 from central in [default]
[2025-05-02T02:39:13.735+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-registry;3.1.0 from central in [default]
[2025-05-02T02:39:13.735+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-applicationhistoryservice;3.1.0 from central in [default]
[2025-05-02T02:39:13.736+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-common;3.1.0 from central in [default]
[2025-05-02T02:39:13.737+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-resourcemanager;3.1.0 from central in [default]
[2025-05-02T02:39:13.737+0000] {spark_submit.py:649} INFO - org.apache.hadoop#hadoop-yarn-server-web-proxy;3.1.0 from central in [default]
[2025-05-02T02:39:13.738+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-client;2.0.0-alpha4 from central in [default]
[2025-05-02T02:39:13.738+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol;2.0.0-alpha4 from central in [default]
[2025-05-02T02:39:13.739+0000] {spark_submit.py:649} INFO - org.apache.hbase#hbase-protocol-shaded;2.0.0-alpha4 from central in [default]
[2025-05-02T02:39:13.740+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-miscellaneous;1.0.1 from central in [default]
[2025-05-02T02:39:13.740+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-netty;1.0.1 from central in [default]
[2025-05-02T02:39:13.741+0000] {spark_submit.py:649} INFO - org.apache.hbase.thirdparty#hbase-shaded-protobuf;1.0.1 from central in [default]
[2025-05-02T02:39:13.741+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-classification;3.1.3 from central in [default]
[2025-05-02T02:39:13.742+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-common;3.1.3 from central in [default]
[2025-05-02T02:39:13.743+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-exec;3.1.3 from central in [default]
[2025-05-02T02:39:13.743+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-client;3.1.3 from central in [default]
[2025-05-02T02:39:13.744+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-common;3.1.3 from central in [default]
[2025-05-02T02:39:13.744+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-llap-tez;3.1.3 from central in [default]
[2025-05-02T02:39:13.745+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-metastore;3.1.3 from central in [default]
[2025-05-02T02:39:13.745+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-serde;3.1.3 from central in [default]
[2025-05-02T02:39:13.746+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-service-rpc;3.1.3 from central in [default]
[2025-05-02T02:39:13.746+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-shims;3.1.3 from central in [default]
[2025-05-02T02:39:13.747+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-standalone-metastore;3.1.3 from central in [default]
[2025-05-02T02:39:13.748+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-storage-api;2.7.0 from central in [default]
[2025-05-02T02:39:13.748+0000] {spark_submit.py:649} INFO - org.apache.hive#hive-upgrade-acid;3.1.3 from central in [default]
[2025-05-02T02:39:13.749+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-0.23;3.1.3 from central in [default]
[2025-05-02T02:39:13.749+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-common;3.1.3 from central in [default]
[2025-05-02T02:39:13.750+0000] {spark_submit.py:649} INFO - org.apache.hive.shims#hive-shims-scheduler;3.1.3 from central in [default]
[2025-05-02T02:39:13.750+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core;3.2.0-incubating from central in [default]
[2025-05-02T02:39:13.751+0000] {spark_submit.py:649} INFO - org.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]
[2025-05-02T02:39:13.751+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpclient;4.5.13 from central in [default]
[2025-05-02T02:39:13.752+0000] {spark_submit.py:649} INFO - org.apache.httpcomponents#httpcore;4.4.13 from central in [default]
[2025-05-02T02:39:13.752+0000] {spark_submit.py:649} INFO - org.apache.ivy#ivy;2.4.0 from central in [default]
[2025-05-02T02:39:13.753+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-admin;1.0.1 from central in [default]
[2025-05-02T02:39:13.754+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-client;1.0.1 from central in [default]
[2025-05-02T02:39:13.754+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-common;1.0.1 from central in [default]
[2025-05-02T02:39:13.755+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-core;1.0.1 from central in [default]
[2025-05-02T02:39:13.755+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-crypto;1.0.1 from central in [default]
[2025-05-02T02:39:13.756+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-identity;1.0.1 from central in [default]
[2025-05-02T02:39:13.756+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-server;1.0.1 from central in [default]
[2025-05-02T02:39:13.757+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]
[2025-05-02T02:39:13.757+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerb-util;1.0.1 from central in [default]
[2025-05-02T02:39:13.758+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-asn1;1.0.1 from central in [default]
[2025-05-02T02:39:13.759+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-config;1.0.1 from central in [default]
[2025-05-02T02:39:13.759+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-pkix;1.0.1 from central in [default]
[2025-05-02T02:39:13.760+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-util;1.0.1 from central in [default]
[2025-05-02T02:39:13.760+0000] {spark_submit.py:649} INFO - org.apache.kerby#kerby-xdr;1.0.1 from central in [default]
[2025-05-02T02:39:13.761+0000] {spark_submit.py:649} INFO - org.apache.kerby#token-provider;1.0.1 from central in [default]
[2025-05-02T02:39:13.761+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-1.2-api;2.17.1 from central in [default]
[2025-05-02T02:39:13.762+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-api;2.10.0 from central in [default]
[2025-05-02T02:39:13.762+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-core;2.17.1 from central in [default]
[2025-05-02T02:39:13.763+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-slf4j-impl;2.17.1 from central in [default]
[2025-05-02T02:39:13.763+0000] {spark_submit.py:649} INFO - org.apache.logging.log4j#log4j-web;2.17.1 from central in [default]
[2025-05-02T02:39:13.764+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-core;1.5.8 from central in [default]
[2025-05-02T02:39:13.765+0000] {spark_submit.py:649} INFO - org.apache.orc#orc-shims;1.5.8 from central in [default]
[2025-05-02T02:39:13.766+0000] {spark_submit.py:649} INFO - org.apache.parquet#parquet-hadoop-bundle;1.10.0 from central in [default]
[2025-05-02T02:39:13.766+0000] {spark_submit.py:649} INFO - org.apache.thrift#libfb303;0.9.3 from central in [default]
[2025-05-02T02:39:13.767+0000] {spark_submit.py:649} INFO - org.apache.thrift#libthrift;0.9.3 from central in [default]
[2025-05-02T02:39:13.768+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-api;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.768+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-common;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.769+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-core;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.770+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-api;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.771+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-discovery-core;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.771+0000] {spark_submit.py:649} INFO - org.apache.twill#twill-zookeeper;0.6.0-incubating from central in [default]
[2025-05-02T02:39:13.772+0000] {spark_submit.py:649} INFO - org.apache.yetus#audience-annotations;0.5.0 from central in [default]
[2025-05-02T02:39:13.773+0000] {spark_submit.py:649} INFO - org.apache.zookeeper#zookeeper;3.4.6 from central in [default]
[2025-05-02T02:39:13.773+0000] {spark_submit.py:649} INFO - org.codehaus.groovy#groovy-all;2.4.11 from central in [default]
[2025-05-02T02:39:13.774+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
[2025-05-02T02:39:13.775+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]
[2025-05-02T02:39:13.776+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
[2025-05-02T02:39:13.776+0000] {spark_submit.py:649} INFO - org.codehaus.jackson#jackson-xc;1.9.13 from central in [default]
[2025-05-02T02:39:13.777+0000] {spark_submit.py:649} INFO - org.codehaus.janino#commons-compiler;2.7.6 from central in [default]
[2025-05-02T02:39:13.778+0000] {spark_submit.py:649} INFO - org.codehaus.janino#janino;2.7.6 from central in [default]
[2025-05-02T02:39:13.779+0000] {spark_submit.py:649} INFO - org.codehaus.jettison#jettison;1.1 from central in [default]
[2025-05-02T02:39:13.779+0000] {spark_submit.py:649} INFO - org.codehaus.woodstox#stax2-api;3.1.4 from central in [default]
[2025-05-02T02:39:13.780+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]
[2025-05-02T02:39:13.781+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-core;4.1.17 from central in [default]
[2025-05-02T02:39:13.782+0000] {spark_submit.py:649} INFO - org.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]
[2025-05-02T02:39:13.783+0000] {spark_submit.py:649} INFO - org.datanucleus#javax.jdo;3.2.0-m3 from central in [default]
[2025-05-02T02:39:13.783+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-client;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.784+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-http;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.785+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-io;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.786+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-rewrite;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.787+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-security;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.787+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-server;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.788+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-servlet;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.789+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util;9.3.19.v20170502 from central in [default]
[2025-05-02T02:39:13.790+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-util-ajax;9.3.19.v20170502 from central in [default]
[2025-05-02T02:39:13.790+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-webapp;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.791+0000] {spark_submit.py:649} INFO - org.eclipse.jetty#jetty-xml;9.3.20.v20170531 from central in [default]
[2025-05-02T02:39:13.792+0000] {spark_submit.py:649} INFO - org.ehcache#ehcache;3.3.1 from central in [default]
[2025-05-02T02:39:13.793+0000] {spark_submit.py:649} INFO - org.fusesource.leveldbjni#leveldbjni-all;1.8 from central in [default]
[2025-05-02T02:39:13.793+0000] {spark_submit.py:649} INFO - org.hamcrest#hamcrest-core;1.3 from central in [default]
[2025-05-02T02:39:13.794+0000] {spark_submit.py:649} INFO - org.jruby.jcodings#jcodings;1.0.18 from central in [default]
[2025-05-02T02:39:13.795+0000] {spark_submit.py:649} INFO - org.jruby.joni#joni;2.1.11 from central in [default]
[2025-05-02T02:39:13.796+0000] {spark_submit.py:649} INFO - org.ow2.asm#asm;5.0.4 from central in [default]
[2025-05-02T02:39:13.797+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.36 from central in [default]
[2025-05-02T02:39:13.797+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-log4j12;1.6.1 from central in [default]
[2025-05-02T02:39:13.798+0000] {spark_submit.py:649} INFO - org.tukaani#xz;1.5 from central in [default]
[2025-05-02T02:39:13.799+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
[2025-05-02T02:39:13.800+0000] {spark_submit.py:649} INFO - sqlline#sqlline;1.3.0 from central in [default]
[2025-05-02T02:39:13.800+0000] {spark_submit.py:649} INFO - stax#stax-api;1.0.1 from central in [default]
[2025-05-02T02:39:13.801+0000] {spark_submit.py:649} INFO - :: evicted modules:
[2025-05-02T02:39:13.802+0000] {spark_submit.py:649} INFO - org.slf4j#slf4j-api;1.7.10 by [org.slf4j#slf4j-api;1.7.36] in [default]
[2025-05-02T02:39:13.803+0000] {spark_submit.py:649} INFO - log4j#log4j;1.2.16 by [log4j#log4j;1.2.17] in [default]
[2025-05-02T02:39:13.804+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-02T02:39:13.805+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.4 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T02:39:13.805+0000] {spark_submit.py:649} INFO - org.xerial.snappy#snappy-java;1.1.4 by [org.xerial.snappy#snappy-java;1.1.8.2] in [default]
[2025-05-02T02:39:13.806+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-02T02:39:13.807+0000] {spark_submit.py:649} INFO - commons-logging#commons-logging;1.0.4 by [commons-logging#commons-logging;1.2] in [default]
[2025-05-02T02:39:13.807+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.0 by [io.dropwizard.metrics#metrics-core;3.2.1] in [default]
[2025-05-02T02:39:13.808+0000] {spark_submit.py:649} INFO - io.dropwizard.metrics#metrics-core;3.1.2 by [io.dropwizard.metrics#metrics-core;3.1.0] in [default]
[2025-05-02T02:39:13.809+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;3.0.2 by [com.google.code.findbugs#jsr305;3.0.1] in [default]
[2025-05-02T02:39:13.810+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.2 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T02:39:13.811+0000] {spark_submit.py:649} INFO - org.apache.commons#commons-lang3;3.6 by [org.apache.commons#commons-lang3;3.9] in [default]
[2025-05-02T02:39:13.811+0000] {spark_submit.py:649} INFO - com.google.inject#guice;3.0 by [com.google.inject#guice;4.0] in [default]
[2025-05-02T02:39:13.812+0000] {spark_submit.py:649} INFO - com.google.code.findbugs#jsr305;2.0.1 by [com.google.code.findbugs#jsr305;3.0.0] in [default]
[2025-05-02T02:39:13.813+0000] {spark_submit.py:649} INFO - com.google.guava#guava;14.0.1 by [com.google.guava#guava;19.0] in [default]
[2025-05-02T02:39:13.814+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:39:13.814+0000] {spark_submit.py:649} INFO - |                  |            modules            ||   artifacts   |
[2025-05-02T02:39:13.815+0000] {spark_submit.py:649} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-05-02T02:39:13.816+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:39:13.817+0000] {spark_submit.py:649} INFO - |      default     |  224  |   0   |   0   |   15  ||  209  |   0   |
[2025-05-02T02:39:13.818+0000] {spark_submit.py:649} INFO - ---------------------------------------------------------------------
[2025-05-02T02:39:13.819+0000] {spark_submit.py:649} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-2f5dfa83-a718-4453-983e-d7e536d9f3f9
[2025-05-02T02:39:13.819+0000] {spark_submit.py:649} INFO - confs: [default]
[2025-05-02T02:39:13.820+0000] {spark_submit.py:649} INFO - 0 artifacts copied, 209 already retrieved (0kB/41ms)
[2025-05-02T02:39:14.730+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:14 INFO IsolatedClientLoader: Downloaded metastore jars to /tmp/hive-v3_1-9c16e7a2-2b14-4775-a2f6-50caf05bb4a2
[2025-05-02T02:39:15.339+0000] {spark_submit.py:649} INFO - Hive Session ID = 8d508dd1-9c90-4fff-95f5-84f98da63057
[2025-05-02T02:39:15.420+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:15 INFO HiveClientImpl: Warehouse location for Hive client (version 3.1.3) is file:/opt/***/spark-warehouse
[2025-05-02T02:39:16.346+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-05-02T02:39:16.367+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:16 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-05-02T02:39:16.368+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:16 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-05-02T02:39:18.045+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:18 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
[2025-05-02T02:39:18.282+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:18 INFO DeltaLog: Loading version 0.
[2025-05-02T02:39:19.141+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:19 INFO Snapshot: [tableId=19c3422f-a667-4c0a-8574-766f3e8225b7] Created snapshot Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log, version=0, metadata=Metadata(1fd8cabc-a73f-4a24-b44b-55dde91b87a3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"ORGANIZATION","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"GENDER","type":"string","nullable":true,"metadata":{}},{"name":"SPECIALITY","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"ENCOUNTERS","type":"long","nullable":true,"metadata":{}},{"name":"PROCEDURES","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147368747)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log/00000000000000000000.json; isDirectory=false; length=3040; replication=1; blocksize=33554432; modification_time=1746147382941; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=fc7d453c91a7660f73c26eec9ae00381 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746147382941), checksumOpt=Some(VersionChecksum(Some(115709c2-d82c-4cd7-b793-7919933db90c),82470,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(1fd8cabc-a73f-4a24-b44b-55dde91b87a3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"ORGANIZATION","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"GENDER","type":"string","nullable":true,"metadata":{}},{"name":"SPECIALITY","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"ENCOUNTERS","type":"long","nullable":true,"metadata":{}},{"name":"PROCEDURES","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147368747)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-7b4eda75-b20f-405f-a422-b40cd8cf6935-c000.snappy.parquet,Map(),82470,1746147380000,false,{"numRecords":272,"minValues":{"ID":"00b1a913-31e7-3941-8f26-9a07f04e","ORGANIZATION":"00ffb204-07b4-3c72-a678-fa252e6c","NAME":"Agnes294 Dooley940","GENDER":"F","SPECIALITY":"GENERAL PRACTICE","ADDRESS":"1 EDGEWATER DRIVE  SUITE 103","CITY":"ABINGTON","STATE":"MA","ZIP":1301,"LAT":41.44720825,"LON":-73.25677853480586,"ENCOUNTERS":1,"PROCEDURES":0,"HASH":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"fffdc2e7-c175-3e01-a2da-185da48c","ORGANIZATION":"fe30f2b4-9bc8-346e-afba-4455d176","NAME":"Zane918 Bosco882","GENDER":"M","SPECIALITY":"GENERAL PRACTICE","ADDRESS":"ONE HOSPITAL ROAD  FIRST FL  WIN","CITY":"Worcester","STATE":"MA","ZIP":27803561,"LAT":42.797835,"LON":-70.06169919459835,"ENCOUNTERS":2063,"PROCEDURES":0,"HASH":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"ORGANIZATION":0,"NAME":0,"GENDER":0,"SPECIALITY":0,"ADDRESS":0,"CITY":0,"STATE":0,"ZIP":0,"LAT":0,"LON":0,"ENCOUNTERS":0,"PROCEDURES":0,"HASH":0}},null,null,None,None,None))))))
[2025-05-02T02:39:19.188+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:19 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log, version=0, metadata=Metadata(1fd8cabc-a73f-4a24-b44b-55dde91b87a3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"ORGANIZATION","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"GENDER","type":"string","nullable":true,"metadata":{}},{"name":"SPECIALITY","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"ENCOUNTERS","type":"long","nullable":true,"metadata":{}},{"name":"PROCEDURES","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147368747)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log/00000000000000000000.json; isDirectory=false; length=3040; replication=1; blocksize=33554432; modification_time=1746147382941; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=fc7d453c91a7660f73c26eec9ae00381 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746147382941), checksumOpt=Some(VersionChecksum(Some(115709c2-d82c-4cd7-b793-7919933db90c),82470,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(1fd8cabc-a73f-4a24-b44b-55dde91b87a3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"ORGANIZATION","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"GENDER","type":"string","nullable":true,"metadata":{}},{"name":"SPECIALITY","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"ENCOUNTERS","type":"long","nullable":true,"metadata":{}},{"name":"PROCEDURES","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147368747)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-7b4eda75-b20f-405f-a422-b40cd8cf6935-c000.snappy.parquet,Map(),82470,1746147380000,false,{"numRecords":272,"minValues":{"ID":"00b1a913-31e7-3941-8f26-9a07f04e","ORGANIZATION":"00ffb204-07b4-3c72-a678-fa252e6c","NAME":"Agnes294 Dooley940","GENDER":"F","SPECIALITY":"GENERAL PRACTICE","ADDRESS":"1 EDGEWATER DRIVE  SUITE 103","CITY":"ABINGTON","STATE":"MA","ZIP":1301,"LAT":41.44720825,"LON":-73.25677853480586,"ENCOUNTERS":1,"PROCEDURES":0,"HASH":"00b1a913-31e7-3941-8f26-9a07f04e"},"maxValues":{"ID":"fffdc2e7-c175-3e01-a2da-185da48c","ORGANIZATION":"fe30f2b4-9bc8-346e-afba-4455d176","NAME":"Zane918 Bosco882","GENDER":"M","SPECIALITY":"GENERAL PRACTICE","ADDRESS":"ONE HOSPITAL ROAD  FIRST FL  WIN","CITY":"Worcester","STATE":"MA","ZIP":27803561,"LAT":42.797835,"LON":-70.06169919459835,"ENCOUNTERS":2063,"PROCEDURES":0,"HASH":"fffdc2e7-c175-3e01-a2da-185da48c"},"nullCount":{"ID":0,"ORGANIZATION":0,"NAME":0,"GENDER":0,"SPECIALITY":0,"ADDRESS":0,"CITY":0,"STATE":0,"ZIP":0,"LAT":0,"LON":0,"ENCOUNTERS":0,"PROCEDURES":0,"HASH":0}},null,null,None,None,None))))))
[2025-05-02T02:39:19.973+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:19 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
[2025-05-02T02:39:20.085+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:20 INFO DeltaLog: Loading version 0.
[2025-05-02T02:39:20.117+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:20 INFO Snapshot: [tableId=32683445-71d9-48fb-b114-0293b592edbe] Created snapshot Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log, version=0, metadata=Metadata(17809698-dced-4ce6-9d17-8a37132ddc9e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"PHONE","type":"string","nullable":true,"metadata":{}},{"name":"REVENUE","type":"double","nullable":true,"metadata":{}},{"name":"UTILIZATION","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147062098)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log/00000000000000000000.json; isDirectory=false; length=2708; replication=1; blocksize=33554432; modification_time=1746147073819; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=f0eaba00ed337111b729f6b62dbb152c versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746147073819), checksumOpt=Some(VersionChecksum(Some(63348bb8-dc2f-4ffc-9543-af397664a7ac),65421,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(17809698-dced-4ce6-9d17-8a37132ddc9e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"PHONE","type":"string","nullable":true,"metadata":{}},{"name":"REVENUE","type":"double","nullable":true,"metadata":{}},{"name":"UTILIZATION","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147062098)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-a823e957-b8d1-43e7-9534-9cb7d1c56692-c000.snappy.parquet,Map(),65421,1746147070000,false,{"numRecords":272,"minValues":{"ID":"00ffb204-07b4-3c72-a678-fa252e6c","NAME":"1200 SUFFIELD STREET OPERATOR LL","ADDRESS":"1 EDGEWATER DRIVE  SUITE 103","CITY":"ABINGTON","STATE":"MA","ZIP":1301,"LAT":41.44720825,"LON":-73.25677853480586,"PHONE":"(413) 789-2200","REVENUE":0.0,"UTILIZATION":1,"HASH":"00ffb204-07b4-3c72-a678-fa252e6c"},"maxValues":{"ID":"fe30f2b4-9bc8-346e-afba-4455d176","NAME":"ZANJABEE INTEGRATIVE MEDICINE AN","ADDRESS":"ONE HOSPITAL ROAD  FIRST FL  WIN","CITY":"Worcester","STATE":"MA","ZIP":27803561,"LAT":42.797835,"LON":-70.06169919459835,"PHONE":"9789832435","REVENUE":0.0,"UTILIZATION":2063,"HASH":"fe30f2b4-9bc8-346e-afba-4455d176"},"nullCount":{"ID":0,"NAME":0,"ADDRESS":0,"CITY":0,"STATE":0,"ZIP":0,"LAT":0,"LON":0,"PHONE":0,"REVENUE":0,"UTILIZATION":0,"HASH":0}},null,null,None,None,None))))))
[2025-05-02T02:39:20.120+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:20 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log, version=0, metadata=Metadata(17809698-dced-4ce6-9d17-8a37132ddc9e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"PHONE","type":"string","nullable":true,"metadata":{}},{"name":"REVENUE","type":"double","nullable":true,"metadata":{}},{"name":"UTILIZATION","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147062098)), logSegment=LogSegment(s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log/00000000000000000000.json; isDirectory=false; length=2708; replication=1; blocksize=33554432; modification_time=1746147073819; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=f0eaba00ed337111b729f6b62dbb152c versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746147073819), checksumOpt=Some(VersionChecksum(Some(63348bb8-dc2f-4ffc-9543-af397664a7ac),65421,1,None,None,1,1,None,Some(List()),Some(List()),Metadata(17809698-dced-4ce6-9d17-8a37132ddc9e,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"ID","type":"string","nullable":true,"metadata":{}},{"name":"NAME","type":"string","nullable":true,"metadata":{}},{"name":"ADDRESS","type":"string","nullable":true,"metadata":{}},{"name":"CITY","type":"string","nullable":true,"metadata":{}},{"name":"STATE","type":"string","nullable":true,"metadata":{}},{"name":"ZIP","type":"long","nullable":true,"metadata":{}},{"name":"LAT","type":"double","nullable":true,"metadata":{}},{"name":"LON","type":"double","nullable":true,"metadata":{}},{"name":"PHONE","type":"string","nullable":true,"metadata":{}},{"name":"REVENUE","type":"double","nullable":true,"metadata":{}},{"name":"UTILIZATION","type":"long","nullable":true,"metadata":{}},{"name":"HASH","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1746147062098)),Protocol(1,2),None,None,Some(List(AddFile(part-00000-a823e957-b8d1-43e7-9534-9cb7d1c56692-c000.snappy.parquet,Map(),65421,1746147070000,false,{"numRecords":272,"minValues":{"ID":"00ffb204-07b4-3c72-a678-fa252e6c","NAME":"1200 SUFFIELD STREET OPERATOR LL","ADDRESS":"1 EDGEWATER DRIVE  SUITE 103","CITY":"ABINGTON","STATE":"MA","ZIP":1301,"LAT":41.44720825,"LON":-73.25677853480586,"PHONE":"(413) 789-2200","REVENUE":0.0,"UTILIZATION":1,"HASH":"00ffb204-07b4-3c72-a678-fa252e6c"},"maxValues":{"ID":"fe30f2b4-9bc8-346e-afba-4455d176","NAME":"ZANJABEE INTEGRATIVE MEDICINE AN","ADDRESS":"ONE HOSPITAL ROAD  FIRST FL  WIN","CITY":"Worcester","STATE":"MA","ZIP":27803561,"LAT":42.797835,"LON":-70.06169919459835,"PHONE":"9789832435","REVENUE":0.0,"UTILIZATION":2063,"HASH":"fe30f2b4-9bc8-346e-afba-4455d176"},"nullCount":{"ID":0,"NAME":0,"ADDRESS":0,"CITY":0,"STATE":0,"ZIP":0,"LAT":0,"LON":0,"PHONE":0,"REVENUE":0,"UTILIZATION":0,"HASH":0}},null,null,None,None,None))))))
[2025-05-02T02:39:22.334+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:22 INFO Snapshot: DELTA: Compute snapshot for version: 0
[2025-05-02T02:39:22.472+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 204.7 KiB, free 434.2 MiB)
[2025-05-02T02:39:22.565+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 434.2 MiB)
[2025-05-02T02:39:22.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ***-scheduler:43081 (size: 35.8 KiB, free: 434.4 MiB)
[2025-05-02T02:39:22.580+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:22 INFO SparkContext: Created broadcast 0 from saveAsTable at NativeMethodAccessorImpl.java:0
[2025-05-02T02:39:23.886+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:23 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 2708)
[2025-05-02T02:39:25.108+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO DataSourceStrategy: Pruning directories with:
[2025-05-02T02:39:25.111+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T02:39:25.111+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T02:39:25.234+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2025-05-02T02:39:25.969+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO CodeGenerator: Code generated in 478.537641 ms
[2025-05-02T02:39:25.976+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 205.0 KiB, free 434.0 MiB)
[2025-05-02T02:39:25.988+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.9 MiB)
[2025-05-02T02:39:25.989+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ***-scheduler:43081 (size: 35.9 KiB, free: 434.3 MiB)
[2025-05-02T02:39:25.992+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:25 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
[2025-05-02T02:39:26.023+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197012 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T02:39:26.318+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Registering RDD 3 (saveAsTable at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-05-02T02:39:26.326+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Got map stage job 0 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-05-02T02:39:26.327+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (saveAsTable at NativeMethodAccessorImpl.java:0)
[2025-05-02T02:39:26.328+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T02:39:26.329+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:26.333+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-05-02T02:39:26.439+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 105.9 KiB, free 433.8 MiB)
[2025-05-02T02:39:26.446+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.8 MiB)
[2025-05-02T02:39:26.448+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ***-scheduler:43081 (size: 32.7 KiB, free: 434.3 MiB)
[2025-05-02T02:39:26.449+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:26.476+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:26.478+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-05-02T02:39:26.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10895 bytes)
[2025-05-02T02:39:26.592+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-05-02T02:39:26.985+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:26 INFO CodeGenerator: Code generated in 187.906315 ms
[2025-05-02T02:39:27.038+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO CodeGenerator: Code generated in 26.852367 ms
[2025-05-02T02:39:27.054+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/_delta_log/00000000000000000000.json, range: 0-2708, partition values: [0]
[2025-05-02T02:39:27.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO CodeGenerator: Code generated in 86.517457 ms
[2025-05-02T02:39:27.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO CodeGenerator: Code generated in 8.94553 ms
[2025-05-02T02:39:27.353+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO CodeGenerator: Code generated in 10.32235 ms
[2025-05-02T02:39:27.413+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1884 bytes result sent to driver
[2025-05-02T02:39:27.427+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 879 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:27.430+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-05-02T02:39:27.439+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO DAGScheduler: ShuffleMapStage 0 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 1.077 s
[2025-05-02T02:39:27.440+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T02:39:27.441+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO DAGScheduler: running: Set()
[2025-05-02T02:39:27.442+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO DAGScheduler: waiting: Set()
[2025-05-02T02:39:27.443+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO DAGScheduler: failed: Set()
[2025-05-02T02:39:27.574+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ***-scheduler:43081 in memory (size: 32.7 KiB, free: 434.3 MiB)
[2025-05-02T02:39:28.056+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-02T02:39:28.057+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO CodeGenerator: Code generated in 407.520236 ms
[2025-05-02T02:39:28.189+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO CodeGenerator: Code generated in 95.232612 ms
[2025-05-02T02:39:28.729+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO CodeGenerator: Code generated in 96.476757 ms
[2025-05-02T02:39:28.748+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Registering RDD 13 (saveAsTable at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2025-05-02T02:39:28.749+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Got map stage job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 50 output partitions
[2025-05-02T02:39:28.749+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (saveAsTable at NativeMethodAccessorImpl.java:0)
[2025-05-02T02:39:28.750+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-05-02T02:39:28.752+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:28.753+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-05-02T02:39:28.842+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 605.6 KiB, free 433.3 MiB)
[2025-05-02T02:39:28.846+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 138.8 KiB, free 433.2 MiB)
[2025-05-02T02:39:28.847+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ***-scheduler:43081 (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-02T02:39:28.848+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:28.849+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T02:39:28.850+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 50 tasks resource profile 0
[2025-05-02T02:39:28.857+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 1) (***-scheduler, executor driver, partition 42, NODE_LOCAL, 10195 bytes)
[2025-05-02T02:39:28.858+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO Executor: Running task 42.0 in stage 2.0 (TID 1)
[2025-05-02T02:39:28.966+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:28.970+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2025-05-02T02:39:29.059+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 82.550205 ms
[2025-05-02T02:39:29.083+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 14.01938 ms
[2025-05-02T02:39:29.108+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 10.280762 ms
[2025-05-02T02:39:29.362+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 119.645419 ms
[2025-05-02T02:39:29.803+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes
[2025-05-02T02:39:29.804+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 343.664775 ms
[2025-05-02T02:39:29.821+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO MemoryStore: Block rdd_10_42 stored as values in memory (estimated size 567.0 B, free 433.2 MiB)
[2025-05-02T02:39:29.822+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO BlockManagerInfo: Added rdd_10_42 in memory on ***-scheduler:43081 (size: 567.0 B, free: 434.2 MiB)
[2025-05-02T02:39:29.974+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:29 INFO CodeGenerator: Code generated in 145.891291 ms
[2025-05-02T02:39:30.014+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 7.705334 ms
[2025-05-02T02:39:30.076+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 15.141799 ms
[2025-05-02T02:39:30.085+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 5.448102 ms
[2025-05-02T02:39:30.118+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 20.809928 ms
[2025-05-02T02:39:30.144+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 11.259871 ms
[2025-05-02T02:39:30.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 7.567382 ms
[2025-05-02T02:39:30.176+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO CodeGenerator: Code generated in 7.733009 ms
[2025-05-02T02:39:30.182+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 42.0 in stage 2.0 (TID 1). 5351 bytes result sent to driver
[2025-05-02T02:39:30.184+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 2) (***-scheduler, executor driver, partition 43, NODE_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.186+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 43.0 in stage 2.0 (TID 2)
[2025-05-02T02:39:30.187+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 1) in 1331 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T02:39:30.216+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.217+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:30.272+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_43 stored as values in memory (estimated size 881.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.275+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_43 in memory on ***-scheduler:43081 (size: 881.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.318+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 43.0 in stage 2.0 (TID 2). 5308 bytes result sent to driver
[2025-05-02T02:39:30.321+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.322+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 2) in 139 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T02:39:30.323+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
[2025-05-02T02:39:30.347+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.349+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:30.398+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_0 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.400+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_0 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.442+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 5308 bytes result sent to driver
[2025-05-02T02:39:30.444+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.445+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
[2025-05-02T02:39:30.446+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 124 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T02:39:30.469+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.470+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:30.517+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_1 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.521+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_1 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 5308 bytes result sent to driver
[2025-05-02T02:39:30.560+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.561+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 118 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T02:39:30.562+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)
[2025-05-02T02:39:30.585+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.587+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:30.633+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_2 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.636+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_2 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.673+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 5308 bytes result sent to driver
[2025-05-02T02:39:30.676+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.677+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 117 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T02:39:30.678+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)
[2025-05-02T02:39:30.700+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.701+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:30.748+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_3 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.749+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_3 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 5308 bytes result sent to driver
[2025-05-02T02:39:30.788+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.789+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 113 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T02:39:30.790+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 4.0 in stage 2.0 (TID 7)
[2025-05-02T02:39:30.813+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.814+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:30.890+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO MemoryStore: Block rdd_10_4 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:30.892+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO BlockManagerInfo: Added rdd_10_4 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:30.940+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Finished task 4.0 in stage 2.0 (TID 7). 5308 bytes result sent to driver
[2025-05-02T02:39:30.943+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 8) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:30.944+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 157 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T02:39:30.945+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO Executor: Running task 5.0 in stage 2.0 (TID 8)
[2025-05-02T02:39:30.980+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:30.981+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:31.034+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_5 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.036+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_5 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.072+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 5.0 in stage 2.0 (TID 8). 5308 bytes result sent to driver
[2025-05-02T02:39:31.074+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 9) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.075+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 6.0 in stage 2.0 (TID 9)
[2025-05-02T02:39:31.076+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 8) in 134 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T02:39:31.103+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.104+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.167+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_6 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.170+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_6 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.226+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 6.0 in stage 2.0 (TID 9). 5351 bytes result sent to driver
[2025-05-02T02:39:31.228+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 10) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.229+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 7.0 in stage 2.0 (TID 10)
[2025-05-02T02:39:31.230+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 9) in 156 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T02:39:31.267+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.268+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:31.319+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_7 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.321+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_7 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.355+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 7.0 in stage 2.0 (TID 10). 5351 bytes result sent to driver
[2025-05-02T02:39:31.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 11) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.359+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 8.0 in stage 2.0 (TID 11)
[2025-05-02T02:39:31.360+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 10) in 131 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T02:39:31.381+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.382+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:31.430+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_8 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.433+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_8 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.466+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 8.0 in stage 2.0 (TID 11). 5308 bytes result sent to driver
[2025-05-02T02:39:31.468+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 12) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.469+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 9.0 in stage 2.0 (TID 12)
[2025-05-02T02:39:31.470+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 11) in 113 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T02:39:31.491+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.492+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.540+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_9 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.543+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_9 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.577+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 9.0 in stage 2.0 (TID 12). 5308 bytes result sent to driver
[2025-05-02T02:39:31.579+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 13) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.580+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 10.0 in stage 2.0 (TID 13)
[2025-05-02T02:39:31.581+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 12) in 112 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T02:39:31.601+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.602+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.651+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_10 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.652+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_10 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.686+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 10.0 in stage 2.0 (TID 13). 5308 bytes result sent to driver
[2025-05-02T02:39:31.688+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 14) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.689+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 13) in 111 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T02:39:31.690+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 11.0 in stage 2.0 (TID 14)
[2025-05-02T02:39:31.709+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.710+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.756+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_11 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.756+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_11 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.798+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 11.0 in stage 2.0 (TID 14). 5308 bytes result sent to driver
[2025-05-02T02:39:31.799+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 15) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.801+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 12.0 in stage 2.0 (TID 15)
[2025-05-02T02:39:31.802+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 14) in 113 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T02:39:31.827+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.828+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.875+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_12 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.876+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_12 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:31.906+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Finished task 12.0 in stage 2.0 (TID 15). 5308 bytes result sent to driver
[2025-05-02T02:39:31.908+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 16) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:31.909+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO Executor: Running task 13.0 in stage 2.0 (TID 16)
[2025-05-02T02:39:31.910+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 15) in 110 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T02:39:31.934+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:31.935+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:31.979+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO MemoryStore: Block rdd_10_13 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:31.980+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:31 INFO BlockManagerInfo: Added rdd_10_13 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.009+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 13.0 in stage 2.0 (TID 16). 5308 bytes result sent to driver
[2025-05-02T02:39:32.010+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 17) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.011+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 14.0 in stage 2.0 (TID 17)
[2025-05-02T02:39:32.012+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 16) in 104 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T02:39:32.030+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.031+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.072+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_14 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.073+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_14 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.104+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 14.0 in stage 2.0 (TID 17). 5308 bytes result sent to driver
[2025-05-02T02:39:32.106+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 18) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.107+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 17) in 97 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T02:39:32.108+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 15.0 in stage 2.0 (TID 18)
[2025-05-02T02:39:32.126+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.127+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:32.167+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_15 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.170+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_15 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.220+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 15.0 in stage 2.0 (TID 18). 5308 bytes result sent to driver
[2025-05-02T02:39:32.222+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 19) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.223+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 18) in 117 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T02:39:32.224+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 16.0 in stage 2.0 (TID 19)
[2025-05-02T02:39:32.247+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.248+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.299+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_16 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.300+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_16 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.338+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 16.0 in stage 2.0 (TID 19). 5351 bytes result sent to driver
[2025-05-02T02:39:32.340+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 20) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.341+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 17.0 in stage 2.0 (TID 20)
[2025-05-02T02:39:32.342+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 19) in 119 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T02:39:32.361+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.362+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.427+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_17 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.430+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_17 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.463+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 17.0 in stage 2.0 (TID 20). 5308 bytes result sent to driver
[2025-05-02T02:39:32.465+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 21) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.466+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 18.0 in stage 2.0 (TID 21)
[2025-05-02T02:39:32.466+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 20) in 126 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T02:39:32.485+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.486+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.527+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_18 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.528+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_18 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.556+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 18.0 in stage 2.0 (TID 21). 5308 bytes result sent to driver
[2025-05-02T02:39:32.557+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 22) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 21) in 94 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T02:39:32.559+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 19.0 in stage 2.0 (TID 22)
[2025-05-02T02:39:32.578+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.579+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.620+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_19 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.623+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_19 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.654+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 19.0 in stage 2.0 (TID 22). 5308 bytes result sent to driver
[2025-05-02T02:39:32.656+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 23) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.657+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 22) in 100 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T02:39:32.658+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 20.0 in stage 2.0 (TID 23)
[2025-05-02T02:39:32.675+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.677+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.717+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_20 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.718+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_20 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.755+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 20.0 in stage 2.0 (TID 23). 5308 bytes result sent to driver
[2025-05-02T02:39:32.756+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 24) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.757+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 21.0 in stage 2.0 (TID 24)
[2025-05-02T02:39:32.758+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 23) in 102 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T02:39:32.777+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.778+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:32.830+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_21 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.831+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_21 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.863+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 21.0 in stage 2.0 (TID 24). 5308 bytes result sent to driver
[2025-05-02T02:39:32.864+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 25) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.865+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 22.0 in stage 2.0 (TID 25)
[2025-05-02T02:39:32.866+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 24) in 109 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T02:39:32.884+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.885+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:32.926+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO MemoryStore: Block rdd_10_22 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:32.927+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO BlockManagerInfo: Added rdd_10_22 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:32.955+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Finished task 22.0 in stage 2.0 (TID 25). 5308 bytes result sent to driver
[2025-05-02T02:39:32.957+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 26) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:32.958+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 25) in 93 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T02:39:32.959+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO Executor: Running task 23.0 in stage 2.0 (TID 26)
[2025-05-02T02:39:32.977+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:32.978+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:33.018+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_23 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.019+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_23 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.049+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 23.0 in stage 2.0 (TID 26). 5308 bytes result sent to driver
[2025-05-02T02:39:33.050+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 27) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.051+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 26) in 95 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T02:39:33.052+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 24.0 in stage 2.0 (TID 27)
[2025-05-02T02:39:33.069+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.070+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.111+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_24 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.112+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_24 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.142+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 24.0 in stage 2.0 (TID 27). 5308 bytes result sent to driver
[2025-05-02T02:39:33.143+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 28) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.144+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 27) in 93 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T02:39:33.145+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 25.0 in stage 2.0 (TID 28)
[2025-05-02T02:39:33.163+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.210+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_25 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.213+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_25 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.241+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 25.0 in stage 2.0 (TID 28). 5351 bytes result sent to driver
[2025-05-02T02:39:33.242+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 29) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.243+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 26.0 in stage 2.0 (TID 29)
[2025-05-02T02:39:33.244+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 28) in 101 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T02:39:33.260+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.261+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.307+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_26 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.308+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_26 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 26.0 in stage 2.0 (TID 29). 5351 bytes result sent to driver
[2025-05-02T02:39:33.337+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 30) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.338+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 29) in 96 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T02:39:33.339+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 27.0 in stage 2.0 (TID 30)
[2025-05-02T02:39:33.363+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.364+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.408+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_27 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.408+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_27 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.435+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 27.0 in stage 2.0 (TID 30). 5308 bytes result sent to driver
[2025-05-02T02:39:33.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 31) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 28.0 in stage 2.0 (TID 31)
[2025-05-02T02:39:33.438+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 30) in 101 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T02:39:33.454+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.456+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_28 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.494+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_28 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.522+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 28.0 in stage 2.0 (TID 31). 5308 bytes result sent to driver
[2025-05-02T02:39:33.524+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 32) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.525+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 31) in 89 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T02:39:33.526+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 29.0 in stage 2.0 (TID 32)
[2025-05-02T02:39:33.542+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.543+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.583+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_29 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.584+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_29 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.610+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 29.0 in stage 2.0 (TID 32). 5308 bytes result sent to driver
[2025-05-02T02:39:33.611+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 33) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.612+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 30.0 in stage 2.0 (TID 33)
[2025-05-02T02:39:33.613+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 32) in 89 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T02:39:33.628+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.629+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.669+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_30 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.670+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_30 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.696+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 30.0 in stage 2.0 (TID 33). 5308 bytes result sent to driver
[2025-05-02T02:39:33.697+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 34) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.698+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 33) in 88 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T02:39:33.699+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 31.0 in stage 2.0 (TID 34)
[2025-05-02T02:39:33.715+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.716+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.755+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_31 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.756+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_31 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.782+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 31.0 in stage 2.0 (TID 34). 5308 bytes result sent to driver
[2025-05-02T02:39:33.784+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 35) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.785+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 32.0 in stage 2.0 (TID 35)
[2025-05-02T02:39:33.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 34) in 87 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T02:39:33.806+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.807+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.846+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_32 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.847+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_32 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.873+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 32.0 in stage 2.0 (TID 35). 5308 bytes result sent to driver
[2025-05-02T02:39:33.874+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 36) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.875+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 35) in 92 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T02:39:33.877+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 33.0 in stage 2.0 (TID 36)
[2025-05-02T02:39:33.891+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.892+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:33.930+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO MemoryStore: Block rdd_10_33 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:33.931+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO BlockManagerInfo: Added rdd_10_33 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:33.956+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Finished task 33.0 in stage 2.0 (TID 36). 5308 bytes result sent to driver
[2025-05-02T02:39:33.958+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 37) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:33.959+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO Executor: Running task 34.0 in stage 2.0 (TID 37)
[2025-05-02T02:39:33.960+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 36) in 84 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T02:39:33.974+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:33.975+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.013+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_34 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.014+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_34 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.039+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 34.0 in stage 2.0 (TID 37). 5308 bytes result sent to driver
[2025-05-02T02:39:34.040+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 38) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.041+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 35.0 in stage 2.0 (TID 38)
[2025-05-02T02:39:34.042+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 37) in 83 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T02:39:34.056+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.058+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.095+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_35 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.097+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_35 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.124+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 35.0 in stage 2.0 (TID 38). 5308 bytes result sent to driver
[2025-05-02T02:39:34.126+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 39) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.127+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 36.0 in stage 2.0 (TID 39)
[2025-05-02T02:39:34.127+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 38) in 87 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T02:39:34.153+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.154+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.200+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_36 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.201+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_36 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.250+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 36.0 in stage 2.0 (TID 39). 5351 bytes result sent to driver
[2025-05-02T02:39:34.251+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 40) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.252+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 37.0 in stage 2.0 (TID 40)
[2025-05-02T02:39:34.253+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 39) in 126 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T02:39:34.275+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.276+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:34.314+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_37 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.315+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_37 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.341+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 37.0 in stage 2.0 (TID 40). 5308 bytes result sent to driver
[2025-05-02T02:39:34.343+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 41) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.344+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 38.0 in stage 2.0 (TID 41)
[2025-05-02T02:39:34.345+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 40) in 93 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T02:39:34.359+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.360+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.398+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_38 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.399+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_38 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.428+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 38.0 in stage 2.0 (TID 41). 5308 bytes result sent to driver
[2025-05-02T02:39:34.429+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 42) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.430+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 41) in 88 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T02:39:34.431+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 39.0 in stage 2.0 (TID 42)
[2025-05-02T02:39:34.446+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.448+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.486+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_39 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.487+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_39 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.512+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 39.0 in stage 2.0 (TID 42). 5308 bytes result sent to driver
[2025-05-02T02:39:34.513+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 43) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.514+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 42) in 86 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T02:39:34.515+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 40.0 in stage 2.0 (TID 43)
[2025-05-02T02:39:34.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.532+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_40 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.571+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_40 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.596+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 40.0 in stage 2.0 (TID 43). 5308 bytes result sent to driver
[2025-05-02T02:39:34.597+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 44) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.598+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 43) in 85 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T02:39:34.599+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 41.0 in stage 2.0 (TID 44)
[2025-05-02T02:39:34.620+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.621+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.685+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_41 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.686+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_41 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.710+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 41.0 in stage 2.0 (TID 44). 5308 bytes result sent to driver
[2025-05-02T02:39:34.711+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 45) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.712+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 44.0 in stage 2.0 (TID 45)
[2025-05-02T02:39:34.713+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 44) in 115 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T02:39:34.727+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.728+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.762+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_44 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.764+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_44 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 44.0 in stage 2.0 (TID 45). 5308 bytes result sent to driver
[2025-05-02T02:39:34.787+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 46) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.788+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 45) in 77 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T02:39:34.789+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 45.0 in stage 2.0 (TID 46)
[2025-05-02T02:39:34.802+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.803+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.837+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_45 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.838+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_45 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.861+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 45.0 in stage 2.0 (TID 46). 5308 bytes result sent to driver
[2025-05-02T02:39:34.862+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 47) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.863+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 46.0 in stage 2.0 (TID 47)
[2025-05-02T02:39:34.864+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 46) in 77 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T02:39:34.876+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.877+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.916+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_46 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.917+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_46 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:34.940+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Finished task 46.0 in stage 2.0 (TID 47). 5351 bytes result sent to driver
[2025-05-02T02:39:34.941+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 48) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:34.942+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO Executor: Running task 47.0 in stage 2.0 (TID 48)
[2025-05-02T02:39:34.943+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 47) in 80 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T02:39:34.955+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:34.956+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:34.994+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO MemoryStore: Block rdd_10_47 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:34.995+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:34 INFO BlockManagerInfo: Added rdd_10_47 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:35.017+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Finished task 47.0 in stage 2.0 (TID 48). 5351 bytes result sent to driver
[2025-05-02T02:39:35.018+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 49) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:35.019+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Running task 48.0 in stage 2.0 (TID 49)
[2025-05-02T02:39:35.020+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 48) in 79 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T02:39:35.032+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:35.033+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:35.065+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO MemoryStore: Block rdd_10_48 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:35.066+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO BlockManagerInfo: Added rdd_10_48 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:35.088+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Finished task 48.0 in stage 2.0 (TID 49). 5308 bytes result sent to driver
[2025-05-02T02:39:35.089+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 50) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:35.090+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Running task 49.0 in stage 2.0 (TID 50)
[2025-05-02T02:39:35.091+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 49) in 72 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T02:39:35.103+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:35.104+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:35.135+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO MemoryStore: Block rdd_10_49 stored as values in memory (estimated size 46.0 B, free 433.2 MiB)
[2025-05-02T02:39:35.136+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO BlockManagerInfo: Added rdd_10_49 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.2 MiB)
[2025-05-02T02:39:35.157+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Finished task 49.0 in stage 2.0 (TID 50). 5308 bytes result sent to driver
[2025-05-02T02:39:35.158+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 50) in 70 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T02:39:35.159+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-05-02T02:39:35.159+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: ShuffleMapStage 2 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 6.388 s
[2025-05-02T02:39:35.160+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T02:39:35.161+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: running: Set()
[2025-05-02T02:39:35.162+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: waiting: Set()
[2025-05-02T02:39:35.162+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: failed: Set()
[2025-05-02T02:39:35.205+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
[2025-05-02T02:39:35.208+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Got job 2 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-05-02T02:39:35.209+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Final stage: ResultStage 5 (saveAsTable at NativeMethodAccessorImpl.java:0)
[2025-05-02T02:39:35.210+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2025-05-02T02:39:35.211+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:35.211+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-05-02T02:39:35.222+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 537.2 KiB, free 432.7 MiB)
[2025-05-02T02:39:35.225+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 125.4 KiB, free 432.6 MiB)
[2025-05-02T02:39:35.227+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ***-scheduler:43081 (size: 125.4 KiB, free: 434.1 MiB)
[2025-05-02T02:39:35.227+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:35.229+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:35.230+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-05-02T02:39:35.232+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 51) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-02T02:39:35.234+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Running task 0.0 in stage 5.0 (TID 51)
[2025-05-02T02:39:35.256+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Getting 50 (4.7 KiB) non-empty blocks including 50 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:35.257+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:35.276+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO CodeGenerator: Code generated in 7.060552 ms
[2025-05-02T02:39:35.308+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO CodeGenerator: Code generated in 22.21276 ms
[2025-05-02T02:39:35.327+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Executor: Finished task 0.0 in stage 5.0 (TID 51). 7013 bytes result sent to driver
[2025-05-02T02:39:35.328+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 51) in 97 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:35.329+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-05-02T02:39:35.330+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: ResultStage 5 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 0.116 s
[2025-05-02T02:39:35.331+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:35.332+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-05-02T02:39:35.333+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DAGScheduler: Job 2 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 0.128291 s
[2025-05-02T02:39:35.383+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO CodeGenerator: Code generated in 33.356456 ms
[2025-05-02T02:39:35.386+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO Snapshot: DELTA: Done
[2025-05-02T02:39:35.463+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a
[2025-05-02T02:39:35.515+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
[2025-05-02T02:39:35.553+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DummySnapshot: [tableId=da18f1d4-5bcc-4b92-9ee1-5cbce538388c] Created snapshot DummySnapshot(path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log, version=-1, metadata=Metadata(9e249d88-bd82-46d2-af50-3f58ab9d19e1,null,null,Format(parquet,Map()),null,List(),Map(),Some(1746153575518)), logSegment=LogSegment(s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,-1), checksumOpt=None)
[2025-05-02T02:39:35.655+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
[2025-05-02T02:39:35.659+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO DummySnapshot: [tableId=9e249d88-bd82-46d2-af50-3f58ab9d19e1] Created snapshot DummySnapshot(path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log, version=-1, metadata=Metadata(afbf6c39-868a-4894-9075-b8451943673e,null,null,Format(parquet,Map()),null,List(),Map(),Some(1746153575656)), logSegment=LogSegment(s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,-1), checksumOpt=None)
[2025-05-02T02:39:35.763+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:35 INFO OptimisticTransaction: [tableId=afbf6c39,txnId=e6dc9472] Updated metadata from - to Metadata(c788f322-2da7-4aab-817f-ac2128740e83,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"Provider_Key","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Id","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Name","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Gender","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Speciality","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Address","type":"string","nullable":true,"metadata":{}},{"name":"Provider_City","type":"string","nullable":true,"metadata":{}},{"name":"Provider_State","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Utilization","type":"long","nullable":true,"metadata":{}},{"name":"Organization_Name","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Address","type":"string","nullable":true,"metadata":{}},{"name":"Organization_City","type":"string","nullable":true,"metadata":{}},{"name":"Organization_State","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Phone","type":"string","nullable":true,"metadata":{}},{"name":"Effective_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Expiration_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Is_Current","type":"boolean","nullable":true,"metadata":{}}]},ArrayBuffer(),Map(),Some(1746153575720))
[2025-05-02T02:39:36.731+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO PrepareDeltaScan: DELTA: Filtering files for query
[2025-05-02T02:39:36.737+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO Snapshot: DELTA: Compute snapshot for version: 0
[2025-05-02T02:39:36.741+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 204.7 KiB, free 432.4 MiB)
[2025-05-02T02:39:36.753+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 432.3 MiB)
[2025-05-02T02:39:36.754+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ***-scheduler:43081 (size: 35.8 KiB, free: 434.0 MiB)
[2025-05-02T02:39:36.755+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO SparkContext: Created broadcast 5 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:36.756+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 3040)
[2025-05-02T02:39:37.003+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DataSourceStrategy: Pruning directories with:
[2025-05-02T02:39:37.004+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T02:39:37.005+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T02:39:37.054+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 205.0 KiB, free 432.1 MiB)
[2025-05-02T02:39:37.065+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 432.1 MiB)
[2025-05-02T02:39:37.066+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ***-scheduler:43081 (size: 35.9 KiB, free: 434.0 MiB)
[2025-05-02T02:39:37.067+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO SparkContext: Created broadcast 6 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:37.068+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4197344 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T02:39:37.078+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Registering RDD 20 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2025-05-02T02:39:37.079+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Got map stage job 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T02:39:37.080+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Final stage: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:37.081+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T02:39:37.081+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:37.082+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[20] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:37.083+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 105.9 KiB, free 432.0 MiB)
[2025-05-02T02:39:37.084+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 431.9 MiB)
[2025-05-02T02:39:37.085+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ***-scheduler:43081 (size: 32.7 KiB, free: 434.0 MiB)
[2025-05-02T02:39:37.086+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:37.087+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[20] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:37.088+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-05-02T02:39:37.089+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 52) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10891 bytes)
[2025-05-02T02:39:37.090+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 52)
[2025-05-02T02:39:37.102+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ***-scheduler:43081 in memory (size: 125.4 KiB, free: 434.1 MiB)
[2025-05-02T02:39:37.106+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/_delta_log/00000000000000000000.json, range: 0-3040, partition values: [0]
[2025-05-02T02:39:37.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 52). 1927 bytes result sent to driver
[2025-05-02T02:39:37.165+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 52) in 78 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:37.166+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-05-02T02:39:37.167+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.086 s
[2025-05-02T02:39:37.168+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T02:39:37.169+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: running: Set()
[2025-05-02T02:39:37.169+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: waiting: Set()
[2025-05-02T02:39:37.169+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: failed: Set()
[2025-05-02T02:39:37.244+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ***-scheduler:43081 in memory (size: 138.8 KiB, free: 434.2 MiB)
[2025-05-02T02:39:37.412+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Registering RDD 30 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 3
[2025-05-02T02:39:37.413+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Got map stage job 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-02T02:39:37.414+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:37.415+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2025-05-02T02:39:37.415+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:37.416+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:37.431+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 605.5 KiB, free 432.7 MiB)
[2025-05-02T02:39:37.434+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 138.9 KiB, free 432.6 MiB)
[2025-05-02T02:39:37.436+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ***-scheduler:43081 (size: 138.9 KiB, free: 434.1 MiB)
[2025-05-02T02:39:37.436+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:37.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T02:39:37.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 50 tasks resource profile 0
[2025-05-02T02:39:37.438+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 53) (***-scheduler, executor driver, partition 41, NODE_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.439+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 41.0 in stage 8.0 (TID 53)
[2025-05-02T02:39:37.455+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.456+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.490+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_41 stored as values in memory (estimated size 967.0 B, free 432.6 MiB)
[2025-05-02T02:39:37.491+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_41 in memory on ***-scheduler:43081 (size: 967.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.512+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 41.0 in stage 8.0 (TID 53). 5308 bytes result sent to driver
[2025-05-02T02:39:37.513+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 54) (***-scheduler, executor driver, partition 42, NODE_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.514+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 42.0 in stage 8.0 (TID 54)
[2025-05-02T02:39:37.515+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 53) in 76 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T02:39:37.526+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.527+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.555+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_42 stored as values in memory (estimated size 602.0 B, free 432.6 MiB)
[2025-05-02T02:39:37.556+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_42 in memory on ***-scheduler:43081 (size: 602.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.589+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 42.0 in stage 8.0 (TID 54). 5308 bytes result sent to driver
[2025-05-02T02:39:37.590+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 55) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.591+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 0.0 in stage 8.0 (TID 55)
[2025-05-02T02:39:37.592+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 54) in 78 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T02:39:37.605+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.606+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.633+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_0 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:37.634+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_0 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.654+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 0.0 in stage 8.0 (TID 55). 5308 bytes result sent to driver
[2025-05-02T02:39:37.656+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 56) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.656+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 1.0 in stage 8.0 (TID 56)
[2025-05-02T02:39:37.657+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 55) in 66 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T02:39:37.668+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.669+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.705+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_1 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:37.706+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_1 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.727+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 1.0 in stage 8.0 (TID 56). 5308 bytes result sent to driver
[2025-05-02T02:39:37.728+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 57) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.729+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 56) in 73 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T02:39:37.730+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 2.0 in stage 8.0 (TID 57)
[2025-05-02T02:39:37.749+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.750+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2025-05-02T02:39:37.751+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ***-scheduler:43081 in memory (size: 32.7 KiB, free: 434.1 MiB)
[2025-05-02T02:39:37.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_2 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:37.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_2 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.806+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 2.0 in stage 8.0 (TID 57). 5351 bytes result sent to driver
[2025-05-02T02:39:37.807+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 58) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.808+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 57) in 81 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T02:39:37.809+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 3.0 in stage 8.0 (TID 58)
[2025-05-02T02:39:37.819+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.820+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.846+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_3 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:37.847+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_3 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.866+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 3.0 in stage 8.0 (TID 58). 5308 bytes result sent to driver
[2025-05-02T02:39:37.868+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 59) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.869+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 4.0 in stage 8.0 (TID 59)
[2025-05-02T02:39:37.870+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 58) in 61 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T02:39:37.880+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.881+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.915+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_4 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:37.916+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_4 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.936+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 4.0 in stage 8.0 (TID 59). 5308 bytes result sent to driver
[2025-05-02T02:39:37.937+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 60) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.938+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 59) in 70 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T02:39:37.939+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 5.0 in stage 8.0 (TID 60)
[2025-05-02T02:39:37.949+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:37.950+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:37.976+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO MemoryStore: Block rdd_27_5 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:37.977+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO BlockManagerInfo: Added rdd_27_5 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:37.997+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Finished task 5.0 in stage 8.0 (TID 60). 5308 bytes result sent to driver
[2025-05-02T02:39:37.998+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 61) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:37.999+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO Executor: Running task 6.0 in stage 8.0 (TID 61)
[2025-05-02T02:39:38.000+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:37 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 60) in 62 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T02:39:38.010+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.011+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.036+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_6 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.037+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_6 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.058+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 6.0 in stage 8.0 (TID 61). 5308 bytes result sent to driver
[2025-05-02T02:39:38.059+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 62) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.060+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 7.0 in stage 8.0 (TID 62)
[2025-05-02T02:39:38.061+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 61) in 62 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T02:39:38.073+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.074+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.099+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_7 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.099+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_7 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.118+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 7.0 in stage 8.0 (TID 62). 5308 bytes result sent to driver
[2025-05-02T02:39:38.119+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 63) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.120+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 62) in 60 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T02:39:38.121+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 8.0 in stage 8.0 (TID 63)
[2025-05-02T02:39:38.131+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.132+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.158+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_8 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.159+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_8 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.178+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 8.0 in stage 8.0 (TID 63). 5308 bytes result sent to driver
[2025-05-02T02:39:38.180+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 64) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.181+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 9.0 in stage 8.0 (TID 64)
[2025-05-02T02:39:38.181+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 63) in 62 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T02:39:38.192+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.193+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.218+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_9 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.219+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_9 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.237+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 9.0 in stage 8.0 (TID 64). 5308 bytes result sent to driver
[2025-05-02T02:39:38.238+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 65) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.239+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 64) in 59 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T02:39:38.240+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 10.0 in stage 8.0 (TID 65)
[2025-05-02T02:39:38.250+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.251+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.276+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_10 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.276+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_10 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.295+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 10.0 in stage 8.0 (TID 65). 5308 bytes result sent to driver
[2025-05-02T02:39:38.296+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 66) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.297+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 65) in 59 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T02:39:38.298+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 11.0 in stage 8.0 (TID 66)
[2025-05-02T02:39:38.308+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.309+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_11 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.336+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_11 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.354+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 11.0 in stage 8.0 (TID 66). 5308 bytes result sent to driver
[2025-05-02T02:39:38.355+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 67) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.356+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 66) in 60 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T02:39:38.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 12.0 in stage 8.0 (TID 67)
[2025-05-02T02:39:38.366+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.367+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.407+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_12 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.408+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_12 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 12.0 in stage 8.0 (TID 67). 5351 bytes result sent to driver
[2025-05-02T02:39:38.439+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 68) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.440+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 67) in 84 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T02:39:38.440+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 13.0 in stage 8.0 (TID 68)
[2025-05-02T02:39:38.451+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.452+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.478+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_13 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.479+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_13 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.511+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 13.0 in stage 8.0 (TID 68). 5308 bytes result sent to driver
[2025-05-02T02:39:38.512+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 69) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.513+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 14.0 in stage 8.0 (TID 69)
[2025-05-02T02:39:38.514+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 68) in 75 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T02:39:38.524+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.525+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.551+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_14 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.551+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_14 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.569+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 14.0 in stage 8.0 (TID 69). 5308 bytes result sent to driver
[2025-05-02T02:39:38.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 70) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.571+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 15.0 in stage 8.0 (TID 70)
[2025-05-02T02:39:38.572+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 69) in 60 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T02:39:38.582+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.583+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.608+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_15 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.609+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_15 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.627+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 15.0 in stage 8.0 (TID 70). 5308 bytes result sent to driver
[2025-05-02T02:39:38.628+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 71) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.629+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 70) in 59 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T02:39:38.630+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 16.0 in stage 8.0 (TID 71)
[2025-05-02T02:39:38.639+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.640+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.665+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_16 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.666+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_16 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.693+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 16.0 in stage 8.0 (TID 71). 5308 bytes result sent to driver
[2025-05-02T02:39:38.695+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 72) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.695+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 17.0 in stage 8.0 (TID 72)
[2025-05-02T02:39:38.696+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 71) in 67 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T02:39:38.706+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.707+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.731+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_17 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.732+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_17 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.750+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 17.0 in stage 8.0 (TID 72). 5308 bytes result sent to driver
[2025-05-02T02:39:38.752+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 73) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.753+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 18.0 in stage 8.0 (TID 73)
[2025-05-02T02:39:38.753+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 72) in 58 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T02:39:38.763+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.764+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.788+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_18 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.789+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_18 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.807+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 18.0 in stage 8.0 (TID 73). 5308 bytes result sent to driver
[2025-05-02T02:39:38.808+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 74) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.809+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 73) in 58 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T02:39:38.810+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 19.0 in stage 8.0 (TID 74)
[2025-05-02T02:39:38.820+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.821+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.845+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_19 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.846+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_19 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.864+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 19.0 in stage 8.0 (TID 74). 5308 bytes result sent to driver
[2025-05-02T02:39:38.865+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 75) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.866+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 20.0 in stage 8.0 (TID 75)
[2025-05-02T02:39:38.867+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 74) in 57 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T02:39:38.877+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.878+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.902+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_20 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.903+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_20 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.920+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 20.0 in stage 8.0 (TID 75). 5308 bytes result sent to driver
[2025-05-02T02:39:38.922+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 76) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.922+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 75) in 57 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T02:39:38.923+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 21.0 in stage 8.0 (TID 76)
[2025-05-02T02:39:38.933+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.934+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:38.958+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO MemoryStore: Block rdd_27_21 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:38.959+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO BlockManagerInfo: Added rdd_27_21 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:38.976+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Finished task 21.0 in stage 8.0 (TID 76). 5308 bytes result sent to driver
[2025-05-02T02:39:38.978+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 77) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:38.978+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO Executor: Running task 22.0 in stage 8.0 (TID 77)
[2025-05-02T02:39:38.979+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 76) in 57 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T02:39:38.989+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:38.990+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.018+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_22 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.019+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_22 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.035+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 22.0 in stage 8.0 (TID 77). 5351 bytes result sent to driver
[2025-05-02T02:39:39.036+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 78) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.037+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 77) in 60 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T02:39:39.038+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 23.0 in stage 8.0 (TID 78)
[2025-05-02T02:39:39.047+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.048+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.078+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_23 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.079+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_23 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.098+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 23.0 in stage 8.0 (TID 78). 5351 bytes result sent to driver
[2025-05-02T02:39:39.099+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 79) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.100+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 24.0 in stage 8.0 (TID 79)
[2025-05-02T02:39:39.101+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 78) in 64 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T02:39:39.110+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.111+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.135+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_24 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.135+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_24 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.152+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 24.0 in stage 8.0 (TID 79). 5308 bytes result sent to driver
[2025-05-02T02:39:39.153+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 80) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.154+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 79) in 54 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T02:39:39.154+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 25.0 in stage 8.0 (TID 80)
[2025-05-02T02:39:39.163+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.187+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_25 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.188+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_25 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.206+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 25.0 in stage 8.0 (TID 80). 5308 bytes result sent to driver
[2025-05-02T02:39:39.207+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 81) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.208+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 80) in 55 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T02:39:39.208+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 26.0 in stage 8.0 (TID 81)
[2025-05-02T02:39:39.218+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.218+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.241+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_26 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.242+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_26 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.259+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 26.0 in stage 8.0 (TID 81). 5308 bytes result sent to driver
[2025-05-02T02:39:39.260+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 82) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.261+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 27.0 in stage 8.0 (TID 82)
[2025-05-02T02:39:39.262+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 81) in 55 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T02:39:39.272+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.273+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.296+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_27 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.297+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_27 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.314+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 27.0 in stage 8.0 (TID 82). 5308 bytes result sent to driver
[2025-05-02T02:39:39.315+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 83) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.316+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 82) in 55 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T02:39:39.316+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 28.0 in stage 8.0 (TID 83)
[2025-05-02T02:39:39.329+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.330+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.353+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_28 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.354+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_28 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.373+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 28.0 in stage 8.0 (TID 83). 5308 bytes result sent to driver
[2025-05-02T02:39:39.374+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 84) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.374+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 29.0 in stage 8.0 (TID 84)
[2025-05-02T02:39:39.375+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 83) in 60 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T02:39:39.385+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.386+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.409+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_29 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.410+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_29 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.426+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 29.0 in stage 8.0 (TID 84). 5308 bytes result sent to driver
[2025-05-02T02:39:39.427+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 85) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.428+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 30.0 in stage 8.0 (TID 85)
[2025-05-02T02:39:39.429+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 84) in 55 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T02:39:39.439+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.440+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.462+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_30 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.463+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_30 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.479+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 30.0 in stage 8.0 (TID 85). 5308 bytes result sent to driver
[2025-05-02T02:39:39.480+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 86) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.481+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 85) in 53 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T02:39:39.481+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 31.0 in stage 8.0 (TID 86)
[2025-05-02T02:39:39.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.494+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.515+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_31 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.516+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_31 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.532+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 31.0 in stage 8.0 (TID 86). 5308 bytes result sent to driver
[2025-05-02T02:39:39.533+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 87) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.534+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 86) in 55 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T02:39:39.535+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 32.0 in stage 8.0 (TID 87)
[2025-05-02T02:39:39.544+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.545+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.567+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_32 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.568+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_32 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.583+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 32.0 in stage 8.0 (TID 87). 5308 bytes result sent to driver
[2025-05-02T02:39:39.584+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 88) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.585+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 87) in 51 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T02:39:39.585+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 33.0 in stage 8.0 (TID 88)
[2025-05-02T02:39:39.594+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.595+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.617+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_33 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.618+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_33 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.634+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 33.0 in stage 8.0 (TID 88). 5308 bytes result sent to driver
[2025-05-02T02:39:39.635+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 89) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.635+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 88) in 51 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T02:39:39.636+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 34.0 in stage 8.0 (TID 89)
[2025-05-02T02:39:39.646+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.647+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.676+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_34 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.677+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_34 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.694+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 34.0 in stage 8.0 (TID 89). 5351 bytes result sent to driver
[2025-05-02T02:39:39.695+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 90) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.696+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 89) in 62 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T02:39:39.697+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 35.0 in stage 8.0 (TID 90)
[2025-05-02T02:39:39.707+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.708+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.731+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_35 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.732+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_35 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.757+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 35.0 in stage 8.0 (TID 90). 5308 bytes result sent to driver
[2025-05-02T02:39:39.758+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 91) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.759+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 90) in 63 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T02:39:39.760+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 36.0 in stage 8.0 (TID 91)
[2025-05-02T02:39:39.772+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.772+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.798+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_36 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.799+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_36 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.815+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 36.0 in stage 8.0 (TID 91). 5308 bytes result sent to driver
[2025-05-02T02:39:39.816+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 92) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.817+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 91) in 58 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T02:39:39.817+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 37.0 in stage 8.0 (TID 92)
[2025-05-02T02:39:39.827+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.827+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.850+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_37 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.851+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_37 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.868+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 37.0 in stage 8.0 (TID 92). 5308 bytes result sent to driver
[2025-05-02T02:39:39.869+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 93) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.870+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 92) in 53 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T02:39:39.871+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 38.0 in stage 8.0 (TID 93)
[2025-05-02T02:39:39.879+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.880+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.903+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_38 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.903+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_38 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.928+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 38.0 in stage 8.0 (TID 93). 5308 bytes result sent to driver
[2025-05-02T02:39:39.929+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 94) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.930+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 93) in 61 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T02:39:39.930+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 39.0 in stage 8.0 (TID 94)
[2025-05-02T02:39:39.940+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:39.941+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:39.963+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO MemoryStore: Block rdd_27_39 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:39.964+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO BlockManagerInfo: Added rdd_27_39 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:39.983+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Finished task 39.0 in stage 8.0 (TID 94). 5308 bytes result sent to driver
[2025-05-02T02:39:39.984+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 95) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:39.985+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO Executor: Running task 40.0 in stage 8.0 (TID 95)
[2025-05-02T02:39:39.986+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 94) in 57 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T02:39:39.999+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.000+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.033+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_40 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.034+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_40 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.050+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 40.0 in stage 8.0 (TID 95). 5308 bytes result sent to driver
[2025-05-02T02:39:40.051+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 96) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.052+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 43.0 in stage 8.0 (TID 96)
[2025-05-02T02:39:40.052+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 95) in 67 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T02:39:40.061+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.062+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.085+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_43 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.086+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_43 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.102+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 43.0 in stage 8.0 (TID 96). 5308 bytes result sent to driver
[2025-05-02T02:39:40.103+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 97) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.104+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 96) in 53 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T02:39:40.105+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 44.0 in stage 8.0 (TID 97)
[2025-05-02T02:39:40.114+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.115+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.137+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_44 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.138+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_44 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.153+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 44.0 in stage 8.0 (TID 97). 5308 bytes result sent to driver
[2025-05-02T02:39:40.154+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 98) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.156+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 45.0 in stage 8.0 (TID 98)
[2025-05-02T02:39:40.156+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 97) in 53 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T02:39:40.165+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.166+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.187+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_45 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.188+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_45 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.203+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 45.0 in stage 8.0 (TID 98). 5308 bytes result sent to driver
[2025-05-02T02:39:40.204+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 99) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.204+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 98) in 50 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T02:39:40.205+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 46.0 in stage 8.0 (TID 99)
[2025-05-02T02:39:40.214+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.215+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.236+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_46 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.237+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_46 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.253+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 46.0 in stage 8.0 (TID 99). 5308 bytes result sent to driver
[2025-05-02T02:39:40.254+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 100) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 47.0 in stage 8.0 (TID 100)
[2025-05-02T02:39:40.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 99) in 51 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T02:39:40.264+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.265+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.292+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_47 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.293+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_47 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.308+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 47.0 in stage 8.0 (TID 100). 5351 bytes result sent to driver
[2025-05-02T02:39:40.309+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 101) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.310+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 100) in 56 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T02:39:40.311+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 48.0 in stage 8.0 (TID 101)
[2025-05-02T02:39:40.322+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.323+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.349+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_48 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.350+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_48 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.366+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 48.0 in stage 8.0 (TID 101). 5351 bytes result sent to driver
[2025-05-02T02:39:40.367+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 102) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10195 bytes)
[2025-05-02T02:39:40.367+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 101) in 58 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T02:39:40.368+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 49.0 in stage 8.0 (TID 102)
[2025-05-02T02:39:40.380+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.381+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.417+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block rdd_27_49 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)
[2025-05-02T02:39:40.418+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added rdd_27_49 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:40.441+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 49.0 in stage 8.0 (TID 102). 5308 bytes result sent to driver
[2025-05-02T02:39:40.443+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 102) in 76 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T02:39:40.443+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-05-02T02:39:40.444+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 3.022 s
[2025-05-02T02:39:40.445+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T02:39:40.445+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: running: Set()
[2025-05-02T02:39:40.446+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: waiting: Set()
[2025-05-02T02:39:40.447+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: failed: Set()
[2025-05-02T02:39:40.467+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:40.469+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Got job 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T02:39:40.470+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:40.470+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-05-02T02:39:40.471+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:40.471+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[33] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:40.478+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 537.2 KiB, free 432.2 MiB)
[2025-05-02T02:39:40.481+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 125.4 KiB, free 432.1 MiB)
[2025-05-02T02:39:40.481+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ***-scheduler:43081 (size: 125.4 KiB, free: 434.0 MiB)
[2025-05-02T02:39:40.482+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:40.483+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[33] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:40.483+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-05-02T02:39:40.484+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 103) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10206 bytes)
[2025-05-02T02:39:40.485+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 0.0 in stage 11.0 (TID 103)
[2025-05-02T02:39:40.497+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Getting 50 (4.7 KiB) non-empty blocks including 50 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:40.498+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-05-02T02:39:40.527+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 0.0 in stage 11.0 (TID 103). 6998 bytes result sent to driver
[2025-05-02T02:39:40.528+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 103) in 45 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:40.529+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-05-02T02:39:40.530+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.059 s
[2025-05-02T02:39:40.530+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:40.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-05-02T02:39:40.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Job 5 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.062296 s
[2025-05-02T02:39:40.538+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Snapshot: DELTA: Done
[2025-05-02T02:39:40.820+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO CodeGenerator: Code generated in 83.011653 ms
[2025-05-02T02:39:40.846+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:40.848+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Got job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-02T02:39:40.849+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:40.849+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2025-05-02T02:39:40.850+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:40.850+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[35] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:40.855+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 687.2 KiB, free 431.4 MiB)
[2025-05-02T02:39:40.858+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 155.4 KiB, free 431.3 MiB)
[2025-05-02T02:39:40.859+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ***-scheduler:43081 (size: 155.4 KiB, free: 433.8 MiB)
[2025-05-02T02:39:40.859+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:40.860+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 13 (MapPartitionsRDD[35] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T02:39:40.861+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSchedulerImpl: Adding task set 13.0 with 50 tasks resource profile 0
[2025-05-02T02:39:40.862+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 104) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:40.863+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 0.0 in stage 13.0 (TID 104)
[2025-05-02T02:39:40.875+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManager: Found block rdd_27_0 locally
[2025-05-02T02:39:40.959+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO CodeGenerator: Code generated in 80.623611 ms
[2025-05-02T02:39:40.964+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 0.0 in stage 13.0 (TID 104). 4181 bytes result sent to driver
[2025-05-02T02:39:40.965+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 105) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:40.966+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 1.0 in stage 13.0 (TID 105)
[2025-05-02T02:39:40.966+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 104) in 104 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T02:39:40.974+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManager: Found block rdd_27_1 locally
[2025-05-02T02:39:40.976+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 1.0 in stage 13.0 (TID 105). 4181 bytes result sent to driver
[2025-05-02T02:39:40.977+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 106) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:40.978+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 105) in 12 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T02:39:40.978+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 2.0 in stage 13.0 (TID 106)
[2025-05-02T02:39:40.988+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO BlockManager: Found block rdd_27_2 locally
[2025-05-02T02:39:40.990+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Finished task 2.0 in stage 13.0 (TID 106). 4181 bytes result sent to driver
[2025-05-02T02:39:40.991+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 107) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:40.991+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO Executor: Running task 3.0 in stage 13.0 (TID 107)
[2025-05-02T02:39:40.992+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:40 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 106) in 15 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T02:39:41.000+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_3 locally
[2025-05-02T02:39:41.001+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 3.0 in stage 13.0 (TID 107). 4181 bytes result sent to driver
[2025-05-02T02:39:41.002+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 108) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.003+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 4.0 in stage 13.0 (TID 108)
[2025-05-02T02:39:41.003+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 107) in 12 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T02:39:41.011+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_4 locally
[2025-05-02T02:39:41.013+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 4.0 in stage 13.0 (TID 108). 4181 bytes result sent to driver
[2025-05-02T02:39:41.013+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 109) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.014+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 5.0 in stage 13.0 (TID 109)
[2025-05-02T02:39:41.015+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 108) in 12 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T02:39:41.022+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_5 locally
[2025-05-02T02:39:41.024+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 5.0 in stage 13.0 (TID 109). 4181 bytes result sent to driver
[2025-05-02T02:39:41.025+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 110) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.026+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 109) in 12 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T02:39:41.026+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 6.0 in stage 13.0 (TID 110)
[2025-05-02T02:39:41.034+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_6 locally
[2025-05-02T02:39:41.036+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 6.0 in stage 13.0 (TID 110). 4181 bytes result sent to driver
[2025-05-02T02:39:41.037+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 111) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.038+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 7.0 in stage 13.0 (TID 111)
[2025-05-02T02:39:41.038+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 110) in 13 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T02:39:41.046+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_7 locally
[2025-05-02T02:39:41.048+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 7.0 in stage 13.0 (TID 111). 4181 bytes result sent to driver
[2025-05-02T02:39:41.048+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 112) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.049+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 8.0 in stage 13.0 (TID 112)
[2025-05-02T02:39:41.050+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 111) in 13 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T02:39:41.058+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_8 locally
[2025-05-02T02:39:41.060+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 8.0 in stage 13.0 (TID 112). 4181 bytes result sent to driver
[2025-05-02T02:39:41.061+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 113) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.061+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 9.0 in stage 13.0 (TID 113)
[2025-05-02T02:39:41.062+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 112) in 13 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T02:39:41.070+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_9 locally
[2025-05-02T02:39:41.071+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 9.0 in stage 13.0 (TID 113). 4181 bytes result sent to driver
[2025-05-02T02:39:41.072+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 114) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.073+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 10.0 in stage 13.0 (TID 114)
[2025-05-02T02:39:41.074+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 113) in 13 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T02:39:41.088+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ***-scheduler:43081 in memory (size: 125.4 KiB, free: 434.0 MiB)
[2025-05-02T02:39:41.089+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_10 locally
[2025-05-02T02:39:41.095+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 10.0 in stage 13.0 (TID 114). 4224 bytes result sent to driver
[2025-05-02T02:39:41.096+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 115) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.097+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 114) in 24 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T02:39:41.098+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 11.0 in stage 13.0 (TID 115)
[2025-05-02T02:39:41.106+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_11 locally
[2025-05-02T02:39:41.108+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 11.0 in stage 13.0 (TID 115). 4181 bytes result sent to driver
[2025-05-02T02:39:41.109+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 116) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.110+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 12.0 in stage 13.0 (TID 116)
[2025-05-02T02:39:41.110+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 115) in 14 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T02:39:41.121+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_12 locally
[2025-05-02T02:39:41.123+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 12.0 in stage 13.0 (TID 116). 4181 bytes result sent to driver
[2025-05-02T02:39:41.124+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 117) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.125+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 13.0 in stage 13.0 (TID 117)
[2025-05-02T02:39:41.126+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 116) in 17 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T02:39:41.135+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_13 locally
[2025-05-02T02:39:41.136+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 13.0 in stage 13.0 (TID 117). 4181 bytes result sent to driver
[2025-05-02T02:39:41.137+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 118) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.138+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 14.0 in stage 13.0 (TID 118)
[2025-05-02T02:39:41.139+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 117) in 14 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T02:39:41.147+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_14 locally
[2025-05-02T02:39:41.149+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 14.0 in stage 13.0 (TID 118). 4181 bytes result sent to driver
[2025-05-02T02:39:41.150+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 119) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.151+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 118) in 13 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T02:39:41.151+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 15.0 in stage 13.0 (TID 119)
[2025-05-02T02:39:41.160+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_15 locally
[2025-05-02T02:39:41.162+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 15.0 in stage 13.0 (TID 119). 4181 bytes result sent to driver
[2025-05-02T02:39:41.163+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 120) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.163+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 16.0 in stage 13.0 (TID 120)
[2025-05-02T02:39:41.164+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 119) in 14 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T02:39:41.181+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ***-scheduler:43081 in memory (size: 138.9 KiB, free: 434.1 MiB)
[2025-05-02T02:39:41.182+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_16 locally
[2025-05-02T02:39:41.183+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 16.0 in stage 13.0 (TID 120). 4181 bytes result sent to driver
[2025-05-02T02:39:41.184+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 121) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.185+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 120) in 23 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T02:39:41.186+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 17.0 in stage 13.0 (TID 121)
[2025-05-02T02:39:41.198+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_17 locally
[2025-05-02T02:39:41.201+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 17.0 in stage 13.0 (TID 121). 4181 bytes result sent to driver
[2025-05-02T02:39:41.201+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 122) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.202+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 18.0 in stage 13.0 (TID 122)
[2025-05-02T02:39:41.203+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 121) in 18 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T02:39:41.214+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_18 locally
[2025-05-02T02:39:41.216+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 18.0 in stage 13.0 (TID 122). 4181 bytes result sent to driver
[2025-05-02T02:39:41.217+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 123) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.218+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 19.0 in stage 13.0 (TID 123)
[2025-05-02T02:39:41.219+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 122) in 17 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T02:39:41.228+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_19 locally
[2025-05-02T02:39:41.230+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 19.0 in stage 13.0 (TID 123). 4181 bytes result sent to driver
[2025-05-02T02:39:41.231+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 124) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.232+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 20.0 in stage 13.0 (TID 124)
[2025-05-02T02:39:41.232+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 123) in 15 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T02:39:41.240+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_20 locally
[2025-05-02T02:39:41.242+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 20.0 in stage 13.0 (TID 124). 4181 bytes result sent to driver
[2025-05-02T02:39:41.243+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 125) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.244+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 21.0 in stage 13.0 (TID 125)
[2025-05-02T02:39:41.244+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 124) in 12 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T02:39:41.252+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_21 locally
[2025-05-02T02:39:41.254+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 21.0 in stage 13.0 (TID 125). 4181 bytes result sent to driver
[2025-05-02T02:39:41.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 126) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.255+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 22.0 in stage 13.0 (TID 126)
[2025-05-02T02:39:41.256+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 125) in 13 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T02:39:41.265+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_22 locally
[2025-05-02T02:39:41.266+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 22.0 in stage 13.0 (TID 126). 4181 bytes result sent to driver
[2025-05-02T02:39:41.267+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 127) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.268+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 23.0 in stage 13.0 (TID 127)
[2025-05-02T02:39:41.269+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 126) in 13 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T02:39:41.278+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_23 locally
[2025-05-02T02:39:41.281+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 23.0 in stage 13.0 (TID 127). 4181 bytes result sent to driver
[2025-05-02T02:39:41.281+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 128) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.282+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 24.0 in stage 13.0 (TID 128)
[2025-05-02T02:39:41.283+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 127) in 15 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T02:39:41.291+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_24 locally
[2025-05-02T02:39:41.292+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 24.0 in stage 13.0 (TID 128). 4181 bytes result sent to driver
[2025-05-02T02:39:41.293+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 129) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.294+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 25.0 in stage 13.0 (TID 129)
[2025-05-02T02:39:41.295+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 128) in 12 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T02:39:41.305+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_25 locally
[2025-05-02T02:39:41.306+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 25.0 in stage 13.0 (TID 129). 4181 bytes result sent to driver
[2025-05-02T02:39:41.307+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 130) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.308+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 26.0 in stage 13.0 (TID 130)
[2025-05-02T02:39:41.309+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 129) in 15 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T02:39:41.317+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_26 locally
[2025-05-02T02:39:41.318+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 26.0 in stage 13.0 (TID 130). 4181 bytes result sent to driver
[2025-05-02T02:39:41.319+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 131) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.320+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 130) in 12 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T02:39:41.321+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 27.0 in stage 13.0 (TID 131)
[2025-05-02T02:39:41.332+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_27 locally
[2025-05-02T02:39:41.334+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 27.0 in stage 13.0 (TID 131). 4181 bytes result sent to driver
[2025-05-02T02:39:41.334+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 132) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 28.0 in stage 13.0 (TID 132)
[2025-05-02T02:39:41.336+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 131) in 16 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T02:39:41.343+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_28 locally
[2025-05-02T02:39:41.345+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 28.0 in stage 13.0 (TID 132). 4181 bytes result sent to driver
[2025-05-02T02:39:41.346+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 133) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.347+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 29.0 in stage 13.0 (TID 133)
[2025-05-02T02:39:41.348+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 132) in 12 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T02:39:41.355+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_29 locally
[2025-05-02T02:39:41.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 29.0 in stage 13.0 (TID 133). 4181 bytes result sent to driver
[2025-05-02T02:39:41.358+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 134) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.359+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 30.0 in stage 13.0 (TID 134)
[2025-05-02T02:39:41.359+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 133) in 12 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T02:39:41.367+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_30 locally
[2025-05-02T02:39:41.368+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 30.0 in stage 13.0 (TID 134). 4181 bytes result sent to driver
[2025-05-02T02:39:41.369+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 135) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.370+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 134) in 13 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T02:39:41.371+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 31.0 in stage 13.0 (TID 135)
[2025-05-02T02:39:41.383+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_31 locally
[2025-05-02T02:39:41.385+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 31.0 in stage 13.0 (TID 135). 4181 bytes result sent to driver
[2025-05-02T02:39:41.386+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 136) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.387+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 135) in 17 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T02:39:41.388+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 32.0 in stage 13.0 (TID 136)
[2025-05-02T02:39:41.398+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_32 locally
[2025-05-02T02:39:41.400+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 32.0 in stage 13.0 (TID 136). 4181 bytes result sent to driver
[2025-05-02T02:39:41.401+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 137) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.402+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 33.0 in stage 13.0 (TID 137)
[2025-05-02T02:39:41.402+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 136) in 15 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T02:39:41.410+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_33 locally
[2025-05-02T02:39:41.412+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 33.0 in stage 13.0 (TID 137). 4181 bytes result sent to driver
[2025-05-02T02:39:41.412+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 138) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.413+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 34.0 in stage 13.0 (TID 138)
[2025-05-02T02:39:41.414+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 137) in 13 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T02:39:41.422+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_34 locally
[2025-05-02T02:39:41.423+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 34.0 in stage 13.0 (TID 138). 4181 bytes result sent to driver
[2025-05-02T02:39:41.424+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 139) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.425+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 35.0 in stage 13.0 (TID 139)
[2025-05-02T02:39:41.425+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 138) in 12 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T02:39:41.433+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_35 locally
[2025-05-02T02:39:41.435+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 35.0 in stage 13.0 (TID 139). 4181 bytes result sent to driver
[2025-05-02T02:39:41.436+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 140) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.436+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 36.0 in stage 13.0 (TID 140)
[2025-05-02T02:39:41.437+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 139) in 12 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T02:39:41.445+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_36 locally
[2025-05-02T02:39:41.447+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 36.0 in stage 13.0 (TID 140). 4181 bytes result sent to driver
[2025-05-02T02:39:41.447+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 141) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.448+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 37.0 in stage 13.0 (TID 141)
[2025-05-02T02:39:41.449+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 140) in 13 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T02:39:41.459+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_37 locally
[2025-05-02T02:39:41.461+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 37.0 in stage 13.0 (TID 141). 4181 bytes result sent to driver
[2025-05-02T02:39:41.462+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 142) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.463+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 38.0 in stage 13.0 (TID 142)
[2025-05-02T02:39:41.463+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 141) in 15 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T02:39:41.471+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_38 locally
[2025-05-02T02:39:41.473+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 38.0 in stage 13.0 (TID 142). 4181 bytes result sent to driver
[2025-05-02T02:39:41.474+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 143) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.474+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 39.0 in stage 13.0 (TID 143)
[2025-05-02T02:39:41.475+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 142) in 13 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T02:39:41.482+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_39 locally
[2025-05-02T02:39:41.484+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 39.0 in stage 13.0 (TID 143). 4181 bytes result sent to driver
[2025-05-02T02:39:41.485+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 144) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.485+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 143) in 12 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T02:39:41.486+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 40.0 in stage 13.0 (TID 144)
[2025-05-02T02:39:41.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_40 locally
[2025-05-02T02:39:41.495+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 40.0 in stage 13.0 (TID 144). 4181 bytes result sent to driver
[2025-05-02T02:39:41.496+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 145) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.497+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 41.0 in stage 13.0 (TID 145)
[2025-05-02T02:39:41.498+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 144) in 12 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T02:39:41.505+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_41 locally
[2025-05-02T02:39:41.507+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 41.0 in stage 13.0 (TID 145). 4348 bytes result sent to driver
[2025-05-02T02:39:41.508+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 146) (***-scheduler, executor driver, partition 42, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.509+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 42.0 in stage 13.0 (TID 146)
[2025-05-02T02:39:41.509+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 145) in 12 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T02:39:41.517+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_42 locally
[2025-05-02T02:39:41.518+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 42.0 in stage 13.0 (TID 146). 4224 bytes result sent to driver
[2025-05-02T02:39:41.524+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 147) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.525+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 146) in 17 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T02:39:41.526+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 43.0 in stage 13.0 (TID 147)
[2025-05-02T02:39:41.533+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_43 locally
[2025-05-02T02:39:41.534+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 43.0 in stage 13.0 (TID 147). 4181 bytes result sent to driver
[2025-05-02T02:39:41.535+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 148) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.536+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 44.0 in stage 13.0 (TID 148)
[2025-05-02T02:39:41.537+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 147) in 12 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T02:39:41.544+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_44 locally
[2025-05-02T02:39:41.546+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 44.0 in stage 13.0 (TID 148). 4181 bytes result sent to driver
[2025-05-02T02:39:41.547+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 149) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.548+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 45.0 in stage 13.0 (TID 149)
[2025-05-02T02:39:41.548+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 148) in 12 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T02:39:41.556+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_45 locally
[2025-05-02T02:39:41.557+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 45.0 in stage 13.0 (TID 149). 4181 bytes result sent to driver
[2025-05-02T02:39:41.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 150) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.559+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 149) in 12 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T02:39:41.559+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 46.0 in stage 13.0 (TID 150)
[2025-05-02T02:39:41.567+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_46 locally
[2025-05-02T02:39:41.568+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 46.0 in stage 13.0 (TID 150). 4181 bytes result sent to driver
[2025-05-02T02:39:41.569+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 151) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.570+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 47.0 in stage 13.0 (TID 151)
[2025-05-02T02:39:41.571+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 150) in 12 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T02:39:41.581+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_47 locally
[2025-05-02T02:39:41.583+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 47.0 in stage 13.0 (TID 151). 4181 bytes result sent to driver
[2025-05-02T02:39:41.583+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 152) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.584+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 151) in 16 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T02:39:41.585+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 48.0 in stage 13.0 (TID 152)
[2025-05-02T02:39:41.598+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_48 locally
[2025-05-02T02:39:41.600+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 48.0 in stage 13.0 (TID 152). 4224 bytes result sent to driver
[2025-05-02T02:39:41.601+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 153) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:41.602+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 152) in 18 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T02:39:41.602+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Running task 49.0 in stage 13.0 (TID 153)
[2025-05-02T02:39:41.610+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO BlockManager: Found block rdd_27_49 locally
[2025-05-02T02:39:41.612+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO Executor: Finished task 49.0 in stage 13.0 (TID 153). 4181 bytes result sent to driver
[2025-05-02T02:39:41.613+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 153) in 13 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T02:39:41.614+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-05-02T02:39:41.614+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO DAGScheduler: ResultStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.763 s
[2025-05-02T02:39:41.615+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:41.616+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-05-02T02:39:41.616+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO DAGScheduler: Job 6 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.767464 s
[2025-05-02T02:39:41.632+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO CodeGenerator: Code generated in 12.905625 ms
[2025-05-02T02:39:41.634+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO PrepareDeltaScan: DELTA: Done
[2025-05-02T02:39:41.723+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO PrepareDeltaScan: DELTA: Filtering files for query
[2025-05-02T02:39:41.970+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:41 INFO CodeGenerator: Code generated in 97.985045 ms
[2025-05-02T02:39:42.105+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO CodeGenerator: Code generated in 28.06806 ms
[2025-05-02T02:39:42.141+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:42.143+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Got job 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2025-05-02T02:39:42.144+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Final stage: ResultStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:42.144+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
[2025-05-02T02:39:42.145+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:42.146+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[41] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:42.152+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 739.3 KiB, free 431.9 MiB)
[2025-05-02T02:39:42.155+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 169.6 KiB, free 431.7 MiB)
[2025-05-02T02:39:42.156+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ***-scheduler:43081 (size: 169.6 KiB, free: 433.9 MiB)
[2025-05-02T02:39:42.156+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:42.157+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 15 (MapPartitionsRDD[41] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-05-02T02:39:42.158+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSchedulerImpl: Adding task set 15.0 with 50 tasks resource profile 0
[2025-05-02T02:39:42.159+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 154) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.160+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 0.0 in stage 15.0 (TID 154)
[2025-05-02T02:39:42.180+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_0 locally
[2025-05-02T02:39:42.277+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO CodeGenerator: Code generated in 96.429375 ms
[2025-05-02T02:39:42.313+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO CodeGenerator: Code generated in 21.545353 ms
[2025-05-02T02:39:42.315+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_0 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.316+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_0 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO CodeGenerator: Code generated in 40.78448 ms
[2025-05-02T02:39:42.361+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 0.0 in stage 15.0 (TID 154). 4666 bytes result sent to driver
[2025-05-02T02:39:42.362+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 155) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.363+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 154) in 204 ms on ***-scheduler (executor driver) (1/50)
[2025-05-02T02:39:42.364+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 1.0 in stage 15.0 (TID 155)
[2025-05-02T02:39:42.373+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_1 locally
[2025-05-02T02:39:42.378+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_1 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.379+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_1 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.381+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 1.0 in stage 15.0 (TID 155). 4666 bytes result sent to driver
[2025-05-02T02:39:42.382+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 156) (***-scheduler, executor driver, partition 2, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.383+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 2.0 in stage 15.0 (TID 156)
[2025-05-02T02:39:42.384+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 155) in 20 ms on ***-scheduler (executor driver) (2/50)
[2025-05-02T02:39:42.393+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_2 locally
[2025-05-02T02:39:42.397+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_2 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.398+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_2 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.399+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 2.0 in stage 15.0 (TID 156). 4666 bytes result sent to driver
[2025-05-02T02:39:42.400+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 157) (***-scheduler, executor driver, partition 3, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.401+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 156) in 18 ms on ***-scheduler (executor driver) (3/50)
[2025-05-02T02:39:42.402+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 3.0 in stage 15.0 (TID 157)
[2025-05-02T02:39:42.423+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_3 locally
[2025-05-02T02:39:42.431+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_3 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.432+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_3 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.433+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 3.0 in stage 15.0 (TID 157). 4709 bytes result sent to driver
[2025-05-02T02:39:42.434+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 158) (***-scheduler, executor driver, partition 4, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.435+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 157) in 35 ms on ***-scheduler (executor driver) (4/50)
[2025-05-02T02:39:42.436+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 4.0 in stage 15.0 (TID 158)
[2025-05-02T02:39:42.448+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_4 locally
[2025-05-02T02:39:42.456+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_4 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.457+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_4 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.458+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 4.0 in stage 15.0 (TID 158). 4666 bytes result sent to driver
[2025-05-02T02:39:42.459+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 159) (***-scheduler, executor driver, partition 5, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.460+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 5.0 in stage 15.0 (TID 159)
[2025-05-02T02:39:42.461+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 158) in 26 ms on ***-scheduler (executor driver) (5/50)
[2025-05-02T02:39:42.471+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_5 locally
[2025-05-02T02:39:42.476+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_5 stored as values in memory (estimated size 46.0 B, free 431.7 MiB)
[2025-05-02T02:39:42.477+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_5 in memory on ***-scheduler:43081 (size: 46.0 B, free: 433.9 MiB)
[2025-05-02T02:39:42.478+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 5.0 in stage 15.0 (TID 159). 4666 bytes result sent to driver
[2025-05-02T02:39:42.479+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 6.0 in stage 15.0 (TID 160) (***-scheduler, executor driver, partition 6, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.480+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 159) in 21 ms on ***-scheduler (executor driver) (6/50)
[2025-05-02T02:39:42.481+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 6.0 in stage 15.0 (TID 160)
[2025-05-02T02:39:42.499+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_6 locally
[2025-05-02T02:39:42.500+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ***-scheduler:43081 in memory (size: 155.4 KiB, free: 434.1 MiB)
[2025-05-02T02:39:42.505+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_6 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.507+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_6 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.508+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 6.0 in stage 15.0 (TID 160). 4666 bytes result sent to driver
[2025-05-02T02:39:42.509+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 7.0 in stage 15.0 (TID 161) (***-scheduler, executor driver, partition 7, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.510+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 7.0 in stage 15.0 (TID 161)
[2025-05-02T02:39:42.510+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 6.0 in stage 15.0 (TID 160) in 30 ms on ***-scheduler (executor driver) (7/50)
[2025-05-02T02:39:42.523+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_7 locally
[2025-05-02T02:39:42.531+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_7 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.532+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_7 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.534+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 7.0 in stage 15.0 (TID 161). 4666 bytes result sent to driver
[2025-05-02T02:39:42.535+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 8.0 in stage 15.0 (TID 162) (***-scheduler, executor driver, partition 8, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.535+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 8.0 in stage 15.0 (TID 162)
[2025-05-02T02:39:42.536+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 7.0 in stage 15.0 (TID 161) in 27 ms on ***-scheduler (executor driver) (8/50)
[2025-05-02T02:39:42.550+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_8 locally
[2025-05-02T02:39:42.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_8 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_8 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.561+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 8.0 in stage 15.0 (TID 162). 4666 bytes result sent to driver
[2025-05-02T02:39:42.561+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 9.0 in stage 15.0 (TID 163) (***-scheduler, executor driver, partition 9, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.562+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 8.0 in stage 15.0 (TID 162) in 28 ms on ***-scheduler (executor driver) (9/50)
[2025-05-02T02:39:42.563+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 9.0 in stage 15.0 (TID 163)
[2025-05-02T02:39:42.573+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_9 locally
[2025-05-02T02:39:42.578+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_9 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.578+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_9 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.580+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 9.0 in stage 15.0 (TID 163). 4666 bytes result sent to driver
[2025-05-02T02:39:42.581+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 10.0 in stage 15.0 (TID 164) (***-scheduler, executor driver, partition 10, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.582+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 9.0 in stage 15.0 (TID 163) in 20 ms on ***-scheduler (executor driver) (10/50)
[2025-05-02T02:39:42.582+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 10.0 in stage 15.0 (TID 164)
[2025-05-02T02:39:42.592+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_10 locally
[2025-05-02T02:39:42.597+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_10 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.597+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_10 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.599+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 10.0 in stage 15.0 (TID 164). 4666 bytes result sent to driver
[2025-05-02T02:39:42.600+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 11.0 in stage 15.0 (TID 165) (***-scheduler, executor driver, partition 11, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.601+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 11.0 in stage 15.0 (TID 165)
[2025-05-02T02:39:42.601+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 10.0 in stage 15.0 (TID 164) in 19 ms on ***-scheduler (executor driver) (11/50)
[2025-05-02T02:39:42.610+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_11 locally
[2025-05-02T02:39:42.615+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_11 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.616+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_11 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.617+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 11.0 in stage 15.0 (TID 165). 4666 bytes result sent to driver
[2025-05-02T02:39:42.618+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 12.0 in stage 15.0 (TID 166) (***-scheduler, executor driver, partition 12, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.618+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 12.0 in stage 15.0 (TID 166)
[2025-05-02T02:39:42.619+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 11.0 in stage 15.0 (TID 165) in 19 ms on ***-scheduler (executor driver) (12/50)
[2025-05-02T02:39:42.628+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_12 locally
[2025-05-02T02:39:42.633+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_12 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.634+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_12 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.635+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 12.0 in stage 15.0 (TID 166). 4666 bytes result sent to driver
[2025-05-02T02:39:42.636+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 13.0 in stage 15.0 (TID 167) (***-scheduler, executor driver, partition 13, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.637+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 13.0 in stage 15.0 (TID 167)
[2025-05-02T02:39:42.638+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 12.0 in stage 15.0 (TID 166) in 20 ms on ***-scheduler (executor driver) (13/50)
[2025-05-02T02:39:42.647+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_13 locally
[2025-05-02T02:39:42.652+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_13 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.653+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_13 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.654+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 13.0 in stage 15.0 (TID 167). 4666 bytes result sent to driver
[2025-05-02T02:39:42.655+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 14.0 in stage 15.0 (TID 168) (***-scheduler, executor driver, partition 14, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.656+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 14.0 in stage 15.0 (TID 168)
[2025-05-02T02:39:42.656+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 13.0 in stage 15.0 (TID 167) in 19 ms on ***-scheduler (executor driver) (14/50)
[2025-05-02T02:39:42.666+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_14 locally
[2025-05-02T02:39:42.670+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_14 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.671+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_14 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.673+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 14.0 in stage 15.0 (TID 168). 4666 bytes result sent to driver
[2025-05-02T02:39:42.673+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 15.0 in stage 15.0 (TID 169) (***-scheduler, executor driver, partition 15, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.674+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 14.0 in stage 15.0 (TID 168) in 19 ms on ***-scheduler (executor driver) (15/50)
[2025-05-02T02:39:42.675+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 15.0 in stage 15.0 (TID 169)
[2025-05-02T02:39:42.685+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_15 locally
[2025-05-02T02:39:42.689+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_15 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.690+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_15 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.692+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 15.0 in stage 15.0 (TID 169). 4666 bytes result sent to driver
[2025-05-02T02:39:42.692+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 16.0 in stage 15.0 (TID 170) (***-scheduler, executor driver, partition 16, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.693+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 15.0 in stage 15.0 (TID 169) in 20 ms on ***-scheduler (executor driver) (16/50)
[2025-05-02T02:39:42.694+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 16.0 in stage 15.0 (TID 170)
[2025-05-02T02:39:42.703+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_16 locally
[2025-05-02T02:39:42.708+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_16 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.709+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_16 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.710+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 16.0 in stage 15.0 (TID 170). 4666 bytes result sent to driver
[2025-05-02T02:39:42.711+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 17.0 in stage 15.0 (TID 171) (***-scheduler, executor driver, partition 17, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.712+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 17.0 in stage 15.0 (TID 171)
[2025-05-02T02:39:42.712+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 16.0 in stage 15.0 (TID 170) in 19 ms on ***-scheduler (executor driver) (17/50)
[2025-05-02T02:39:42.722+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_17 locally
[2025-05-02T02:39:42.727+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_17 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.728+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_17 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.729+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 17.0 in stage 15.0 (TID 171). 4666 bytes result sent to driver
[2025-05-02T02:39:42.730+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 18.0 in stage 15.0 (TID 172) (***-scheduler, executor driver, partition 18, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.731+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 18.0 in stage 15.0 (TID 172)
[2025-05-02T02:39:42.732+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 17.0 in stage 15.0 (TID 171) in 20 ms on ***-scheduler (executor driver) (18/50)
[2025-05-02T02:39:42.741+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_18 locally
[2025-05-02T02:39:42.746+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_18 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.747+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_18 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.748+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 18.0 in stage 15.0 (TID 172). 4666 bytes result sent to driver
[2025-05-02T02:39:42.748+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 19.0 in stage 15.0 (TID 173) (***-scheduler, executor driver, partition 19, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.749+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 19.0 in stage 15.0 (TID 173)
[2025-05-02T02:39:42.750+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 18.0 in stage 15.0 (TID 172) in 19 ms on ***-scheduler (executor driver) (19/50)
[2025-05-02T02:39:42.759+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_19 locally
[2025-05-02T02:39:42.763+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_19 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.764+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_19 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.766+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 19.0 in stage 15.0 (TID 173). 4666 bytes result sent to driver
[2025-05-02T02:39:42.767+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 20.0 in stage 15.0 (TID 174) (***-scheduler, executor driver, partition 20, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.768+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 20.0 in stage 15.0 (TID 174)
[2025-05-02T02:39:42.769+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 19.0 in stage 15.0 (TID 173) in 20 ms on ***-scheduler (executor driver) (20/50)
[2025-05-02T02:39:42.778+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_20 locally
[2025-05-02T02:39:42.783+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_20 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.784+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_20 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.785+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 20.0 in stage 15.0 (TID 174). 4666 bytes result sent to driver
[2025-05-02T02:39:42.786+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 21.0 in stage 15.0 (TID 175) (***-scheduler, executor driver, partition 21, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.787+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 21.0 in stage 15.0 (TID 175)
[2025-05-02T02:39:42.788+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 20.0 in stage 15.0 (TID 174) in 20 ms on ***-scheduler (executor driver) (21/50)
[2025-05-02T02:39:42.797+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_21 locally
[2025-05-02T02:39:42.801+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_21 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.802+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_21 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.803+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 21.0 in stage 15.0 (TID 175). 4666 bytes result sent to driver
[2025-05-02T02:39:42.804+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 22.0 in stage 15.0 (TID 176) (***-scheduler, executor driver, partition 22, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.805+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 22.0 in stage 15.0 (TID 176)
[2025-05-02T02:39:42.806+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 21.0 in stage 15.0 (TID 175) in 19 ms on ***-scheduler (executor driver) (22/50)
[2025-05-02T02:39:42.815+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_22 locally
[2025-05-02T02:39:42.819+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_22 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.820+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_22 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.821+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 22.0 in stage 15.0 (TID 176). 4666 bytes result sent to driver
[2025-05-02T02:39:42.822+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 23.0 in stage 15.0 (TID 177) (***-scheduler, executor driver, partition 23, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.823+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 22.0 in stage 15.0 (TID 176) in 18 ms on ***-scheduler (executor driver) (23/50)
[2025-05-02T02:39:42.824+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 23.0 in stage 15.0 (TID 177)
[2025-05-02T02:39:42.833+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_23 locally
[2025-05-02T02:39:42.838+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_23 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.839+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_23 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.840+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 23.0 in stage 15.0 (TID 177). 4666 bytes result sent to driver
[2025-05-02T02:39:42.841+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 24.0 in stage 15.0 (TID 178) (***-scheduler, executor driver, partition 24, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.842+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 24.0 in stage 15.0 (TID 178)
[2025-05-02T02:39:42.842+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 23.0 in stage 15.0 (TID 177) in 19 ms on ***-scheduler (executor driver) (24/50)
[2025-05-02T02:39:42.855+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_24 locally
[2025-05-02T02:39:42.860+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_24 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.861+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_24 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.863+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 24.0 in stage 15.0 (TID 178). 4666 bytes result sent to driver
[2025-05-02T02:39:42.864+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 25.0 in stage 15.0 (TID 179) (***-scheduler, executor driver, partition 25, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.864+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 24.0 in stage 15.0 (TID 178) in 24 ms on ***-scheduler (executor driver) (25/50)
[2025-05-02T02:39:42.865+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 25.0 in stage 15.0 (TID 179)
[2025-05-02T02:39:42.880+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_25 locally
[2025-05-02T02:39:42.885+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_25 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.886+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_25 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.888+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 25.0 in stage 15.0 (TID 179). 4709 bytes result sent to driver
[2025-05-02T02:39:42.889+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 26.0 in stage 15.0 (TID 180) (***-scheduler, executor driver, partition 26, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.890+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 26.0 in stage 15.0 (TID 180)
[2025-05-02T02:39:42.890+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 25.0 in stage 15.0 (TID 179) in 26 ms on ***-scheduler (executor driver) (26/50)
[2025-05-02T02:39:42.900+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_26 locally
[2025-05-02T02:39:42.905+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_26 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.906+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_26 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.907+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 26.0 in stage 15.0 (TID 180). 4666 bytes result sent to driver
[2025-05-02T02:39:42.908+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 27.0 in stage 15.0 (TID 181) (***-scheduler, executor driver, partition 27, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.908+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 27.0 in stage 15.0 (TID 181)
[2025-05-02T02:39:42.909+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 26.0 in stage 15.0 (TID 180) in 20 ms on ***-scheduler (executor driver) (27/50)
[2025-05-02T02:39:42.918+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_27 locally
[2025-05-02T02:39:42.923+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_27 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.924+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_27 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.925+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 27.0 in stage 15.0 (TID 181). 4666 bytes result sent to driver
[2025-05-02T02:39:42.926+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 28.0 in stage 15.0 (TID 182) (***-scheduler, executor driver, partition 28, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.927+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 27.0 in stage 15.0 (TID 181) in 20 ms on ***-scheduler (executor driver) (28/50)
[2025-05-02T02:39:42.928+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 28.0 in stage 15.0 (TID 182)
[2025-05-02T02:39:42.946+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_28 locally
[2025-05-02T02:39:42.951+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_28 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.952+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_28 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.953+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 28.0 in stage 15.0 (TID 182). 4709 bytes result sent to driver
[2025-05-02T02:39:42.955+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 29.0 in stage 15.0 (TID 183) (***-scheduler, executor driver, partition 29, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.955+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 29.0 in stage 15.0 (TID 183)
[2025-05-02T02:39:42.956+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 28.0 in stage 15.0 (TID 182) in 29 ms on ***-scheduler (executor driver) (29/50)
[2025-05-02T02:39:42.965+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_29 locally
[2025-05-02T02:39:42.970+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_29 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.971+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_29 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.973+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 29.0 in stage 15.0 (TID 183). 4666 bytes result sent to driver
[2025-05-02T02:39:42.973+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 30.0 in stage 15.0 (TID 184) (***-scheduler, executor driver, partition 30, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.974+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 29.0 in stage 15.0 (TID 183) in 20 ms on ***-scheduler (executor driver) (30/50)
[2025-05-02T02:39:42.975+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 30.0 in stage 15.0 (TID 184)
[2025-05-02T02:39:42.984+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManager: Found block rdd_10_30 locally
[2025-05-02T02:39:42.989+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO MemoryStore: Block rdd_39_30 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:42.990+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO BlockManagerInfo: Added rdd_39_30 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:42.991+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Finished task 30.0 in stage 15.0 (TID 184). 4666 bytes result sent to driver
[2025-05-02T02:39:42.991+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Starting task 31.0 in stage 15.0 (TID 185) (***-scheduler, executor driver, partition 31, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:42.992+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO TaskSetManager: Finished task 30.0 in stage 15.0 (TID 184) in 19 ms on ***-scheduler (executor driver) (31/50)
[2025-05-02T02:39:42.993+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:42 INFO Executor: Running task 31.0 in stage 15.0 (TID 185)
[2025-05-02T02:39:43.006+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_31 locally
[2025-05-02T02:39:43.011+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_31 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.012+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_31 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.013+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 31.0 in stage 15.0 (TID 185). 4666 bytes result sent to driver
[2025-05-02T02:39:43.014+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 32.0 in stage 15.0 (TID 186) (***-scheduler, executor driver, partition 32, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.015+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 32.0 in stage 15.0 (TID 186)
[2025-05-02T02:39:43.016+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 31.0 in stage 15.0 (TID 185) in 24 ms on ***-scheduler (executor driver) (32/50)
[2025-05-02T02:39:43.025+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_32 locally
[2025-05-02T02:39:43.029+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_32 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.032+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_32 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.033+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 32.0 in stage 15.0 (TID 186). 4666 bytes result sent to driver
[2025-05-02T02:39:43.034+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 33.0 in stage 15.0 (TID 187) (***-scheduler, executor driver, partition 33, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.034+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 33.0 in stage 15.0 (TID 187)
[2025-05-02T02:39:43.035+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 32.0 in stage 15.0 (TID 186) in 20 ms on ***-scheduler (executor driver) (33/50)
[2025-05-02T02:39:43.047+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_33 locally
[2025-05-02T02:39:43.053+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_33 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.053+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_33 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.055+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 33.0 in stage 15.0 (TID 187). 4666 bytes result sent to driver
[2025-05-02T02:39:43.056+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 34.0 in stage 15.0 (TID 188) (***-scheduler, executor driver, partition 34, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.057+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 33.0 in stage 15.0 (TID 187) in 23 ms on ***-scheduler (executor driver) (34/50)
[2025-05-02T02:39:43.058+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 34.0 in stage 15.0 (TID 188)
[2025-05-02T02:39:43.068+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_34 locally
[2025-05-02T02:39:43.072+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_34 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.073+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_34 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.075+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 34.0 in stage 15.0 (TID 188). 4666 bytes result sent to driver
[2025-05-02T02:39:43.076+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 35.0 in stage 15.0 (TID 189) (***-scheduler, executor driver, partition 35, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.077+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 34.0 in stage 15.0 (TID 188) in 21 ms on ***-scheduler (executor driver) (35/50)
[2025-05-02T02:39:43.077+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 35.0 in stage 15.0 (TID 189)
[2025-05-02T02:39:43.087+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_35 locally
[2025-05-02T02:39:43.091+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_35 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.092+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_35 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.094+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 35.0 in stage 15.0 (TID 189). 4666 bytes result sent to driver
[2025-05-02T02:39:43.094+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 36.0 in stage 15.0 (TID 190) (***-scheduler, executor driver, partition 36, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.095+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 35.0 in stage 15.0 (TID 189) in 20 ms on ***-scheduler (executor driver) (36/50)
[2025-05-02T02:39:43.096+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 36.0 in stage 15.0 (TID 190)
[2025-05-02T02:39:43.105+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_36 locally
[2025-05-02T02:39:43.110+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_36 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.110+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_36 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.112+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 36.0 in stage 15.0 (TID 190). 4666 bytes result sent to driver
[2025-05-02T02:39:43.113+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 37.0 in stage 15.0 (TID 191) (***-scheduler, executor driver, partition 37, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.113+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 37.0 in stage 15.0 (TID 191)
[2025-05-02T02:39:43.114+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 36.0 in stage 15.0 (TID 190) in 19 ms on ***-scheduler (executor driver) (37/50)
[2025-05-02T02:39:43.124+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_37 locally
[2025-05-02T02:39:43.129+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_37 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.130+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_37 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.131+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 37.0 in stage 15.0 (TID 191). 4666 bytes result sent to driver
[2025-05-02T02:39:43.132+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 38.0 in stage 15.0 (TID 192) (***-scheduler, executor driver, partition 38, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.133+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 38.0 in stage 15.0 (TID 192)
[2025-05-02T02:39:43.134+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 37.0 in stage 15.0 (TID 191) in 20 ms on ***-scheduler (executor driver) (38/50)
[2025-05-02T02:39:43.142+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_38 locally
[2025-05-02T02:39:43.147+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_38 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.148+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_38 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.149+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 38.0 in stage 15.0 (TID 192). 4666 bytes result sent to driver
[2025-05-02T02:39:43.150+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 39.0 in stage 15.0 (TID 193) (***-scheduler, executor driver, partition 39, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.151+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 39.0 in stage 15.0 (TID 193)
[2025-05-02T02:39:43.152+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 38.0 in stage 15.0 (TID 192) in 20 ms on ***-scheduler (executor driver) (39/50)
[2025-05-02T02:39:43.161+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_39 locally
[2025-05-02T02:39:43.165+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_39 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.166+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_39 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.167+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 39.0 in stage 15.0 (TID 193). 4666 bytes result sent to driver
[2025-05-02T02:39:43.168+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 40.0 in stage 15.0 (TID 194) (***-scheduler, executor driver, partition 40, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.169+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 40.0 in stage 15.0 (TID 194)
[2025-05-02T02:39:43.170+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 39.0 in stage 15.0 (TID 193) in 18 ms on ***-scheduler (executor driver) (40/50)
[2025-05-02T02:39:43.179+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_40 locally
[2025-05-02T02:39:43.183+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_40 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.184+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_40 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.186+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 40.0 in stage 15.0 (TID 194). 4666 bytes result sent to driver
[2025-05-02T02:39:43.187+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 41.0 in stage 15.0 (TID 195) (***-scheduler, executor driver, partition 41, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.188+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 41.0 in stage 15.0 (TID 195)
[2025-05-02T02:39:43.189+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 40.0 in stage 15.0 (TID 194) in 19 ms on ***-scheduler (executor driver) (41/50)
[2025-05-02T02:39:43.201+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_41 locally
[2025-05-02T02:39:43.206+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_41 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.207+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_41 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.208+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 41.0 in stage 15.0 (TID 195). 4666 bytes result sent to driver
[2025-05-02T02:39:43.209+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 42.0 in stage 15.0 (TID 196) (***-scheduler, executor driver, partition 42, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.210+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 41.0 in stage 15.0 (TID 195) in 22 ms on ***-scheduler (executor driver) (42/50)
[2025-05-02T02:39:43.211+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 42.0 in stage 15.0 (TID 196)
[2025-05-02T02:39:43.220+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_42 locally
[2025-05-02T02:39:43.225+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_42 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.226+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_42 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.227+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 42.0 in stage 15.0 (TID 196). 4709 bytes result sent to driver
[2025-05-02T02:39:43.228+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 43.0 in stage 15.0 (TID 197) (***-scheduler, executor driver, partition 43, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.229+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 43.0 in stage 15.0 (TID 197)
[2025-05-02T02:39:43.230+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 42.0 in stage 15.0 (TID 196) in 21 ms on ***-scheduler (executor driver) (43/50)
[2025-05-02T02:39:43.239+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_43 locally
[2025-05-02T02:39:43.250+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_43 stored as values in memory (estimated size 745.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.251+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_43 in memory on ***-scheduler:43081 (size: 745.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.258+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO CodeGenerator: Code generated in 4.854695 ms
[2025-05-02T02:39:43.265+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 43.0 in stage 15.0 (TID 197). 4834 bytes result sent to driver
[2025-05-02T02:39:43.266+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 44.0 in stage 15.0 (TID 198) (***-scheduler, executor driver, partition 44, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.267+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 43.0 in stage 15.0 (TID 197) in 38 ms on ***-scheduler (executor driver) (44/50)
[2025-05-02T02:39:43.268+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 44.0 in stage 15.0 (TID 198)
[2025-05-02T02:39:43.277+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_44 locally
[2025-05-02T02:39:43.282+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_44 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.283+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_44 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.285+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 44.0 in stage 15.0 (TID 198). 4666 bytes result sent to driver
[2025-05-02T02:39:43.285+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 45.0 in stage 15.0 (TID 199) (***-scheduler, executor driver, partition 45, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.286+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 44.0 in stage 15.0 (TID 198) in 21 ms on ***-scheduler (executor driver) (45/50)
[2025-05-02T02:39:43.287+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 45.0 in stage 15.0 (TID 199)
[2025-05-02T02:39:43.296+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_45 locally
[2025-05-02T02:39:43.301+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_45 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.302+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_45 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.303+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 45.0 in stage 15.0 (TID 199). 4666 bytes result sent to driver
[2025-05-02T02:39:43.304+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 46.0 in stage 15.0 (TID 200) (***-scheduler, executor driver, partition 46, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.305+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 46.0 in stage 15.0 (TID 200)
[2025-05-02T02:39:43.306+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 45.0 in stage 15.0 (TID 199) in 19 ms on ***-scheduler (executor driver) (46/50)
[2025-05-02T02:39:43.314+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_46 locally
[2025-05-02T02:39:43.319+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_46 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.320+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_46 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.321+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 46.0 in stage 15.0 (TID 200). 4666 bytes result sent to driver
[2025-05-02T02:39:43.322+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 47.0 in stage 15.0 (TID 201) (***-scheduler, executor driver, partition 47, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.323+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 47.0 in stage 15.0 (TID 201)
[2025-05-02T02:39:43.323+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 46.0 in stage 15.0 (TID 200) in 18 ms on ***-scheduler (executor driver) (47/50)
[2025-05-02T02:39:43.332+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_47 locally
[2025-05-02T02:39:43.337+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_47 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.338+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_47 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.340+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 47.0 in stage 15.0 (TID 201). 4666 bytes result sent to driver
[2025-05-02T02:39:43.341+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 48.0 in stage 15.0 (TID 202) (***-scheduler, executor driver, partition 48, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.341+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 48.0 in stage 15.0 (TID 202)
[2025-05-02T02:39:43.342+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 47.0 in stage 15.0 (TID 201) in 20 ms on ***-scheduler (executor driver) (48/50)
[2025-05-02T02:39:43.351+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_48 locally
[2025-05-02T02:39:43.356+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_48 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_48 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.358+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 48.0 in stage 15.0 (TID 202). 4666 bytes result sent to driver
[2025-05-02T02:39:43.359+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Starting task 49.0 in stage 15.0 (TID 203) (***-scheduler, executor driver, partition 49, PROCESS_LOCAL, 10206 bytes)
[2025-05-02T02:39:43.360+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Running task 49.0 in stage 15.0 (TID 203)
[2025-05-02T02:39:43.360+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 48.0 in stage 15.0 (TID 202) in 19 ms on ***-scheduler (executor driver) (49/50)
[2025-05-02T02:39:43.369+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManager: Found block rdd_10_49 locally
[2025-05-02T02:39:43.374+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO MemoryStore: Block rdd_39_49 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)
[2025-05-02T02:39:43.375+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO BlockManagerInfo: Added rdd_39_49 in memory on ***-scheduler:43081 (size: 46.0 B, free: 434.1 MiB)
[2025-05-02T02:39:43.376+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO Executor: Finished task 49.0 in stage 15.0 (TID 203). 4666 bytes result sent to driver
[2025-05-02T02:39:43.377+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSetManager: Finished task 49.0 in stage 15.0 (TID 203) in 19 ms on ***-scheduler (executor driver) (50/50)
[2025-05-02T02:39:43.377+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-05-02T02:39:43.378+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO DAGScheduler: ResultStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.230 s
[2025-05-02T02:39:43.379+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:43.379+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-05-02T02:39:43.380+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO DAGScheduler: Job 7 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.236814 s
[2025-05-02T02:39:43.385+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO PrepareDeltaScan: DELTA: Done
[2025-05-02T02:39:43.416+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T02:39:43.417+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T02:39:43.422+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO FileSourceStrategy: Pushed Filters: IsNotNull(ID)
[2025-05-02T02:39:43.423+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:43 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(ID#28)
[2025-05-02T02:39:36.190+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-05-02T02:39:36.249+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO CodeGenerator: Code generated in 13.156492 ms
[2025-05-02T02:39:36.259+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 207.4 KiB, free 432.4 MiB)
[2025-05-02T02:39:36.276+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 432.3 MiB)
[2025-05-02T02:39:36.277+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ***-scheduler:43081 (size: 36.6 KiB, free: 434.0 MiB)
[2025-05-02T02:39:36.278+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO SparkContext: Created broadcast 12 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T02:39:36.286+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4259725 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T02:39:36.330+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T02:39:36.332+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2025-05-02T02:39:36.333+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Final stage: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2025-05-02T02:39:36.333+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T02:39:36.334+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:36.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[46] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2025-05-02T02:39:36.336+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 22.8 KiB, free 432.3 MiB)
[2025-05-02T02:39:36.337+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 432.3 MiB)
[2025-05-02T02:39:36.338+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ***-scheduler:43081 (size: 8.6 KiB, free: 434.0 MiB)
[2025-05-02T02:39:36.338+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:36.339+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[46] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:36.339+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-05-02T02:39:36.340+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 204) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10923 bytes)
[2025-05-02T02:39:36.341+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO Executor: Running task 0.0 in stage 16.0 (TID 204)
[2025-05-02T02:39:36.357+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ***-scheduler:43081 in memory (size: 169.6 KiB, free: 434.2 MiB)
[2025-05-02T02:39:36.375+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO CodeGenerator: Code generated in 8.227914 ms
[2025-05-02T02:39:36.377+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:36 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_organizations/part-00000-a823e957-b8d1-43e7-9534-9cb7d1c56692-c000.snappy.parquet, range: 0-65421, partition values: [empty row]
[2025-05-02T02:39:46.566+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:46 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-02T02:39:46.945+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:46 INFO FilterCompat: Filtering using predicate: noteq(ID, null)
[2025-05-02T02:39:47.029+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-02T02:39:47.108+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-05-02T02:39:47.291+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO Executor: Finished task 0.0 in stage 16.0 (TID 204). 39683 bytes result sent to driver
[2025-05-02T02:39:47.292+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 204) in 10953 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:47.293+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-05-02T02:39:47.294+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: ResultStage 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 10.960 s
[2025-05-02T02:39:47.295+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:47.296+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-05-02T02:39:47.296+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.874202 s
[2025-05-02T02:39:47.311+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 3.924835 ms
[2025-05-02T02:39:47.319+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 16.0 MiB, free 417.2 MiB)
[2025-05-02T02:39:47.323+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 417.1 MiB)
[2025-05-02T02:39:47.324+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ***-scheduler:43081 (size: 40.2 KiB, free: 434.2 MiB)
[2025-05-02T02:39:47.325+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2025-05-02T02:39:47.334+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO FileSourceStrategy: Pushed Filters:
[2025-05-02T02:39:47.335+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO FileSourceStrategy: Post-Scan Filters:
[2025-05-02T02:39:47.446+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 64.192899 ms
[2025-05-02T02:39:47.450+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 207.8 KiB, free 416.9 MiB)
[2025-05-02T02:39:47.459+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 416.9 MiB)
[2025-05-02T02:39:47.460+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ***-scheduler:43081 (size: 36.6 KiB, free: 434.1 MiB)
[2025-05-02T02:39:47.460+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO SparkContext: Created broadcast 15 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:47.462+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4276774 bytes, open cost is considered as scanning 4194304 bytes.
[2025-05-02T02:39:47.487+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Registering RDD 50 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 4
[2025-05-02T02:39:47.488+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Got map stage job 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2025-05-02T02:39:47.488+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Final stage: ShuffleMapStage 17 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:47.489+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Parents of final stage: List()
[2025-05-02T02:39:47.490+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:47.490+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[50] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:47.491+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 91.7 KiB, free 416.8 MiB)
[2025-05-02T02:39:47.491+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 32.2 KiB, free 416.8 MiB)
[2025-05-02T02:39:47.492+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ***-scheduler:43081 (size: 32.2 KiB, free: 434.1 MiB)
[2025-05-02T02:39:47.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:47.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[50] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2025-05-02T02:39:47.494+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-05-02T02:39:47.495+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 205) (***-scheduler, executor driver, partition 0, PROCESS_LOCAL, 10908 bytes)
[2025-05-02T02:39:47.496+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO Executor: Running task 0.0 in stage 17.0 (TID 205)
[2025-05-02T02:39:47.537+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 37.768623 ms
[2025-05-02T02:39:47.549+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 8.59246 ms
[2025-05-02T02:39:47.565+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 7.705194 ms
[2025-05-02T02:39:47.569+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO FileScanRDD: Reading File path: s3a://medical-bucket/enriched/transactional/medical-data-sample/enriched_providers/part-00000-7b4eda75-b20f-405f-a422-b40cd8cf6935-c000.snappy.parquet, range: 0-82470, partition values: [empty row]
[2025-05-02T02:39:47.603+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO S3AInputStream: Switching to Random IO seek policy
[2025-05-02T02:39:47.703+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO Executor: Finished task 0.0 in stage 17.0 (TID 205). 3832 bytes result sent to driver
[2025-05-02T02:39:47.704+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 205) in 211 ms on ***-scheduler (executor driver) (1/1)
[2025-05-02T02:39:47.705+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-05-02T02:39:47.706+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: ShuffleMapStage 17 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.217 s
[2025-05-02T02:39:47.707+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: looking for newly runnable stages
[2025-05-02T02:39:47.708+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: running: Set()
[2025-05-02T02:39:47.708+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: waiting: Set()
[2025-05-02T02:39:47.709+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: failed: Set()
[2025-05-02T02:39:47.715+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-05-02T02:39:47.780+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 31.179277 ms
[2025-05-02T02:39:47.799+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 5.665626 ms
[2025-05-02T02:39:47.861+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:47.866+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Got job 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2025-05-02T02:39:47.867+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2025-05-02T02:39:47.868+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
[2025-05-02T02:39:47.869+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Missing parents: List()
[2025-05-02T02:39:47.869+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Submitting ResultStage 19 (UnionRDD[54] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2025-05-02T02:39:47.902+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 449.9 KiB, free 416.3 MiB)
[2025-05-02T02:39:47.905+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 154.9 KiB, free 416.2 MiB)
[2025-05-02T02:39:47.905+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ***-scheduler:43081 (size: 154.9 KiB, free: 433.9 MiB)
[2025-05-02T02:39:47.906+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-05-02T02:39:47.907+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 19 (UnionRDD[54] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2025-05-02T02:39:47.907+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0
[2025-05-02T02:39:47.909+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 206) (***-scheduler, executor driver, partition 0, NODE_LOCAL, 10315 bytes)
[2025-05-02T02:39:47.909+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO Executor: Running task 0.0 in stage 19.0 (TID 206)
[2025-05-02T02:39:47.947+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO ShuffleBlockFetcherIterator: Getting 1 (87.6 KiB) non-empty blocks including 1 (87.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-05-02T02:39:47.948+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-05-02T02:39:47.977+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:47 INFO CodeGenerator: Code generated in 29.488925 ms
[2025-05-02T02:39:48.003+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodeGenerator: Code generated in 12.590842 ms
[2025-05-02T02:39:48.091+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ***-scheduler:43081 in memory (size: 8.6 KiB, free: 434.0 MiB)
[2025-05-02T02:39:48.095+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ***-scheduler:43081 in memory (size: 32.2 KiB, free: 434.0 MiB)
[2025-05-02T02:39:48.109+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodeGenerator: Code generated in 52.62648 ms
[2025-05-02T02:39:48.120+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodeGenerator: Code generated in 4.747826 ms
[2025-05-02T02:39:48.124+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T02:39:48.127+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T02:39:48.147+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-05-02T02:39:48.159+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-05-02T02:39:48.160+0000] {spark_submit.py:649} INFO - {
[2025-05-02T02:39:48.161+0000] {spark_submit.py:649} INFO - "type" : "struct",
[2025-05-02T02:39:48.162+0000] {spark_submit.py:649} INFO - "fields" : [ {
[2025-05-02T02:39:48.162+0000] {spark_submit.py:649} INFO - "name" : "Provider_Key",
[2025-05-02T02:39:48.163+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.164+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.164+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.165+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.166+0000] {spark_submit.py:649} INFO - "name" : "Provider_Id",
[2025-05-02T02:39:48.167+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.167+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.168+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.168+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.169+0000] {spark_submit.py:649} INFO - "name" : "Provider_Name",
[2025-05-02T02:39:48.170+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.170+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.171+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.172+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.172+0000] {spark_submit.py:649} INFO - "name" : "Provider_Gender",
[2025-05-02T02:39:48.173+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.173+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.174+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.174+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.175+0000] {spark_submit.py:649} INFO - "name" : "Provider_Speciality",
[2025-05-02T02:39:48.175+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.176+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.176+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.177+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.177+0000] {spark_submit.py:649} INFO - "name" : "Provider_Address",
[2025-05-02T02:39:48.178+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.178+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.179+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.179+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.180+0000] {spark_submit.py:649} INFO - "name" : "Provider_City",
[2025-05-02T02:39:48.180+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.181+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.181+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.182+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.182+0000] {spark_submit.py:649} INFO - "name" : "Provider_State",
[2025-05-02T02:39:48.183+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.183+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.184+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.184+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.185+0000] {spark_submit.py:649} INFO - "name" : "Provider_Zip",
[2025-05-02T02:39:48.186+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.186+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.187+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.187+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.188+0000] {spark_submit.py:649} INFO - "name" : "Provider_Lat",
[2025-05-02T02:39:48.188+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.189+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.189+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.190+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.190+0000] {spark_submit.py:649} INFO - "name" : "Provider_Lon",
[2025-05-02T02:39:48.191+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.191+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.192+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.192+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.193+0000] {spark_submit.py:649} INFO - "name" : "Provider_Utilization",
[2025-05-02T02:39:48.194+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T02:39:48.194+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.195+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.195+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.196+0000] {spark_submit.py:649} INFO - "name" : "Organization_Name",
[2025-05-02T02:39:48.196+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.197+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.197+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.198+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.198+0000] {spark_submit.py:649} INFO - "name" : "Organization_Address",
[2025-05-02T02:39:48.199+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.199+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.200+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.200+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.201+0000] {spark_submit.py:649} INFO - "name" : "Organization_City",
[2025-05-02T02:39:48.201+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.202+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.202+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.203+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.204+0000] {spark_submit.py:649} INFO - "name" : "Organization_State",
[2025-05-02T02:39:48.204+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.205+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.205+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.206+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.206+0000] {spark_submit.py:649} INFO - "name" : "Organization_Zip",
[2025-05-02T02:39:48.207+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.207+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.208+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.208+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.209+0000] {spark_submit.py:649} INFO - "name" : "Organization_Lat",
[2025-05-02T02:39:48.210+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.210+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.211+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.212+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.212+0000] {spark_submit.py:649} INFO - "name" : "Organization_Lon",
[2025-05-02T02:39:48.213+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.214+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.214+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.215+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.215+0000] {spark_submit.py:649} INFO - "name" : "Organization_Phone",
[2025-05-02T02:39:48.216+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.217+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.217+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.218+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.218+0000] {spark_submit.py:649} INFO - "name" : "Effective_Date",
[2025-05-02T02:39:48.219+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T02:39:48.219+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.220+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.220+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.221+0000] {spark_submit.py:649} INFO - "name" : "Expiration_Date",
[2025-05-02T02:39:48.222+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T02:39:48.222+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.223+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.223+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.224+0000] {spark_submit.py:649} INFO - "name" : "Is_Current",
[2025-05-02T02:39:48.225+0000] {spark_submit.py:649} INFO - "type" : "boolean",
[2025-05-02T02:39:48.226+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.227+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.228+0000] {spark_submit.py:649} INFO - } ]
[2025-05-02T02:39:48.228+0000] {spark_submit.py:649} INFO - }
[2025-05-02T02:39:48.229+0000] {spark_submit.py:649} INFO - and corresponding Parquet message type:
[2025-05-02T02:39:48.230+0000] {spark_submit.py:649} INFO - message spark_schema {
[2025-05-02T02:39:48.230+0000] {spark_submit.py:649} INFO - optional binary Provider_Key (STRING);
[2025-05-02T02:39:48.231+0000] {spark_submit.py:649} INFO - optional binary Provider_Id (STRING);
[2025-05-02T02:39:48.232+0000] {spark_submit.py:649} INFO - optional binary Provider_Name (STRING);
[2025-05-02T02:39:48.232+0000] {spark_submit.py:649} INFO - optional binary Provider_Gender (STRING);
[2025-05-02T02:39:48.233+0000] {spark_submit.py:649} INFO - optional binary Provider_Speciality (STRING);
[2025-05-02T02:39:48.233+0000] {spark_submit.py:649} INFO - optional binary Provider_Address (STRING);
[2025-05-02T02:39:48.234+0000] {spark_submit.py:649} INFO - optional binary Provider_City (STRING);
[2025-05-02T02:39:48.235+0000] {spark_submit.py:649} INFO - optional binary Provider_State (STRING);
[2025-05-02T02:39:48.236+0000] {spark_submit.py:649} INFO - optional binary Provider_Zip (STRING);
[2025-05-02T02:39:48.236+0000] {spark_submit.py:649} INFO - optional double Provider_Lat;
[2025-05-02T02:39:48.237+0000] {spark_submit.py:649} INFO - optional double Provider_Lon;
[2025-05-02T02:39:48.238+0000] {spark_submit.py:649} INFO - optional int64 Provider_Utilization;
[2025-05-02T02:39:48.238+0000] {spark_submit.py:649} INFO - optional binary Organization_Name (STRING);
[2025-05-02T02:39:48.239+0000] {spark_submit.py:649} INFO - optional binary Organization_Address (STRING);
[2025-05-02T02:39:48.240+0000] {spark_submit.py:649} INFO - optional binary Organization_City (STRING);
[2025-05-02T02:39:48.240+0000] {spark_submit.py:649} INFO - optional binary Organization_State (STRING);
[2025-05-02T02:39:48.241+0000] {spark_submit.py:649} INFO - optional binary Organization_Zip (STRING);
[2025-05-02T02:39:48.242+0000] {spark_submit.py:649} INFO - optional double Organization_Lat;
[2025-05-02T02:39:48.242+0000] {spark_submit.py:649} INFO - optional double Organization_Lon;
[2025-05-02T02:39:48.243+0000] {spark_submit.py:649} INFO - optional binary Organization_Phone (STRING);
[2025-05-02T02:39:48.244+0000] {spark_submit.py:649} INFO - optional int96 Effective_Date;
[2025-05-02T02:39:48.245+0000] {spark_submit.py:649} INFO - optional int96 Expiration_Date;
[2025-05-02T02:39:48.246+0000] {spark_submit.py:649} INFO - optional boolean Is_Current;
[2025-05-02T02:39:48.246+0000] {spark_submit.py:649} INFO - }
[2025-05-02T02:39:48.247+0000] {spark_submit.py:649} INFO - 
[2025-05-02T02:39:48.248+0000] {spark_submit.py:649} INFO - 
[2025-05-02T02:39:48.251+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-05-02T02:39:48.618+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO Executor: Finished task 0.0 in stage 19.0 (TID 206). 10314 bytes result sent to driver
[2025-05-02T02:39:48.621+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 207) (***-scheduler, executor driver, partition 1, PROCESS_LOCAL, 10523 bytes)
[2025-05-02T02:39:48.622+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO Executor: Running task 1.0 in stage 19.0 (TID 207)
[2025-05-02T02:39:48.622+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 206) in 714 ms on ***-scheduler (executor driver) (1/2)
[2025-05-02T02:39:48.663+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodeGenerator: Code generated in 8.254173 ms
[2025-05-02T02:39:48.713+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T02:39:48.714+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO CodecConfig: Compression: SNAPPY
[2025-05-02T02:39:48.715+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-05-02T02:39:48.716+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-05-02T02:39:48.717+0000] {spark_submit.py:649} INFO - {
[2025-05-02T02:39:48.717+0000] {spark_submit.py:649} INFO - "type" : "struct",
[2025-05-02T02:39:48.718+0000] {spark_submit.py:649} INFO - "fields" : [ {
[2025-05-02T02:39:48.719+0000] {spark_submit.py:649} INFO - "name" : "Provider_Key",
[2025-05-02T02:39:48.719+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.720+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.721+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.721+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.722+0000] {spark_submit.py:649} INFO - "name" : "Provider_Id",
[2025-05-02T02:39:48.722+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.723+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.723+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.724+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.724+0000] {spark_submit.py:649} INFO - "name" : "Provider_Name",
[2025-05-02T02:39:48.725+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.725+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.726+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.726+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.727+0000] {spark_submit.py:649} INFO - "name" : "Provider_Gender",
[2025-05-02T02:39:48.727+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.728+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.728+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.729+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.729+0000] {spark_submit.py:649} INFO - "name" : "Provider_Speciality",
[2025-05-02T02:39:48.730+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.730+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.731+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.731+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.732+0000] {spark_submit.py:649} INFO - "name" : "Provider_Address",
[2025-05-02T02:39:48.732+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.733+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.733+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.734+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.734+0000] {spark_submit.py:649} INFO - "name" : "Provider_City",
[2025-05-02T02:39:48.735+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.735+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.736+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.736+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.737+0000] {spark_submit.py:649} INFO - "name" : "Provider_State",
[2025-05-02T02:39:48.737+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.738+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.738+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.739+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.739+0000] {spark_submit.py:649} INFO - "name" : "Provider_Zip",
[2025-05-02T02:39:48.739+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.740+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.740+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.741+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.741+0000] {spark_submit.py:649} INFO - "name" : "Provider_Lat",
[2025-05-02T02:39:48.742+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.742+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.743+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.743+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.744+0000] {spark_submit.py:649} INFO - "name" : "Provider_Lon",
[2025-05-02T02:39:48.744+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.744+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.745+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.745+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.746+0000] {spark_submit.py:649} INFO - "name" : "Provider_Utilization",
[2025-05-02T02:39:48.746+0000] {spark_submit.py:649} INFO - "type" : "long",
[2025-05-02T02:39:48.747+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.747+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.748+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.748+0000] {spark_submit.py:649} INFO - "name" : "Organization_Name",
[2025-05-02T02:39:48.749+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.749+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.749+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.750+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.750+0000] {spark_submit.py:649} INFO - "name" : "Organization_Address",
[2025-05-02T02:39:48.751+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.751+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.752+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.752+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.752+0000] {spark_submit.py:649} INFO - "name" : "Organization_City",
[2025-05-02T02:39:48.753+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.753+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.754+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.754+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.755+0000] {spark_submit.py:649} INFO - "name" : "Organization_State",
[2025-05-02T02:39:48.755+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.756+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.756+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.756+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.757+0000] {spark_submit.py:649} INFO - "name" : "Organization_Zip",
[2025-05-02T02:39:48.757+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.758+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.759+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.759+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.760+0000] {spark_submit.py:649} INFO - "name" : "Organization_Lat",
[2025-05-02T02:39:48.760+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.761+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.761+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.762+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.762+0000] {spark_submit.py:649} INFO - "name" : "Organization_Lon",
[2025-05-02T02:39:48.763+0000] {spark_submit.py:649} INFO - "type" : "double",
[2025-05-02T02:39:48.764+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.764+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.765+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.765+0000] {spark_submit.py:649} INFO - "name" : "Organization_Phone",
[2025-05-02T02:39:48.766+0000] {spark_submit.py:649} INFO - "type" : "string",
[2025-05-02T02:39:48.766+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.767+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.767+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.768+0000] {spark_submit.py:649} INFO - "name" : "Effective_Date",
[2025-05-02T02:39:48.769+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T02:39:48.769+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.770+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.771+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.771+0000] {spark_submit.py:649} INFO - "name" : "Expiration_Date",
[2025-05-02T02:39:48.772+0000] {spark_submit.py:649} INFO - "type" : "timestamp",
[2025-05-02T02:39:48.772+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.773+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.773+0000] {spark_submit.py:649} INFO - }, {
[2025-05-02T02:39:48.774+0000] {spark_submit.py:649} INFO - "name" : "Is_Current",
[2025-05-02T02:39:48.774+0000] {spark_submit.py:649} INFO - "type" : "boolean",
[2025-05-02T02:39:48.775+0000] {spark_submit.py:649} INFO - "nullable" : true,
[2025-05-02T02:39:48.775+0000] {spark_submit.py:649} INFO - "metadata" : { }
[2025-05-02T02:39:48.776+0000] {spark_submit.py:649} INFO - } ]
[2025-05-02T02:39:48.776+0000] {spark_submit.py:649} INFO - }
[2025-05-02T02:39:48.777+0000] {spark_submit.py:649} INFO - and corresponding Parquet message type:
[2025-05-02T02:39:48.777+0000] {spark_submit.py:649} INFO - message spark_schema {
[2025-05-02T02:39:48.778+0000] {spark_submit.py:649} INFO - optional binary Provider_Key (STRING);
[2025-05-02T02:39:48.778+0000] {spark_submit.py:649} INFO - optional binary Provider_Id (STRING);
[2025-05-02T02:39:48.778+0000] {spark_submit.py:649} INFO - optional binary Provider_Name (STRING);
[2025-05-02T02:39:48.779+0000] {spark_submit.py:649} INFO - optional binary Provider_Gender (STRING);
[2025-05-02T02:39:48.780+0000] {spark_submit.py:649} INFO - optional binary Provider_Speciality (STRING);
[2025-05-02T02:39:48.780+0000] {spark_submit.py:649} INFO - optional binary Provider_Address (STRING);
[2025-05-02T02:39:48.780+0000] {spark_submit.py:649} INFO - optional binary Provider_City (STRING);
[2025-05-02T02:39:48.781+0000] {spark_submit.py:649} INFO - optional binary Provider_State (STRING);
[2025-05-02T02:39:48.781+0000] {spark_submit.py:649} INFO - optional binary Provider_Zip (STRING);
[2025-05-02T02:39:48.782+0000] {spark_submit.py:649} INFO - optional double Provider_Lat;
[2025-05-02T02:39:48.782+0000] {spark_submit.py:649} INFO - optional double Provider_Lon;
[2025-05-02T02:39:48.783+0000] {spark_submit.py:649} INFO - optional int64 Provider_Utilization;
[2025-05-02T02:39:48.784+0000] {spark_submit.py:649} INFO - optional binary Organization_Name (STRING);
[2025-05-02T02:39:48.784+0000] {spark_submit.py:649} INFO - optional binary Organization_Address (STRING);
[2025-05-02T02:39:48.785+0000] {spark_submit.py:649} INFO - optional binary Organization_City (STRING);
[2025-05-02T02:39:48.785+0000] {spark_submit.py:649} INFO - optional binary Organization_State (STRING);
[2025-05-02T02:39:48.786+0000] {spark_submit.py:649} INFO - optional binary Organization_Zip (STRING);
[2025-05-02T02:39:48.787+0000] {spark_submit.py:649} INFO - optional double Organization_Lat;
[2025-05-02T02:39:48.787+0000] {spark_submit.py:649} INFO - optional double Organization_Lon;
[2025-05-02T02:39:48.788+0000] {spark_submit.py:649} INFO - optional binary Organization_Phone (STRING);
[2025-05-02T02:39:48.788+0000] {spark_submit.py:649} INFO - optional int96 Effective_Date;
[2025-05-02T02:39:48.789+0000] {spark_submit.py:649} INFO - optional int96 Expiration_Date;
[2025-05-02T02:39:48.789+0000] {spark_submit.py:649} INFO - optional boolean Is_Current;
[2025-05-02T02:39:48.790+0000] {spark_submit.py:649} INFO - }
[2025-05-02T02:39:48.790+0000] {spark_submit.py:649} INFO - 
[2025-05-02T02:39:48.791+0000] {spark_submit.py:649} INFO - 
[2025-05-02T02:39:48.880+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO Executor: Finished task 1.0 in stage 19.0 (TID 207). 9190 bytes result sent to driver
[2025-05-02T02:39:48.881+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 207) in 263 ms on ***-scheduler (executor driver) (2/2)
[2025-05-02T02:39:48.882+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-05-02T02:39:48.883+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DAGScheduler: ResultStage 19 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.009 s
[2025-05-02T02:39:48.883+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-05-02T02:39:48.884+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-05-02T02:39:48.884+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DAGScheduler: Job 10 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 1.022025 s
[2025-05-02T02:39:48.886+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DeltaFileFormatWriter: Start to commit write Job 0a28d7b9-1059-4d1e-b99c-09c6b9d6b157.
[2025-05-02T02:39:48.888+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DeltaFileFormatWriter: Write Job 0a28d7b9-1059-4d1e-b99c-09c6b9d6b157 committed. Elapsed time: 0 ms.
[2025-05-02T02:39:48.891+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:48 INFO DeltaFileFormatWriter: Finished processing stats for write job 0a28d7b9-1059-4d1e-b99c-09c6b9d6b157.
[2025-05-02T02:39:49.139+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:49.140+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DAGScheduler: Job 11 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.000317 s
[2025-05-02T02:39:49.269+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO OptimisticTransaction: [tableId=afbf6c39,txnId=e6dc9472] Attempting to commit version 0 with 5 actions with Serializable isolation level
[2025-05-02T02:39:49.593+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO OptimisticTransaction: Incremental commit: starting with snapshot version -1
[2025-05-02T02:39:49.792+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2025-05-02T02:39:49.793+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DAGScheduler: Job 12 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.000257 s
[2025-05-02T02:39:49.801+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DeltaLog: Creating a new snapshot v0 for commit version 0
[2025-05-02T02:39:49.802+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DeltaLog: Loading version 0.
[2025-05-02T02:39:49.803+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO Snapshot: [tableId=afbf6c39-868a-4894-9075-b8451943673e] Created snapshot Snapshot(path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log, version=0, metadata=Metadata(c788f322-2da7-4aab-817f-ac2128740e83,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"Provider_Key","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Id","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Name","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Gender","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Speciality","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Address","type":"string","nullable":true,"metadata":{}},{"name":"Provider_City","type":"string","nullable":true,"metadata":{}},{"name":"Provider_State","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Utilization","type":"long","nullable":true,"metadata":{}},{"name":"Organization_Name","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Address","type":"string","nullable":true,"metadata":{}},{"name":"Organization_City","type":"string","nullable":true,"metadata":{}},{"name":"Organization_State","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Phone","type":"string","nullable":true,"metadata":{}},{"name":"Effective_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Expiration_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Is_Current","type":"boolean","nullable":true,"metadata":{}}]},ArrayBuffer(),Map(),Some(1746153575720)), logSegment=LogSegment(s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log,0,List(S3AFileStatus{path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/00000000000000000000.json; isDirectory=false; length=7473; replication=1; blocksize=33554432; modification_time=1746153589000; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=8b5ae863e71f6e6967055317edb24a95 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746153589000), checksumOpt=Some(VersionChecksum(Some(e6dc9472-e385-4ceb-a302-1ba7173145e6),80764,2,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(c788f322-2da7-4aab-817f-ac2128740e83,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"Provider_Key","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Id","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Name","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Gender","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Speciality","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Address","type":"string","nullable":true,"metadata":{}},{"name":"Provider_City","type":"string","nullable":true,"metadata":{}},{"name":"Provider_State","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Utilization","type":"long","nullable":true,"metadata":{}},{"name":"Organization_Name","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Address","type":"string","nullable":true,"metadata":{}},{"name":"Organization_City","type":"string","nullable":true,"metadata":{}},{"name":"Organization_State","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Phone","type":"string","nullable":true,"metadata":{}},{"name":"Effective_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Expiration_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Is_Current","type":"boolean","nullable":true,"metadata":{}}]},ArrayBuffer(),Map(),Some(1746153575720)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-f28fcddd-7ad8-4ee3-96a7-6287e2db40ac-c000.snappy.parquet,Map(),73424,1746153588000,false,{"numRecords":272,"minValues":{"Provider_Key":"00168481a614b9b17e8fd0c18210d0d8","Provider_Id":"00b1a913-31e7-3941-8f26-9a07f04e","Provider_Name":"Agnes294 Dooley940","Provider_Gender":"F","Provider_Speciality":"GENERAL PRACTICE","Provider_Address":"1 EDGEWATER DRIVE  SUITE 103","Provider_City":"ABINGTON","Provider_State":"MA","Provider_Zip":"10011826","Provider_Lat":41.44720825,"Provider_Lon":-73.25677853480586,"Provider_Utilization":1,"Organization_Name":"1200 SUFFIELD STREET OPERATOR LL","Organization_Address":"1 EDGEWATER DRIVE  SUITE 103","Organization_City":"ABINGTON","Organization_State":"MA","Organization_Zip":"10011826","Organization_Lat":41.44720825,"Organization_Lon":-73.25677853480586,"Organization_Phone":"(413) 789-2200","Effective_Date":"2025-05-02T02:39:36.587Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"maxValues":{"Provider_Key":"ffc36f6ec149a0fc9f64cdbf22029ec6","Provider_Id":"fffdc2e7-c175-3e01-a2da-185da48c","Provider_Name":"Zane918 Bosco882","Provider_Gender":"M","Provider_Speciality":"GENERAL PRACTICE","Provider_Address":"ONE HOSPITAL ROAD  FIRST FL  WIN","Provider_City":"Worcester","Provider_State":"MA","Provider_Zip":"27803561","Provider_Lat":42.797835,"Provider_Lon":-70.06169919459835,"Provider_Utilization":2063,"Organization_Name":"ZANJABEE INTEGRATIVE MEDICINE AN","Organization_Address":"ONE HOSPITAL ROAD  FIRST FL  WIN","Organization_City":"Worcester","Organization_State":"MA","Organization_Zip":"27803561","Organization_Lat":42.797835,"Organization_Lon":-70.06169919459835,"Organization_Phone":"9789832435","Effective_Date":"2025-05-02T02:39:36.587Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"nullCount":{"Provider_Key":0,"Provider_Id":0,"Provider_Name":0,"Provider_Gender":0,"Provider_Speciality":0,"Provider_Address":0,"Provider_City":0,"Provider_State":0,"Provider_Zip":0,"Provider_Lat":0,"Provider_Lon":0,"Provider_Utilization":0,"Organization_Name":0,"Organization_Address":0,"Organization_City":0,"Organization_State":0,"Organization_Zip":0,"Organization_Lat":0,"Organization_Lon":0,"Organization_Phone":0,"Effective_Date":0,"Expiration_Date":0,"Is_Current":0}},null,null,None,None,None), AddFile(part-00001-a1e3d4fa-237e-4e80-b74b-6782f298b3ed-c000.snappy.parquet,Map(),7340,1746153588000,false,{"numRecords":1,"minValues":{"Provider_Key":"00000000000000000000000000000000","Provider_Id":"00000000-0000-0000-0000-00000000","Provider_Name":"Unknown","Provider_Gender":"Unknown","Provider_Speciality":"Unknown","Provider_Address":"Unknown","Provider_City":"Unknown","Provider_State":"Unknown","Provider_Zip":"Unknown","Provider_Lat":0.0,"Provider_Lon":0.0,"Provider_Utilization":0,"Organization_Name":"Unknown","Organization_Address":"Unknown","Organization_City":"Unknown","Organization_State":"Unknown","Organization_Zip":"Unknown","Organization_Lat":0.0,"Organization_Lon":0.0,"Organization_Phone":"Unknown","Effective_Date":"1900-01-01T00:00:00.000Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"maxValues":{"Provider_Key":"00000000000000000000000000000000","Provider_Id":"00000000-0000-0000-0000-00000000","Provider_Name":"Unknown","Provider_Gender":"Unknown","Provider_Speciality":"Unknown","Provider_Address":"Unknown","Provider_City":"Unknown","Provider_State":"Unknown","Provider_Zip":"Unknown","Provider_Lat":0.0,"Provider_Lon":0.0,"Provider_Utilization":0,"Organization_Name":"Unknown","Organization_Address":"Unknown","Organization_City":"Unknown","Organization_State":"Unknown","Organization_Zip":"Unknown","Organization_Lat":0.0,"Organization_Lon":0.0,"Organization_Phone":"Unknown","Effective_Date":"1900-01-01T00:00:00.000Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"nullCount":{"Provider_Key":0,"Provider_Id":0,"Provider_Name":0,"Provider_Gender":0,"Provider_Speciality":0,"Provider_Address":0,"Provider_City":0,"Provider_State":0,"Provider_Zip":0,"Provider_Lat":0,"Provider_Lon":0,"Provider_Utilization":0,"Organization_Name":0,"Organization_Address":0,"Organization_City":0,"Organization_State":0,"Organization_Zip":0,"Organization_Lat":0,"Organization_Lon":0,"Organization_Phone":0,"Effective_Date":0,"Expiration_Date":0,"Is_Current":0}},null,null,None,None,None))))))
[2025-05-02T02:39:49.804+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log, version=0, metadata=Metadata(c788f322-2da7-4aab-817f-ac2128740e83,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"Provider_Key","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Id","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Name","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Gender","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Speciality","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Address","type":"string","nullable":true,"metadata":{}},{"name":"Provider_City","type":"string","nullable":true,"metadata":{}},{"name":"Provider_State","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Utilization","type":"long","nullable":true,"metadata":{}},{"name":"Organization_Name","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Address","type":"string","nullable":true,"metadata":{}},{"name":"Organization_City","type":"string","nullable":true,"metadata":{}},{"name":"Organization_State","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Phone","type":"string","nullable":true,"metadata":{}},{"name":"Effective_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Expiration_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Is_Current","type":"boolean","nullable":true,"metadata":{}}]},ArrayBuffer(),Map(),Some(1746153575720)), logSegment=LogSegment(s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log,0,List(S3AFileStatus{path=s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/00000000000000000000.json; isDirectory=false; length=7473; replication=1; blocksize=33554432; modification_time=1746153589000; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=8b5ae863e71f6e6967055317edb24a95 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@79ac5fe8,1746153589000), checksumOpt=Some(VersionChecksum(Some(e6dc9472-e385-4ceb-a302-1ba7173145e6),80764,2,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(c788f322-2da7-4aab-817f-ac2128740e83,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"Provider_Key","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Id","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Name","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Gender","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Speciality","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Address","type":"string","nullable":true,"metadata":{}},{"name":"Provider_City","type":"string","nullable":true,"metadata":{}},{"name":"Provider_State","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Provider_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Provider_Utilization","type":"long","nullable":true,"metadata":{}},{"name":"Organization_Name","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Address","type":"string","nullable":true,"metadata":{}},{"name":"Organization_City","type":"string","nullable":true,"metadata":{}},{"name":"Organization_State","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Zip","type":"string","nullable":true,"metadata":{}},{"name":"Organization_Lat","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Lon","type":"double","nullable":true,"metadata":{}},{"name":"Organization_Phone","type":"string","nullable":true,"metadata":{}},{"name":"Effective_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Expiration_Date","type":"timestamp","nullable":true,"metadata":{}},{"name":"Is_Current","type":"boolean","nullable":true,"metadata":{}}]},ArrayBuffer(),Map(),Some(1746153575720)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-f28fcddd-7ad8-4ee3-96a7-6287e2db40ac-c000.snappy.parquet,Map(),73424,1746153588000,false,{"numRecords":272,"minValues":{"Provider_Key":"00168481a614b9b17e8fd0c18210d0d8","Provider_Id":"00b1a913-31e7-3941-8f26-9a07f04e","Provider_Name":"Agnes294 Dooley940","Provider_Gender":"F","Provider_Speciality":"GENERAL PRACTICE","Provider_Address":"1 EDGEWATER DRIVE  SUITE 103","Provider_City":"ABINGTON","Provider_State":"MA","Provider_Zip":"10011826","Provider_Lat":41.44720825,"Provider_Lon":-73.25677853480586,"Provider_Utilization":1,"Organization_Name":"1200 SUFFIELD STREET OPERATOR LL","Organization_Address":"1 EDGEWATER DRIVE  SUITE 103","Organization_City":"ABINGTON","Organization_State":"MA","Organization_Zip":"10011826","Organization_Lat":41.44720825,"Organization_Lon":-73.25677853480586,"Organization_Phone":"(413) 789-2200","Effective_Date":"2025-05-02T02:39:36.587Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"maxValues":{"Provider_Key":"ffc36f6ec149a0fc9f64cdbf22029ec6","Provider_Id":"fffdc2e7-c175-3e01-a2da-185da48c","Provider_Name":"Zane918 Bosco882","Provider_Gender":"M","Provider_Speciality":"GENERAL PRACTICE","Provider_Address":"ONE HOSPITAL ROAD  FIRST FL  WIN","Provider_City":"Worcester","Provider_State":"MA","Provider_Zip":"27803561","Provider_Lat":42.797835,"Provider_Lon":-70.06169919459835,"Provider_Utilization":2063,"Organization_Name":"ZANJABEE INTEGRATIVE MEDICINE AN","Organization_Address":"ONE HOSPITAL ROAD  FIRST FL  WIN","Organization_City":"Worcester","Organization_State":"MA","Organization_Zip":"27803561","Organization_Lat":42.797835,"Organization_Lon":-70.06169919459835,"Organization_Phone":"9789832435","Effective_Date":"2025-05-02T02:39:36.587Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"nullCount":{"Provider_Key":0,"Provider_Id":0,"Provider_Name":0,"Provider_Gender":0,"Provider_Speciality":0,"Provider_Address":0,"Provider_City":0,"Provider_State":0,"Provider_Zip":0,"Provider_Lat":0,"Provider_Lon":0,"Provider_Utilization":0,"Organization_Name":0,"Organization_Address":0,"Organization_City":0,"Organization_State":0,"Organization_Zip":0,"Organization_Lat":0,"Organization_Lon":0,"Organization_Phone":0,"Effective_Date":0,"Expiration_Date":0,"Is_Current":0}},null,null,None,None,None), AddFile(part-00001-a1e3d4fa-237e-4e80-b74b-6782f298b3ed-c000.snappy.parquet,Map(),7340,1746153588000,false,{"numRecords":1,"minValues":{"Provider_Key":"00000000000000000000000000000000","Provider_Id":"00000000-0000-0000-0000-00000000","Provider_Name":"Unknown","Provider_Gender":"Unknown","Provider_Speciality":"Unknown","Provider_Address":"Unknown","Provider_City":"Unknown","Provider_State":"Unknown","Provider_Zip":"Unknown","Provider_Lat":0.0,"Provider_Lon":0.0,"Provider_Utilization":0,"Organization_Name":"Unknown","Organization_Address":"Unknown","Organization_City":"Unknown","Organization_State":"Unknown","Organization_Zip":"Unknown","Organization_Lat":0.0,"Organization_Lon":0.0,"Organization_Phone":"Unknown","Effective_Date":"1900-01-01T00:00:00.000Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"maxValues":{"Provider_Key":"00000000000000000000000000000000","Provider_Id":"00000000-0000-0000-0000-00000000","Provider_Name":"Unknown","Provider_Gender":"Unknown","Provider_Speciality":"Unknown","Provider_Address":"Unknown","Provider_City":"Unknown","Provider_State":"Unknown","Provider_Zip":"Unknown","Provider_Lat":0.0,"Provider_Lon":0.0,"Provider_Utilization":0,"Organization_Name":"Unknown","Organization_Address":"Unknown","Organization_City":"Unknown","Organization_State":"Unknown","Organization_Zip":"Unknown","Organization_Lat":0.0,"Organization_Lon":0.0,"Organization_Phone":"Unknown","Effective_Date":"1900-01-01T00:00:00.000Z","Expiration_Date":"9999-12-31T00:00:00.000Z"},"nullCount":{"Provider_Key":0,"Provider_Id":0,"Provider_Name":0,"Provider_Gender":0,"Provider_Speciality":0,"Provider_Address":0,"Provider_City":0,"Provider_State":0,"Provider_Zip":0,"Provider_Lat":0,"Provider_Lon":0,"Provider_Utilization":0,"Organization_Name":0,"Organization_Address":0,"Organization_City":0,"Organization_State":0,"Organization_Zip":0,"Organization_Lat":0,"Organization_Lon":0,"Organization_Phone":0,"Effective_Date":0,"Expiration_Date":0,"Is_Current":0}},null,null,None,None,None))))))
[2025-05-02T02:39:49.819+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 7473)
[2025-05-02T02:39:49.852+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO OptimisticTransaction: [tableId=afbf6c39,txnId=e6dc9472] Committed delta #0 to s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log
[2025-05-02T02:39:49.854+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO ChecksumHook: Writing checksum file for table path s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log version 0
[2025-05-02T02:39:49.914+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:49 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
[2025-05-02T02:39:50.012+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:50 INFO CheckpointFileManager: Writing atomically to s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/00000000000000000000.crc using temp file s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/.00000000000000000000.crc.2403f9a1-b91f-4fc2-b0ad-05e0bc7c1e5c.tmp
[2025-05-02T02:39:50.568+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:50 INFO CheckpointFileManager: Renamed temp file s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/.00000000000000000000.crc.2403f9a1-b91f-4fc2-b0ad-05e0bc7c1e5c.tmp to s3a://medical-bucket/curated/transactional/medical-data-sample/dim_providers/_delta_log/00000000000000000000.crc
[2025-05-02T02:39:50.574+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:50 INFO CreateDeltaTableCommand: Table is path-based table: false. Update catalog with mode: CreateOrReplace
[2025-05-02T02:39:50.625+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:50 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`curated`.`dim_providers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.
[2025-05-02T02:39:51.492+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO SparkContext: Invoking stop() from shutdown hook
[2025-05-02T02:39:51.493+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-02T02:39:51.511+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO SparkUI: Stopped Spark web UI at http://***-scheduler:4040
[2025-05-02T02:39:51.526+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-02T02:39:51.557+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO MemoryStore: MemoryStore cleared
[2025-05-02T02:39:51.558+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO BlockManager: BlockManager stopped
[2025-05-02T02:39:51.562+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-02T02:39:51.566+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-02T02:39:51.574+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO SparkContext: Successfully stopped SparkContext
[2025-05-02T02:39:51.575+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO ShutdownHookManager: Shutdown hook called
[2025-05-02T02:39:51.576+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-81c98496-f870-4648-b93d-8de06ac379f7
[2025-05-02T02:39:51.581+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f/pyspark-a95d4291-c5a7-455f-a801-23bc10d5b9ea
[2025-05-02T02:39:51.586+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-7fc122af-21d5-45b7-b190-39a58f93901f
[2025-05-02T02:39:51.590+0000] {spark_submit.py:649} INFO - 25/05/02 02:39:51 INFO ShutdownHookManager: Deleting directory /tmp/hive-v3_1-9c16e7a2-2b14-4775-a2f6-50caf05bb4a2
[2025-05-02T02:39:51.726+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-05-02T02:39:51.727+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=medical_dag_v17, task_id=dim_group.creating_dim_providers, run_id=manual__2025-05-02T02:18:00.042814+00:00, execution_date=20250502T021800, start_date=20250502T023850, end_date=20250502T023951
[2025-05-02T02:39:53.671+0000] {job.py:229} INFO - Heartbeat recovered after 15.10 seconds
[2025-05-02T02:39:53.680+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-05-02T02:39:53.698+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-05-02T02:39:53.699+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
